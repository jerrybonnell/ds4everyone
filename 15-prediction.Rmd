# Prediction

An important aspect of data science is to find out what data can tell us about the future. What do data about climate and pollution say about temperatures a few decades from now? Based on a person's internet profile, which websites are likely to interest them? How can a patient's medical history be used to judge how well he or she will respond to a treatment?

To answer such questions, data scientists have developed methods for making predictions. In this chapter we will study one of the most commonly used ways of predicting the value of one variable based on the value of another.

The foundations of the method were laid by [Sir Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton). As we saw in Section 7.1, Galton studied how physical characteristics are passed down from one generation to the next. Among his best known work is the prediction of the heights of adults based on the heights of their parents. We have studied the dataset that Galton collected for this. The data frame `galton` contains his data on the midparent height and child's height (all in inches) for a population of 934 adult "children".

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/galton.csv"
galton <- read_csv(url) %>%
  transmute(midparent = midparentHeight, 
            child = childHeight)
galton
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(galton) + 
  geom_point(aes(x = midparent, y = child), color = "darkcyan", alpha = 0.5)
```

The primary reason for collecting the data was to be able to predict the adult height of a child born to parents similar to those in the dataset. We made these predictions in Section 7.1, after noticing the positive association between the two variables.

Our approach was to base the prediction on all the points that correspond to a midparent height of around the midparent height of the new person. To do this, we wrote a function called `predict_child` which takes a midparent height as its argument and returns the average height of all the children who had midparent heights within half an inch of the argument.

```{r}
predict_child <- function(mpht) {
  # Return a prediction of the height of a child 
  # whose parents have a midparent height of mpht.
  # The prediction is the average height of the children 
  # whose midparent height is in the range mpht plus or minus 0.5 inches.
  close_points <- filter(galton, between(midparent, {{ mpht }} -0.5, {{ mpht }} + 0.5))
  return(mean(pull(close_points, child)))
}
```

We applied the function to the column of `midparent` heights, and visualized our results.

```{r}
galton_with_predictions <- galton %>%
  mutate(prediction = map_dbl(midparent, predict_child))
```

```{r dpi=80, fig.align="center", message = FALSE}
# JB: for our own note, discussion on pivot 
# (need to roll this change to earlier sections) https://community.rstudio.com/t/adding-manual-legend-to-ggplot2/41651
plot_df <- galton_with_predictions %>%
  pivot_longer(c(child, prediction), names_to = "measure", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = measure), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

The prediction at a given midparent height lies roughly at the center of the vertical strip of points at the given height. This method of prediction is called *regression*. Later in this chapter we will see where this term came from. We will also see whether we can avoid our arbitrary definitions of "closeness" being "within 0.5 inches". But first we will develop a measure that can be used in many settings to decide how good one variable will be as a predictor of another.

## Correlation

In this section we will develop a measure of how tightly clustered a scatter diagram is about a straight line. Formally, this is called measuring *linear association*.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

In some of the experiments we will be demonstrating, we will be working directly with the population so the corrections R provides with `sd` is unnecessary. We will define a function `pop.sd` and our own version of `scale` called `pop.scale`, as `scale` makes use of `sd`.

```{r}
pop.sd <- function(x) sd(x) * sqrt((length(x)-1) / length(x))
pop.scale <- function(x) (x - mean(x)) / pop.sd(x)
```

### The `hybrid` data frame 

The data frame `hybrid` contains data on hybrid passenger cars sold in the United States from 1997 to 2013. The data were adapted from the online data archive of [Prof. Larry Winner](http://www.stat.ufl.edu/%7Ewinner/) of the University of Florida. The columns:

* `vehicle`: model of the car
* `year`: year of manufacture
* `msrp`: manufacturer's suggested retail price in 2013 dollars
* `acceleration`: acceleration rate in km per hour per second
* `mpg`: fuel econonmy in miles per gallon
* `class`: the model's class.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/hybrid.csv"
hybrid <- read_csv(url)
hybrid
```

The graph below is a scatter plot of `msrp` versus `acceleration`. That means `msrp` is plotted on the vertical axis and `acceleration` on the horizontal.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(hybrid) + 
  geom_point(aes(x = acceleration, y = msrp), color = "darkcyan", alpha = 0.5)
```

Notice the positive association. The scatter of points is sloping upwards, indicating that cars with greater acceleration tended to cost more, on average; conversely, the cars that cost more tended to have greater acceleration on average.

The scatter diagram of MSRP versus mileage shows a negative association. Hybrid cars with higher mileage tended to cost less, on average. This seems surprising till you consider that cars that accelerate fast tend to be less fuel efficient and have lower mileage. As the previous scatter plot showed, those were also the cars that tended to cost more.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(hybrid) + 
  geom_point(aes(x = mpg, y = msrp), color = "darkcyan", alpha = 0.5)
```

Along with the negative association, the scatter diagram of price versus efficiency shows a non-linear relation between the two variables. The points appear to be clustered around a curve, not around a straight line.

If we restrict the data just to the SUV class, however, the association between price and efficiency is still negative but the relation appears to be more linear. The relation between the price and acceleration of SUV's also shows a linear trend, but with a positive slope.

```{r dpi=80, fig.align="center", message = FALSE}
suv <- filter(hybrid, class == 'SUV')
ggplot(suv) + 
  geom_point(aes(x = mpg, y = msrp), color = "darkcyan", alpha = 0.5)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(suv) + 
  geom_point(aes(x = acceleration, y = msrp), color = "darkcyan", alpha = 0.5)
```

You will have noticed that we can derive useful information from the general orientation and shape of a scatter diagram even without paying attention to the units in which the variables were measured.

Indeed, we could plot all the variables in standard units and the plots would look the same. This gives us a way to compare the degree of linearity in two scatter diagrams.

Recall that in an earlier section we applied the built-in function `scale` to convert a vector of numbers to standard units with formula:

$$z = \frac{\text{value} - \text{average}}{\text{SD}}$$

We can use this function to re-draw the two scatter diagrams for SUVs, with all the variables measured in standard units.

```{r dpi=80, fig.align="center", message = FALSE}
suv %>%
  transmute(mpg_standard = scale(mpg),
            msrp_standard = scale(msrp)) %>%
  ggplot() + 
  geom_point(aes(x = mpg_standard, y = msrp_standard), 
             color = "darkcyan", alpha = 0.5)
```

```{r dpi=80, fig.align="center", message = FALSE}
suv %>%
  transmute(acceleration_standard = scale(acceleration),
            msrp_standard = scale(msrp)) %>%
  ggplot() + 
  geom_point(aes(x = acceleration_standard, y = msrp_standard), 
             color = "darkcyan", alpha = 0.5)
```

The associations that we see in these figures are the same as those we saw before. Also, because the two scatter diagrams are now drawn on exactly the same scale, we can see that the linear relation in the second diagram is a little more fuzzy than in the first.

We will now define a measure that uses standard units to quantify the kinds of association that we have seen.

### The correlation coefficient

The *correlation coefficient* measures the strength of the linear relationship between two variables. Graphically, it measures how clustered the scatter diagram is around a straight line.

The term *correlation coefficient* isn't easy to say, so it is usually shortened to *correlation* and denoted by $r$.

Here are some mathematical facts about $r$ that we will just observe by simulation.

* The correlation coefficient $r$ is a number between -1 and 1.
* $r$ measures the extent to which the scatter plot clusters around a straight line.
* $r = 1$  if the scatter diagram is a perfect straight line sloping upwards, and  $r = -1$ if the scatter diagram is a perfect straight line sloping downwards.

The function `r_scatter` takes a value of $r$ as its argument and simulates a scatter plot with a correlation very close to $r$. Because of randomness in the simulation, the correlation is not expected to be exactly equal to  $r$.

Call `r_scatter` a few times, with different values of $r$ as the argument, and see how the scatter plot changes.

When  $r = 1$ the scatter plot is perfectly linear and slopes upward. When  $r = -1$, the scatter plot is perfectly linear and slopes downward. When $r = 0$, the scatter plot is a formless cloud around the horizontal axis, and the variables are said to be *uncorrelated*.

```{r echo = FALSE}
r_scatter <- function(r) {
  # Generate a scatter plot with a correlation approximately r
  x <- rnorm(10000)
  z <- rnorm(10000)
  y = r*x + (sqrt(1-r**2))*z
  tibble(x = x, y = y) %>%
    ggplot() + 
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
    xlab("") + 
    ylab("")
}
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(0.9)
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(0.25)
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(0)
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(-0.55)
```

### Calculating $r$

The formula for $r$ is not apparent from our observations so far. It has a mathematical basis that is outside the scope of this class. However, as you will see, the calculation is straightforward and helps us understand several of the properties of $r$. Note how we are using `pop.scale` for these steps.

__Formula for $r$__:

> $r$ is the average of the products of the two variables, when both variables are measured in standard units.

Here are the steps in the calculation. We will apply the steps to a simple table of values of  $x$ and  $y$.

```{r}
t <- tibble(
  x = seq(1, 6),
  y = c(2, 3, 1, 5, 2, 7)
)
t
```

Based on the scatter diagram, we expect that $r$ will be positive but not equal to 1.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(t) + 
  geom_point(aes(x = x, y = y), color = "red")
```

__Step 1.__ Convert each variable to standard units.

```{r}
t_su <- t %>%
  mutate(
    x_standard = pop.scale(x),
    y_standard = pop.scale(y))
t_su
```

__Step 2.__ Multiply each pair of standard units.

```{r}
t_product <- t_su %>%
  mutate(product_standard_units = x_standard * y_standard)
t_product
```

__Step 3.__ $r$ is the average of the products computed in Step 2.

```{r}
# r is the average of the products of standard units
r <- mean(pull(t_product, product_standard_units))
r
```

As expected, $r$ is positive but not equal to 1.

### Properties of $r$

The calculation shows that:

* $r$ is a pure number. It has no units. This is because $r$ is based on standard units.
* $r$ is unaffected by changing the units on either axis. This too is because $r$ is based on standard units.
$r$ is unaffected by switching the axes. Algebraically, this is because the product of standard units does not depend on which variable is called $x$ and which $y$. Geometrically, switching axes reflects the scatter plot about the line $y = x$, but does not change the amount of clustering nor the sign of the association.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(t) + 
  geom_point(aes(x = y, y = x), color = "red")
```

### The `cor` function 

We are going to be calculating correlations repeatedly, so it will help to have a function that computes it by performing all the steps described above. That function might look like: 

```{r}
correlation <- function(x, y) mean(pop.scale(x) * pop.scale(y))
```

Luckily, R provides such a function for us already called `cor`. The function takes two vectors as input and returns $r$, the mean of the products of those vectors in standard units.

Let's call `cor` on the `x` and `y` columns of `t`. The function returns the same answer to the correlation between $x$ and  $y$ as we got by direct application of the formula for $r$.

```{r}
cor(pull(t, x), pull(t, y))
```

As we noticed, the order in which the variables are specified doesn't matter.

```{r}
cor(pull(t, y), pull(t, x))
```

Calling `corr` on columns of the data frame `suv` gives us the correlation between price and mileage as well as the correlation between price and acceleration.

```{r}
cor(pull(suv, mpg), pull(suv, msrp))
```

```{r}
cor(pull(suv, acceleration), pull(suv, msrp))
```

These values confirm what we had observed:
* There is a negative association between price and efficiency, whereas the association between price and acceleration is positive.
* The linear relation between price and acceleration is a little weaker (correlation about 0.5) than between price and mileage (correlation about -0.67).

Correlation is a simple and powerful concept, but it is sometimes misused. Before using $r$, it is important to be aware of what correlation does and does not measure.

### Association is not Causation

Correlation only measures association. Correlation does not imply causation. Though the correlation between the weight and the math ability of children in a school district may be positive, that does not mean that doing math makes children heavier or that putting on weight improves the children's math skills. Age is a confounding variable: older children are both heavier and better at math than younger children, on average.

### Correlation Measures *Linear* Association

Correlation measures only one kind of association – linear. Variables that have strong non-linear association might have very low correlation. Here is an example of variables that have a perfect quadratic relation $y = x^2$ but have correlation equal to 0.

```{r dpi=80, fig.align="center", message = FALSE}
new_x <- seq(-4, 4.1, 0.5)
nonlinear <- tibble(x = new_x, 
                    y = new_x**2)
ggplot(nonlinear) + 
  geom_point(aes(x = x, y = y), color = "red")
```

```{r}
cor(pull(nonlinear, x), pull(nonlinear, y))
```

### Correlation is Affected by Outliers

Outliers can have a big effect on correlation. Here is an example where a scatter plot for which $r$ is equal to 1 is turned into a plot for which $r$ is equal to 0, by the addition of just one outlying point.

```{r dpi=80, fig.align="center", message = FALSE}
line <- tibble(
  x = 1:4, 
  y = 1:4
)
ggplot(line) + 
  geom_point(aes(x = x, y = y), color = "red")
```

```{r}
cor(pull(line, x), pull(line, y))
```

```{r dpi=80, fig.align="center", message = FALSE}
outlier <- tibble(
  x = 1:5, 
  y = c(1, 2, 3, 4, 0)
)
ggplot(outlier) + 
  geom_point(aes(x = x, y = y), color = "red")
```

```{r}
cor(pull(outlier, x), pull(outlier, y))
```

### Ecological Correlations Should be Interpreted with Care

Correlations based on aggregated data can be misleading. As an example, here are data on the Critical Reading and Math SAT scores in 2014. There is one point for each of the 50 states and one for Washington, D.C. The column `Participation Rate` contains the percent of high school seniors who took the test. The next three columns show the average score in the state on each portion of the test, and the final column is the average of the total scores on the test.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/sat2014.csv"
sat2014 <- read_csv(url) %>%
  arrange(State)
sat2014
```

The scatter diagram of Math scores versus Critical Reading scores is very tightly clustered around a straight line; the correlation is close to 0.985.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(sat2014) + 
  geom_point(aes(x = `Critical Reading`, y = Math), alpha = 0.5, color = "darkcyan")
```

```{r}
cor(pull(sat2014, `Critical Reading`), pull(sat2014, Math))
```

That's an extremely high correlation. But it's important to note that this does not reflect the strength of the relation between the Math and Critical Reading scores of *students*.

The data consist of average scores in each state. But states don't take tests – students do. The data in the table have been created by lumping all the students in each state into a single point at the average values of the two variables in that state. But not all students in the state will be at that point, as students vary in their performance. If you plot a point for each student instead of just one for each state, there will be a cloud of points around each point in the figure above. The overall picture will be more fuzzy. The correlation between the Math and Critical Reading scores of the students will be *lower* than the value calculated based on state averages.

Correlations based on aggregates and averages are called *ecological correlations* and are frequently reported. As we have just seen, they must be interpreted with care.

### Serious or tongue-in-cheek? 

In 2012, a [paper](http://www.biostat.jhsph.edu/courses/bio621/misc/Chocolate%20consumption%20cognitive%20function%20and%20nobel%20laurates%20%28NEJM%29.pdf) in the respected New England Journal of Medicine examined the relation between chocolate consumption and Nobel Prizes in a group of countries. The [Scientific American](http://blogs.scientificamerican.com/the-curious-wavefunction/chocolate-consumption-and-nobel-prizes-a-bizarre-juxtaposition-if-there-ever-was-one/) responded seriously whereas [others](http://www.reuters.com/article/2012/10/10/us-eat-chocolate-win-the-nobel-prize-idUSBRE8991MS20121010#vFdfFkbPVlilSjsB.97) were more relaxed. You are welcome to make your own decision! The following graph, provided in the paper, should motivate you to go and take a look.

```{r, echo=FALSE, fig.align="center", out.width='60%'}
knitr::include_graphics('images/Correlation_65_0.png')
```


## The Regression Line

The correlation coefficient $r$ doesn't just measure how clustered the points in a scatter plot are about a straight line. It also helps identify the straight line about which the points are clustered. In this section we will retrace the path that Galton and Pearson took to discover that line.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Galton's data on the heights of parents and their adult children showed a linear association. The linearity was confirmed when our predictions of the children's heights based on the midparent heights roughly followed a straight line.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/galton.csv"
galton <- read_csv(url) %>%
  transmute(midparent = midparentHeight, 
            child = childHeight)
galton
```

```{r}
predict_child <- function(mpht) {
  # Return a prediction of the height of a child 
  # whose parents have a midparent height of mpht.
  # The prediction is the average height of the children 
  # whose midparent height is in the range mpht plus or minus 0.5 inches.
  close_points <- filter(galton, between(midparent, {{ mpht }} - 0.5, {{ mpht }} + 0.5))
  return(mean(pull(close_points, child)))
}
```

```{r}
galton_with_predictions <- galton %>%
  mutate(prediction = map_dbl(midparent, predict_child))
```

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton_with_predictions %>%
  pivot_longer(c(child, prediction), names_to = "measure", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = measure), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

### Measuring in Standard Units

Let's see if we can find a way to identify this line. First, notice that linear association doesn't depend on the units of measurement – we might as well measure both variables in standard units.

```{r}
galton_su <- galton %>%
  transmute(midparent_su = scale(midparent),
            child_su = scale(child))
galton_su
```

On this scale, we can calculate our predictions exactly as before. But first we have to figure out how to convert our old definition of "close" points to a value on the new scale. We had said that midparent heights were "close" if they were within 0.5 inches of each other. Since standard units measure distances in units of SDs, we have to figure out how many SDs of midparent height correspond to 0.5 inches.

One SD of midparent heights is about 1.8 inches. So 0.5 inches is about 0.28 SDs.

```{r}
sd_midparent <- sd(pull(galton, midparent))
sd_midparent
```

```{r}
0.5/sd_midparent
```

We are now ready to modify our prediction function to make predictions on the standard units scale. All that has changed is that we are using the table of values in standard units, and defining "close" as above.

```{r}
predict_child_su <- function(mpht_su) {
  close = 0.5/sd_midparent
  # Return a prediction of the height (in standard units) of a child 
  # whose parents have a midparent height of mpht_su in standard units.
  close_points <- filter(galton_su, 
                         between(midparent_su, {{ mpht_su }} - close, {{ mpht_su }} + close))
  return(mean(pull(close_points, child_su)))
}
```

```{r}
galton_with_su_predictions <- galton_su %>%
  mutate(prediction_su = map_dbl(midparent_su, predict_child_su))
```

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton_with_su_predictions %>%
  pivot_longer(c(child_su, prediction_su), names_to = "measure", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent_su, y = height, color = measure), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

This plot looks exactly like the plot drawn on the original scale. Only the numbers on the axes have changed. This confirms that we can understand the prediction process by just working in standard units.

### Identifying the Line in Standard Units

Galton's scatter plot has a *football* shape – that is, it is roughly oval like an American football. Not all scatter plots are football shaped, not even those that show linear association. But in this section we will pretend we are Galton and work only with football shaped scatter plots. In the next section, we will generalize our analysis to other shapes of plots.

Here is a football shaped scatter plot with both variables measured in standard units. The 45 degree line is shown in red.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
r <- 0.5
x <- rnorm(10000)
z <- rnorm(10000)
y = r*x + (sqrt(1-r**2))*z
tibble(x = x, y = y) %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
  geom_line(data = data.frame(x = c(-4,4), y = c(-4,4)),
            aes(x = x, y = y), color = "salmon", size = 1) + 
  xlab("x in standard units") + 
  ylab("y in standard units")
```

But the 45 degree line is not the line that picks off the centers of the vertical strips. You can see that in the figure below, where the vertical line at 1.5 standard units is shown in black. The points on the scatter plot near the black line all have heights roughly in the -2 to 3 range. The red line is too high to pick off the center.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
r <- 0.5
x <- rnorm(10000)
z <- rnorm(10000)
y = r*x + (sqrt(1-r**2))*z
tibble(x = x, y = y) %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
  geom_line(data = data.frame(x = c(-4,4), y = c(-4,4)),
            aes(x = x, y = y), color = "salmon", size = 1) + 
  geom_line(data = data.frame(x = c(1.5,1.5), y = c(-4,4)),
            aes(x = x, y = y), color = "black", size = 1) + 
  xlab("x in standard units") + 
  ylab("y in standard units")
```

So the 45 degree line is not the "graph of averages." That line is the yellow one shown below.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
r <- 0.5
x <- rnorm(10000)
z <- rnorm(10000)
y = r*x + (sqrt(1-r**2))*z
tibble(x = x, y = y) %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
  geom_line(data = data.frame(x = c(-4,4), y = c(-4,4)),
            aes(x = x, y = y), color = "salmon", size = 1) + 
  geom_line(data = data.frame(x = c(1.5,1.5), y = c(-4,4)),
            aes(x = x, y = y), color = "black", size = 1) + 
  geom_line(data = data.frame(x = c(-4, 4), y = c(-4*0.6,4*0.6)),
            aes(x = x, y = y), color = "khaki", size = 1) +
  xlab("x in standard units") + 
  ylab("y in standard units")
```

Both lines go through the origin (0, 0). The yellow line goes through the centers of the vertical strips (at least roughly), and is *flatter* than the red 45 degree line.

The slope of the 45 degree line is 1. So the slope of the yellow "graph of averages" line is a value that is positive but less than 1.

What value could that be? You've guessed it – it's $r$.

### The Regression Line, in Standard Units

The yellow "graph of averages" line is called the *regression line*, for reasons we will explain shortly. But first, let's simulate some football shaped scatter plots with different values of $r$, and see how the line changes. In each case, the red 45 degree line has been drawn for comparison.

The function that performs the simulation is called `regression_line` and takes $r$ as its argument.

```{r echo = FALSE}
regression_line <- function(r) {
  x = rnorm(10000)
  z = rnorm(10000)
  y = r*x + (sqrt(1-r**2))*z
  if (r >= 0) {
    y_pts <- c(-4,4)
  } else {
    y_pts <- c(4,-4)
  }
  tibble(x = x, y = y) %>%
    ggplot() +
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
    geom_line(data = data.frame(x = c(-4, 4), y = c(-4*r,4*r)),
              aes(x = x, y = y), color = "khaki", size = 1) +
    geom_line(data = data.frame(x = c(-4, 4), y = y_pts),
              aes(x = x, y = y), color = "salmon", size = 1)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
regression_line(0.95)
```

```{r dpi=80, fig.align="center", message = FALSE}
regression_line(0.6)
```

When $r$ is close to 1, the scatter plot, the 45 degree line, and the regression line are all very close to each other. But for more moderate values of $r$, the regression line is noticeably flatter.

### The Regression Effect

In terms of prediction, this means that for a parents whose midparent height is at 1.5 standard units, our prediction of the child's height is somewhat *less* than 1.5 standard units. If the midparent height is 2 standard units, we predict that the child's height will be somewhat less than 2 standard units.

In other words, we predict that the child will be somewhat closer to average than the parents were.

This didn't please Sir Francis Galton. He had been hoping that exceptionally tall parents would have children who were just as exceptionally tall. However, the data were clear, and Galton realized that the tall parents have children who are not quite as exceptionally tall, on average. Frustrated, Galton called this phenomenon "regression to mediocrity." 

Galton also noticed that exceptionally short parents had children who were somewhat taller relative to their generation, on average. In general, individuals who are away from average on one variable are expected to be not quite as far away from average on the other. This is called the *regression effect*.

### The Equation of the Regression Line

In regression, we use the value of one variable (which we will call $x$) to predict the value of another (which we will call $y$). When the variables $x$ and $y$ are measured in standard units, the regression line for predicting $y$ based on $x$ has slope $r$ and passes through the origin. Thus the equation of the regression line can be written as:

$$
\mbox{estimate of }y ~=~ r \cdot x ~~~
\mbox{when both variables are measured in standard units}
$$

In the original units of the data, this becomes

$$
\frac{\mbox{estimate of}~y ~-~\mbox{average of}~y}{\mbox{SD of}~y}
~=~ r \times 
\frac{\mbox{the given}~x ~-~\mbox{average of}~x}{\mbox{SD of}~x}
$$

```{r, echo=FALSE, fig.align="center", out.width='60%'}
knitr::include_graphics('images/regline.png')
```

The slope and intercept of the regression line in original units can be derived from the diagram above. 

$$
\mathbf{\mbox{slope of the regression line}} ~=~ r \cdot
\frac{\mbox{SD of }y}{\mbox{SD of }x}
$$

$$
\mathbf{\mbox{intercept of the regression line}} ~=~
\mbox{average of }y ~-~ \mbox{slope} \cdot \mbox{average of }x
$$

The three functions below compute the correlation, slope, and intercept. All of them take three arguments: the name of the table, the label of the column containing $x$ , and the label of the column containing $y$.

```{r}
slope <- function(t, label_x, label_y) {
  r <- cor(pull(t, {{ label_x }}), pull(t, {{ label_y }}) )
  return (r * sd(pull(t, {{ label_y }})) / sd(pull(t, {{ label_x }})))
}

intercept <- function(t, label_x, label_y) {
  return(mean(pull(t, {{ label_y }})) - 
           slope(t, {{ label_x }}, {{ label_y }}) * mean(pull(t, {{ label_x }})))
}
```

### The Regression Line and Galton's Data

The correlation between midparent height and child's height is 0.32:

```{r}
galton_r <- cor(pull(galton, midparent), pull(galton, child))
galton_r
```

We can also find the equation of the regression line for predicting the child's height based on midparent height.

```{r}
galton_slope <- slope(galton, midparent, child)
galton_intercept <- intercept(galton, midparent, child)
c(galton_slope, galton_intercept)
```

The equation of the regression line is

$$
\mbox{estimate of child's height} ~=~ 0.64 \cdot \mbox{midparent height} ~+~ 22.64
$$

This is also known as the *regression equation.* The principal use of the regression equation is to predict $y$ based on $x$.

For example, for a midparent height of 70.48 inches, the regression equation predicts the child's height to be 67.56 inches.

```{r}
galton_slope*70.48 + galton_intercept
```

Our original prediction, created by taking the average height of all children who had midparent heights close to 70.48, came out to be pretty close: 67.63 inches compared to the regression line's prediction of 67.55 inches.

```{r}
filter(galton_with_predictions, midparent == 70.48) %>%
  head(3)
```

Here are all of the rows in Galton's table, along with our original predictions and the new regression predictions of the children's heights.

```{r}
galton_with_predictions <- galton_with_predictions %>%
  mutate(regression_prediction = galton_slope * midparent + galton_intercept)
galton_with_predictions
```

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton_with_predictions %>%
  pivot_longer(c(child, prediction, regression_prediction), 
               names_to = "attribute", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = attribute), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon", "blue"))
```

The grey dots show the regression predictions, all on the regression line. Notice how the line is very close to the gold graph of averages. For these data, the regression line does a good job of approximating the centers of the vertical strips.

### Fitted Values

The predictions all lie on the line and are known as the "fitted values". The function `fit` takes the name of the table and the labels of $x$ and $y$, and returns an array of fitted values, one fitted value for each point in the scatter plot.

```{r}
fit <- function(df, x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(df, {{ x }}, {{ y }})
  b <- intercept(df, {{ x }}, {{ y }})
  return(a * pull(df, {{ x }}) + b)
}

```

It is easier to see the line in the graph below than in the one above.

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton %>% 
  mutate(
    fitted = fit(galton, midparent, child)
  ) %>%
  pivot_longer(c(child, fitted), 
               names_to = "attribute", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = attribute), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

### Units of Measurement of the Slope

The slope is a ratio, and it worth taking a moment to study the units in which it is measured. Our example comes from the familiar dataset about mothers who gave birth in a hospital system. The scatter plot of pregnancy weights versus heights looks like a football that has been used in one game too many, but it's close enough to a football that we can justify putting our fitted line through it. In later sections we will see how to make such justifications more formal.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

```{r dpi=80, fig.align="center", message = FALSE}
baby %>% 
  mutate(fitted = fit(baby, `Maternal Height`, `Maternal Pregnancy Weight`)) %>%
  ggplot() + 
  geom_point(aes(x = `Maternal Height`, y = `Maternal Pregnancy Weight`),
             color = "darkcyan", alpha = 0.5) + 
  geom_line(aes(x = `Maternal Height`, y = fitted), color = "salmon", size = 2)
```

```{r}
slope(baby, `Maternal Height`, `Maternal Pregnancy Weight`)
```

The slope of the regression line is **3.57 pounds per inch**. This means that for two women who are 1 inch apart in height, our prediction of pregnancy weight will differ by 3.57 pounds. For a woman who is 2 inches taller than another, our prediction of pregnancy weight will be 
$$
2 \times 3.57 ~=~ 7.14
$$
pounds more than our prediction for the shorter woman.

Notice that the successive vertical strips in the scatter plot are one inch apart, because the heights have been rounded to the nearest inch. Another way to think about the slope is to take any two consecutive strips (which are necessarily 1 inch apart), corresponding to two groups of women who are separated by 1 inch in height. The slope of 3.57 pounds per inch means that the average pregnancy weight of the taller group is about 3.57 pounds more than that of the shorter group.

### Example

Suppose that our goal is to use regression to estimate the height of a basset hound based on its weight, using a sample that looks consistent with the regression model. Suppose the observed correlation $r$ is 0.5, and that the summary statistics for the two variables are as in the table below: 

|       |**average**    |**SD**     |   
|------:|:-------------:|:---------:|
|height |14 inches      |2 inches   |
|weight |50 pounds      |5 pounds   |

To calculate the equation of the regression line, we need the slope and the intercept.

$$
\mbox{slope} ~=~ \frac{r \cdot \mbox{SD of }y}{\mbox{SD of }x} ~=~
\frac{0.5 \cdot 2 \mbox{ inches}}{5 \mbox{ pounds}} ~=~ 0.2 ~\mbox{inches per pound}
$$


$$
\mbox{intercept} ~=~ \mbox{average of }y - \mbox{slope}\cdot \mbox{average of } x
~=~ 14 \mbox{ inches} ~-~ 0.2 \mbox{ inches per pound} \cdot 50 \mbox{ pounds}
~=~ 4 \mbox{ inches}
$$

The equation of the regression line allows us to calculate the estimated height, in inches,
based on a given weight in pounds:

$$
\mbox{estimated height} ~=~ 0.2 \cdot \mbox{given weight} ~+~ 4
$$

The slope of the line is measures the increase in the estimated height per unit increase in weight. The slope is positive, and it is important to note that this does not mean that we think basset hounds get taller if they put on weight. The slope reflects the difference in the average heights of two groups of dogs that are 1 pound apart in weight. Specifically, consider a group of dogs whose weight is $w$ pounds, and the group whose weight is $w+1$ pounds. The second group is estimated to be 0.2 inches taller, on average. This is true for all values of $w$ in the sample.

In general, the slope of the regression line can be interpreted as the average increase in $y$ per unit increase in $x$. Note that if the slope is negative, then for every unit increase in $x$, the average of $y$ decreases.

### Endnote

Even though we won't establish the mathematical basis for the regression equation, we can see that it gives pretty good predictions when the scatter plot is football shaped. It is a surprising mathematical fact that no matter what the shape of the scatter plot, the same equation gives the "best" among all straight lines. That's the topic of the next section.

## The Method of Least Squares

We have retraced the steps that Galton and Pearson took to develop the equation of the regression line that runs through a football shaped scatter plot. But not all scatter plots are football shaped, not even linear ones. Does every scatter plot have a "best" line that goes through it? If so, can we still use the formulas for the slope and intercept developed in the previous section, or do we need new ones?

To address these questions, we need a reasonable definition of "best". Recall that the purpose of the line is to *predict* or *estimate* values of $y$, given values of $x$. Estimates typically aren't perfect. Each one is off the true value by an *error*. A reasonable criterion for a line to be the "best" is for it to have the smallest possible overall error among all straight lines.

In this section we will make this criterion precise and see if we can identify the best straight line under the criterion.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Our first example is a dataset that has one row for every chapter of the novel "Little Women." The goal is to estimate the number of characters (that is, letters, spaces punctuation marks, and so on) based on the number of periods. Recall that we attempted to do this in the very first lecture of this course.

```{r message = FALSE}
little_women <- read_csv("data/little_women.csv")
head(little_women, 3)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(little_women) + 
  geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5)
```

To explore the data, we will need to use the functions `slope`, `intercept`, and `fit` we defined in the previous section.

```{r}
slope <- function(t, label_x, label_y) {
  r <- cor(pull(t, {{ label_x }}), pull(t, {{ label_y }}) )
  return (r * sd(pull(t, {{ label_y }})) / sd(pull(t, {{ label_x }})))
}

intercept <- function(t, label_x, label_y) {
  return(mean(pull(t, {{ label_y }})) - 
           slope(t, {{ label_x }}, {{ label_y }}) * mean(pull(t, {{ label_x }})))
}

fit <- function(df, x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(df, {{ x }}, {{ y }})
  b <- intercept(df, {{ x }}, {{ y }})
  return(a * pull(df, {{ x }}) + b)
}
```

Recall that we can check the correlation in the data using `cor`. 

```{r}
cor(pull(little_women, Periods), pull(little_women, Characters))
```

The scatter plot is remarkably close to linear, and the correlation is more than 0.92.

### Error in Estimation

The graph below shows the scatter plot and line that we developed in the previous section. We don't yet know if that's the best among all lines. We first have to say precisely what "best" means.

```{r}
lw_with_predictions <- little_women %>% 
  mutate(fitted = fit(little_women, Periods, Characters))

ggplot(lw_with_predictions) + 
  geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5) + 
  geom_line(aes(x = Periods, y = fitted), color = "salmon")
```

Corresponding to each point on the scatter plot, there is an error of prediction calculated as the actual value minus the predicted value. It is the vertical distance between the point and the line, with a negative sign if the point is below the line.

```{r}
lw_with_predictions <- lw_with_predictions %>%
  mutate(error = Characters - fitted)
lw_with_predictions
```

We can use `slope` and `intercept` to calculate the slope and intercept of the fitted line. The ggplot below shows the line (in blue). The errors corresponding to the points are shown in red. 

```{r}
lw_reg_slope <- slope(little_women, Periods, Characters)
lw_reg_intercept <- intercept(little_women, Periods, Characters)

paste('Slope of Regression Line: ', round(lw_reg_slope), ' characters per period')
paste('Intercept of Regression Line: ', round(lw_reg_intercept), ' characters')
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(lw_with_predictions) + 
  geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5) + 
  geom_line(aes(x = Periods, y = fitted), color = "blue") +
  geom_segment(aes(x = Periods, y = Characters,
                   xend = Periods, yend = fitted), color = "pink")
```

Let's package this ggplot call into a function we can play with. 

```{r}
plot_lw_errors <- function(lw_df, slope, intercept) {
  ggplot(lw_df) + 
    geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5) + 
    geom_line(aes(x = Periods, y = slope * Periods + intercept), color = "blue") +
    geom_segment(aes(x = Periods, y = Characters,
                     xend = Periods, yend = slope * Periods + intercept), color = "pink")
}
```

```{r dpi=80, fig.align="center", message = FALSE}
little_women %>%
  plot_lw_errors(lw_reg_slope, lw_reg_intercept)
```

Had we used a different line to create our estimates, the errors would have been different. The graph below shows how big the errors would be if we were to use another line for estimation. The second graph shows large errors obtained by using a line that is downright silly.

```{r dpi=80, fig.align="center", message = FALSE}
little_women %>%
  plot_lw_errors(50, 10000)
```

```{r dpi=80, fig.align="center", message = FALSE}
little_women %>%
  plot_lw_errors(-100, 50000)
```

### Root Mean Squared Error

What we need now is one overall measure of the rough size of the errors. You will recognize the approach to creating this – it's exactly the way we developed the SD.

If you use any arbitrary line to calculate your estimates, then some of your errors are likely to be positive and others negative. To avoid cancellation when measuring the rough size of the errors, we will take the mean of the squared errors rather than the mean of the errors themselves.

The mean squared error of estimation is a measure of roughly how big the squared errors are, but as we have noted earlier, its units are hard to interpret. Taking the square root yields the root mean square error (rmse), which is in the same units as the variable being predicted and therefore much easier to understand.

### Minimizing the Root Mean Squared Error

Our observations so far can be summarized as follows.

- To get estimates of $y$ based on $x$, you can use any line you want.
- Every line has a root mean squared error of estimation.
- "Better" lines have smaller errors.

Is there a "best" line? That is, is there a line that minimizes the root mean squared error among all lines? 

To answer this question, we will start by defining a function `lw_rmse` to compute the root mean squared error of any line through the Little Women scatter diagram. The function takes the slope and the intercept (in that order) as its arguments.

```{r}
lw_rmse <- function(slope, intercept) {
  g <- little_women %>%
    plot_lw_errors(slope, intercept)
  x = pull(little_women, Periods)
  y = pull(little_women, Characters)
  fitted = slope * x + intercept
  mse = mean((y - fitted) ** 2)
  print(g)
  print(paste("Root mean squared error:", mse ** 0.5))
}
```

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(50, 10000)
```

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(-100, 50000)
```

Bad lines have big values of rmse, as expected. But the rmse is much smaller if we choose a slope and intercept close to those of the regression line.

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(90, 4000)
```

Here is the root mean squared error corresponding to the regression line. By a remarkable fact of mathematics, no other line can beat this one. 

> The regression line is the unique straight line that minimizes the mean squared error of estimation among all straight lines.

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(lw_reg_slope, lw_reg_intercept)
```

The proof of this statement requires abstract mathematics that is beyond the scope of this course. On the other hand, we do have a powerful tool – R – that performs large numerical computations with ease. So we can use R to confirm that the regression line minimizes the mean squared error.

### Numerical Optimization

First note that a line that minimizes the root mean squared error is also a line that minimizes the squared error. The square root makes no difference to the minimization. So we will save ourselves a step of computation and just minimize the mean squared error (mse).

We are trying to predict the number of characters ($y$) based on the number of periods ($x$) in chapters of Little Women. If we use the line 
$$
\mbox{prediction} ~=~ ax + b
$$
it will have an mse that depends on the slope $a$ and the intercept $b$. The function `lw_mse` takes a `params` vector as its argument where its first element is the slope and its second element the intercept, and returns the corresponding mse.

```{r}
lw_mse <- function(params) {
  any_slope <- params[1]
  any_intercept <- params[2]
  
  x = pull(little_women, Periods)
  y = pull(little_women, Characters)
  fitted = any_slope * x + any_intercept
  return(mean((y - fitted) ** 2))
}
```

Let's check that `lw_mse` gets the right answer for the root mean squared error of the regression line. Remember that `lw_mse` returns the mean squared error, so we have to take the square root to get the rmse.

```{r}
params <- c(lw_reg_slope, lw_reg_intercept)
lw_mse(params)**0.5
```

That's the same as the value we got by using `lw_rmse` earlier. You can confirm that `lw_mse` returns the correct value for other slopes and intercepts too. For example, here is the rmse of the extremely bad line that we tried earlier.

```{r}
params <- c(-100, 50000)
lw_mse(params)**0.5
```

And here is the rmse for a line that is close to the regression line.

```{r}
params <- c(90, 4000)
lw_mse(params)**0.5
```

If we experiment with different values, we can find a low-error slope and intercept through trial and error, but that would take a while. Fortunately, there is a R function that does all the trial and error for us.

The `optim` function can be used to find the values in a vector for which the function returns its minimum value. R uses a similar trial-and-error approach, following the changes that lead to incrementally lower output values.

`optim` takes as arguments an initial vector of values, let's call it the `initial_guess` vector, and a function to be minimized using `initial_guess`. For example, the function `lw_mse` takes a vector `params` where the first element is a slope and the second element an intercept, and returns the corresponding mse.  

The call `optim(initial_guess, lw_mse)` will nudge around the values of slope and intercept inside `initial_guess` until a minimum is reached. At that point, a vector is returned consisting of the slope and intercept that minimize the mse. These minimizing values are excellent approximations arrived at by intelligent trial-and-error, not exact values based on formulas.

```{r}
initial_guess <- c(-100,50000)  # start with a very bad line! 
best <- optim(initial_guess, lw_mse)
best$par
```

These values are the same as the values we calculated earlier by using the `slope` and `intercept` functions. We see small deviations due to the inexact nature of `minimize`, but the values are essentially the same.

```{r}
paste("slope from formula: ", lw_reg_slope)
paste("slope from minimize: ", best$par[1])
paste("intercept from formula: ", lw_reg_intercept)
paste("intercept from minimize: ", best$par[2])

```


### The Least Squares Line

Therefore, we have found not only that the regression line minimizes mean squared error, but also that minimizing mean squared error gives us the regression line. The regression line is the only line that minimizes mean squared error.

That is why the regression line is sometimes called the "least squares line."


## Least Squares Regression

TBA 

## Visual Diagnostics

TBA



