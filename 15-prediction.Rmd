# (PART) Advanced Topic {-} 

# Regression

We will now use R to conduct investigations for information.
In this chapter, we study the concept of modeling the data.
We study specifically the concept of *regression*, which is a group of methods for modeling the values of a variable based on the values of other variables.
We have seen something that resembles regression in the introduction for plotting, where we guessed by examination that the line explaining the highway mileage with the displacement.
R has the functionality for doing the determination of the line automatically, which we call the *linear model*.

## Linear Regression

*Linear regression* is a method that estimates the value of a variable (which we call the *target*) against a collection of other variables (which we the *factors*) so we can express the value of the target as some combination of the factors, where each factor either adds to or subtracts from the target value estimation.
The estimation takes the form
\[
\mathrm{target} \approx \mathrm{intercept} + \mathrm{scalor}_1 * \mathrm{factor}_1 + \cdots + \mathrm{scalor}_k * \mathrm{factor}_k.
\]
Here $\mathrm{intercept}, \mathrm{scalor}_1, \ldots, \mathrm{scalor}_k$ are the parameters we estimate and are universally applicable to data points in the data set.

Let $c$ be the intercept and let $s_1, \ldots, s_k$ be the scalors.
Given $f_1, \ldots, f_k$ as values of the factors, the estimate according to the model is the sum of $s_i * f_i$ for all $i$ and the intercept $c$; that is, we scale the value of the $i$-th attribute by $s_i$ and add together the "scaled" factors and the intercept.
The formula for the estimate is thus
\[
c + s_1 * f_1 + \cdots + s_k * f_k .
\]
The estimate the model produces may not match the actual value.
So, given a data point $(t,f_1,\ldots,f_k)$ appearing in the data, we compute how much it differs from the estimate by subtracting the estimate from the actual.
We call this difference the *residual*.
If the residual is 0, our model perfectly fits the data point.
We want our model to fit all the data points perfectly, but such a perfect model usually does not exist for a real data set.
This means that regardless of the parameter choice, some data point produces a non-zero residual.
Why do we pick our model then?
We choose our model so that an "overall* residual amount is small.

To assess an *overall* residual amount, simply summing the residual is not good because some residuals are positive and some residuals are negative, and positive and negative values cancel each other.
We thus use the square of each residual in place of the simple residual amount.
Since squares are nonnegative, we can avoid the cancellation problem.
In other words, we use the following *squared residual*
$$
(t - (c + s_1 * f_1 + \cdots + s_k * f_k))^2
$$
and make the total squared residual as small as possible.
There is a mathematical method for obtaining the parameter values that achieve the minimum total squared distance, but learning such methods is beyond the scope of this course.

## Correlation

To explain, how a linear regression estimator in R works, let us use an artifial data set.
Recall that `tibble()` allows you to create a data set.
The data set `dt` below has four attributes `xx`, `yy`, `zz`, and `ww`.
The attribute `xx` ranges from 10 to 30 with an equal gap of $0.5$ between two values.
The value for `yy` is 25 times `xx`.
The value for `ww` is $0.3$ times the square of `xx` minus 10 times `sin(xx)`.
The value for `ww` is the sum of `yy` and `zz`.


```{r}
dt <- tibble(xx = seq(10,30,0.5), yy = 25 * xx, zz = - 10 * sin(xx), ww = yy + zz)
```

Let us use `ggplot` to plot `yy`, `zz`, and `ww` against `xx.


```{r}
ggplot(dt, aes(x = xx, y = yy)) + geom_point() + geom_smooth()
ggplot(dt, aes(x = xx, y = zz)) + geom_point()
ggplot(dt, aes(x = xx, y = ww)) + geom_point() + geom_smooth()
```

We observe that `yy` aligns perfectly with `xx`, `zz` is wavy and shows no trend, and `ww` almost perfectly aligns with `xx` except that it is a bit wavy.
The observation makes sense because we created `ww` by adding the small wavy `zz` to the perfectly aligning `yy`.
We thus can anticipate that the regression produces the line close to the function of `yy` as an estimate.

Before getting into the use of regression, we present how in a case like this (one factor for one target), how likely it is the linear progression produces good estimates.
The statistical concept we use for the investigation is *correlation*.
Suppose we have some $n$ observations in which we see two attributes $sx$ and $y$.
Let $\overline{x}$ and $\overline{y}$ denote the means of $x$ and $y$ values, respectively.
Obtain from each value of $x$ a new value by subtracting $\overline{x}$ from it.
Similarly, from each value of $y$ a nmew value by subtracting $\overline{y}$ from it.
The definition of a correlation is as follows:
\[
\frac{\mathrm{the~average~of~} $x' y'$}{(\mathrm{the~standard~deviation~of~}$x$) \times (\mathrm{the~standard~deviation~of ~}$y$)}
\]
where the average in the numerator uses division by $n-1$ instead of division by $n$. 

R comes the function for computing the correlation. You have only two provide the two sources between which you need to compute the correlation.
Let us obtain the correlations between `xx` and `yy`, `xx` and `ww`, and `zz`.

```{r}
cor(dt$xx,dt$yy)
cor(dt$xx,dt$zz)
cor(dt$xx,dt$ww)
```

The range of correlation is between $-1$ and $1$, where $1$ is the perfect positive correlation (for example, a variable has the perfect positive correlation with itself), $-1$ is the perfect negative correlation (for example, a variable has the perfect positive correlation with itself when the opposite sign), and $0$ is no correlation at all.
We see that the correlation of `xx` is high with `yy` and `zz` and almost nonexistent with `zz`.

## Linear Regression

Now let us run the linear regression on R.
The function is `lm()`, which is the short-hand for "linear model".
Running the linear regression function requires two parameters.
One parameter is the data set, which takes the form `data = DATA_SET_NAME`.
The other is the target and the factors, which takes the form `TARGET ~ FACTOR_1 + \cdots + FACTOR_k`.
The regression we have at hand has only one factor, `xx`, and the target is `ww`, and so we call the function as `lm(data = dt, ww ~ xx)`.
The function returns the result as a collection of tables and numbers.
Below we store it in `fitted`.

```{r}
fitted <- lm(data = dt, ww ~ xx)
```

The function `summary` reveals the information the model has.

```{r}
summary(fitted)
```

A key piece of information appears under the heading "Coefficients".
The "Estimate" column of the segment shows the estimates of the coefficients.
The function is
$$
2.0944 + 24.9283 xx
$$
This may be slightly off because the original had 0 intercept but the estimate has 2.0944.

The components of the `summary` are accessible by attaching `$` and the attribute in question.

```{r}
summary(fitted)$call
summary(fitted)$r.squared
summary(fitted)$coefficients
summary(fitted)$residuals
```


We can force the modeling function to keep the intercept to be 0 by adding `-1` to the formula when calling `lm`.


```{r}
fitted <- lm(data = dt, ww ~ xx - 1)
summary(fitted)
```
We see that the "Intercept" row under "Coefficients" has disappeared.
Thus, in the revised model, the estimate is `ww = 25.02464 xx`.
This is very close to the original.

```{r}
summary(fitted)$r.squared
summary(fitted)$coefficients
# summary(fit)$residuals
summary(fitted)$correlation
summary(fitted)$coefficients[1:2]

```

[The UCLA SOCR Statistics](http://www.socr.ucla.edu/) is a great resource for statistical concepts and tools.
[The MLB Height Weight Data Set](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) is the data set of 1,034 Major League Baseball players.
We have downloaded the data and formatted into a comma-separated table.
Let us load the data.
```{r}
mlb <- read_csv("data/mlb.csv")
```

The number of positions is 9 and the number of teams is 30.

```{r}
unique(mlb$Position)
unique(mlb$Team)
```

Let us plot the weight against the height and draw the smoothed line.

```{r}
ggplot(mlb, aes(x = Height, y = Weight)) + geom_point() + geom_smooth()
```
The smoothed plot is the line that connects the mean for each weight value.

Let us use R compute the regression.
The way to use linear regression is `lm(data = DATA_NAME, TARGET ~ FORMULA)`, where `DATA_NAME` is the attribute as the target and `FORMULA` is the factors.
In our case, there is only one factor, `Height`, so we state `Height` in the place for `FORMULA`.

```{r}
fitted <- lm(data = mlb0, Weight ~ Height)
```

We can generate information for the model using `summary()`.

```{r}
summary(fitted)
```

The call produces a variety of values.

```{r}
cor(x = hw$Height, hw$Weight)
summary(fitted)$r.squared
summary(fitted)$coefficients
summary(fitted)$residuals
```


## Exploring the `mtcars`

As usual, our exploration starts with loading the library `tidyverse`.

```{r}
library(tidyverse)
```

Our previous exploration of gas mileage used the `mpg` data set.
This time, we use `mtcars`.
By typing `?mtcars` you get the information of the data set.
Here, for your convenience, the information of the eleven attributes.

*	`mpg	Miles/(US) gallon`
* `cyl	Number of cylinders`
*	`disp	Displacement (cu.in.)`
*	`hp	Gross horsepower`
* `drat	Rear axle ratio`
*	`wt	Weight (1000 lbs)`
*	`qsec	1/4 mile time`
*	`vs	Engine (0 = V-shaped, 1 = straight)`
*	`am	Transmission (0 = automatic, 1 = manual)`
*	`gear	Number of forward gears`
*	`carb	Number of carburetors`

As you can see, all the attributes are numerical.
By `glimpse(mtcars)` you can inspect the attributes.

```{r}
glimpse(mtcars)
```

```{r}
ggplot(mtcars, aes(x = cyl, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = disp, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = drat, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = qsec, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = vs, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = am, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = gear, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = carb, y = mpg)) + geom_point() + geom_smooth()
```
The last four, `vs`, `am`, `gear`, and `carb`, and `cyl` are categorical variables each with a small number of choices, and so we will ignore it.
Let us create a new data set `mtcars1` without these variables, that is, `mpg` and the variables from `disp` to `qsec`.

```{r}
mtcars1 <- mtcars %>% select(mpg, disp:qsec)
head(mtcars1)
```
The *linear regression* is a method that estimates the value of a variable (which we call the *target*) against a collection of other variables (which we the *factors*) so we can express the value of the target as some combination of the factors, where each factor either adds to or subtracts from the target value estimation.
The estimation takes the form
\[
\mathrm{target} \approx \mathrm{intersept} + \mathrm{scale}_1 * \mathrm{factor}_1 + \cdots + \mathrm{scale}_k * \mathrm{factor}_k
\] 


```{r}
ffitted <- lm(mpg ~ disp, data = mtcars1)
summary(fitted)
fitted <- lm(mpg ~ hp, data = mtcars1)
summary(fitted)
fitted <- lm(mpg ~ drat, data = mtcars1)
summary(fitted)
fitted <- lm(mpg ~ wt, data = mtcars1)
summary(fitted)
fitted <- lm(mpg ~ qsec, data = mtcars1)
summary(fitted)
```


```{r}
fitted <- lm(mpg ~., data = mtcars1)
summary(fitted)
```


```{r}
ggplot(mtcars, aes(x = cyl, y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = log(disp), y =mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = log(hp), y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = log(drat), y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = log(wt), y = mpg)) + geom_point() + geom_smooth()
ggplot(mtcars, aes(x = log(qsec), y = mpg)) + geom_point() + geom_smooth()
```


```{r}
fitted <- lm(mpg ~., data = mtcars)
summary(fitted)
```

```{r}
ggplot(aes(x = wt, y = mpg), data = mtcars) +
  geom_point() +
  geom_line(aes(y = 12.30377 - 3.71530 * wt))
```

```{r}
mtcars %>% mutate(mpg_fit = 12.30337 - 3.71530 * wt + 2.52023 * am + 0.78711 * drat + 0.82104 * qsec + 0.65541 * gear) -> mtcars2
```


```{r}
fitted <- lm(mpg ~ mpg_fit, data = mtcars2)
summary(fitted)
```


```{r}
fitted <- lm(mpg ~ wt, data = mtcars)
summary(fitted)
```


```{r}
ggplot(aes(x = wt, y = mpg), data = mtcars) +
  geom_point() +
  geom_line(aes(y = 37.2851 - 5.3445 * wt))
```





Let us use the other car data set.

```{r}
glimpse(mpg)
```

```{r, fig.align="center", dpi=80}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy))
```
Let us compute the correlation values first.

```{r}
cor(mpg$hwy, mpg$displ)
cor(mpg$hwy, mpg$cyl)
cor(mpg$hwy, mpg$year)
```

Let us now compute the regression.

```{r}
fit1 <- lm(hwy ~ displ + year + cyl, data = mpg)
fit2 <- lm(cty ~ displ + year + cyl, data = mpg)
```
```{r}
summary(fit1)
```
```{r}
fit <- lm(hwy~., data = mpg)
summary(fit)
```
<!-- # Predictive Analysis

# Prediction

An important aspect of data science is to find out what data can tell us about the future. What do data about climate and pollution say about temperatures a few decades from now? Based on a person's internet profile, which websites are likely to interest them? How can a patient's medical history be used to judge how well he or she will respond to a treatment?

To answer such questions, data scientists have developed methods for making predictions. In this chapter we will study one of the most commonly used ways of predicting the value of one variable based on the value of another.

The foundations of the method were laid by [Sir Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton). As we saw in Section 7.1, Galton studied how physical characteristics are passed down from one generation to the next. Among his best known work is the prediction of the heights of adults based on the heights of their parents. We have studied the dataset that Galton collected for this. The data frame `galton` contains his data on the midparent height and child's height (all in inches) for a population of 934 adult "children".

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/galton.csv"
galton <- read_csv(url) %>%
  transmute(midparent = midparentHeight, 
            child = childHeight)
galton
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(galton) + 
  geom_point(aes(x = midparent, y = child), color = "darkcyan", alpha = 0.5)
```

The primary reason for collecting the data was to be able to predict the adult height of a child born to parents similar to those in the dataset. We made these predictions in Section 7.1, after noticing the positive association between the two variables.

Our approach was to base the prediction on all the points that correspond to a midparent height of around the midparent height of the new person. To do this, we wrote a function called `predict_child` which takes a midparent height as its argument and returns the average height of all the children who had midparent heights within half an inch of the argument.

```{r}
predict_child <- function(mpht) {
  # Return a prediction of the height of a child 
  # whose parents have a midparent height of mpht.
  # The prediction is the average height of the children 
  # whose midparent height is in the range mpht plus or minus 0.5 inches.
  close_points <- filter(galton, between(midparent, {{ mpht }} -0.5, {{ mpht }} + 0.5))
  return(mean(pull(close_points, child)))
}
```

We applied the function to the column of `midparent` heights, and visualized our results.

```{r}
galton_with_predictions <- galton %>%
  mutate(prediction = map_dbl(midparent, predict_child))
```

```{r dpi=80, fig.align="center", message = FALSE}
# JB: for our own note, discussion on pivot 
# (need to roll this change to earlier sections) https://community.rstudio.com/t/adding-manual-legend-to-ggplot2/41651
plot_df <- galton_with_predictions %>%
  pivot_longer(c(child, prediction), names_to = "measure", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = measure), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

The prediction at a given midparent height lies roughly at the center of the vertical strip of points at the given height. This method of prediction is called *regression*. Later in this chapter we will see where this term came from. We will also see whether we can avoid our arbitrary definitions of "closeness" being "within 0.5 inches". But first we will develop a measure that can be used in many settings to decide how good one variable will be as a predictor of another.

## Correlation

In this section we will develop a measure of how tightly clustered a scatter diagram is about a straight line. Formally, this is called measuring *linear association*.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

In some of the experiments we will be demonstrating, we will be working directly with the population so the corrections R provides with `sd` is unnecessary. We will define a function `pop.sd` and our own version of `scale` called `pop.scale`, as `scale` makes use of `sd`.

```{r}
pop.sd <- function(x) sd(x) * sqrt((length(x)-1) / length(x))
pop.scale <- function(x) (x - mean(x)) / pop.sd(x)
```

### The `hybrid` data frame 

The data frame `hybrid` contains data on hybrid passenger cars sold in the United States from 1997 to 2013. The data were adapted from the online data archive of [Prof. Larry Winner](http://www.stat.ufl.edu/%7Ewinner/) of the University of Florida. The columns:

* `vehicle`: model of the car
* `year`: year of manufacture
* `msrp`: manufacturer's suggested retail price in 2013 dollars
* `acceleration`: acceleration rate in km per hour per second
* `mpg`: fuel econonmy in miles per gallon
* `class`: the model's class.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/hybrid.csv"
hybrid <- read_csv(url)
hybrid
```

The graph below is a scatter plot of `msrp` versus `acceleration`. That means `msrp` is plotted on the vertical axis and `acceleration` on the horizontal.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(hybrid) + 
  geom_point(aes(x = acceleration, y = msrp), color = "darkcyan", alpha = 0.5)
```

Notice the positive association. The scatter of points is sloping upwards, indicating that cars with greater acceleration tended to cost more, on average; conversely, the cars that cost more tended to have greater acceleration on average.

The scatter diagram of MSRP versus mileage shows a negative association. Hybrid cars with higher mileage tended to cost less, on average. This seems surprising till you consider that cars that accelerate fast tend to be less fuel efficient and have lower mileage. As the previous scatter plot showed, those were also the cars that tended to cost more.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(hybrid) + 
  geom_point(aes(x = mpg, y = msrp), color = "darkcyan", alpha = 0.5)
```

Along with the negative association, the scatter diagram of price versus efficiency shows a non-linear relation between the two variables. The points appear to be clustered around a curve, not around a straight line.

If we restrict the data just to the SUV class, however, the association between price and efficiency is still negative but the relation appears to be more linear. The relation between the price and acceleration of SUV's also shows a linear trend, but with a positive slope.

```{r dpi=80, fig.align="center", message = FALSE}
suv <- filter(hybrid, class == 'SUV')
ggplot(suv) + 
  geom_point(aes(x = mpg, y = msrp), color = "darkcyan", alpha = 0.5)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(suv) + 
  geom_point(aes(x = acceleration, y = msrp), color = "darkcyan", alpha = 0.5)
```

You will have noticed that we can derive useful information from the general orientation and shape of a scatter diagram even without paying attention to the units in which the variables were measured.

Indeed, we could plot all the variables in standard units and the plots would look the same. This gives us a way to compare the degree of linearity in two scatter diagrams.

Recall that in an earlier section we applied the built-in function `scale` to convert a vector of numbers to standard units with formula:

$$z = \frac{\text{value} - \text{average}}{\text{SD}}$$

We can use this function to re-draw the two scatter diagrams for SUVs, with all the variables measured in standard units.

```{r dpi=80, fig.align="center", message = FALSE}
suv %>%
  transmute(mpg_standard = scale(mpg),
            msrp_standard = scale(msrp)) %>%
  ggplot() + 
  geom_point(aes(x = mpg_standard, y = msrp_standard), 
             color = "darkcyan", alpha = 0.5)
```

```{r dpi=80, fig.align="center", message = FALSE}
suv %>%
  transmute(acceleration_standard = scale(acceleration),
            msrp_standard = scale(msrp)) %>%
  ggplot() + 
  geom_point(aes(x = acceleration_standard, y = msrp_standard), 
             color = "darkcyan", alpha = 0.5)
```

The associations that we see in these figures are the same as those we saw before. Also, because the two scatter diagrams are now drawn on exactly the same scale, we can see that the linear relation in the second diagram is a little more fuzzy than in the first.

We will now define a measure that uses standard units to quantify the kinds of association that we have seen.

### The correlation coefficient

The *correlation coefficient* measures the strength of the linear relationship between two variables. Graphically, it measures how clustered the scatter diagram is around a straight line.

The term *correlation coefficient* isn't easy to say, so it is usually shortened to *correlation* and denoted by $r$.

Here are some mathematical facts about $r$ that we will just observe by simulation.

* The correlation coefficient $r$ is a number between -1 and 1.
* $r$ measures the extent to which the scatter plot clusters around a straight line.
* $r = 1$  if the scatter diagram is a perfect straight line sloping upwards, and  $r = -1$ if the scatter diagram is a perfect straight line sloping downwards.

The function `r_scatter` takes a value of $r$ as its argument and simulates a scatter plot with a correlation very close to $r$. Because of randomness in the simulation, the correlation is not expected to be exactly equal to  $r$.

Call `r_scatter` a few times, with different values of $r$ as the argument, and see how the scatter plot changes.

When $r = 1$ the scatter plot is perfectly linear and slopes upward. When $r = -1$, the scatter plot is perfectly linear and slopes downward. When $r = 0$, the scatter plot is a formless cloud around the horizontal axis, and the variables are said to be *uncorrelated*.

```{r echo = FALSE}
r_scatter <- function(r) {
  # Generate a scatter plot with a correlation approximately r
  x <- rnorm(10000)
  z <- rnorm(10000)
  y = r*x + (sqrt(1-r**2))*z
  tibble(x = x, y = y) %>%
    ggplot() + 
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
    xlab("") + 
    ylab("")
}
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(0.9)
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(0.25)
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(0)
```

```{r dpi=80, fig.align="center", message = FALSE}
r_scatter(-0.55)
```

### Calculating $r$

The formula for $r$ is not apparent from our observations so far. It has a mathematical basis that is outside the scope of this class. However, as you will see, the calculation is straightforward and helps us understand several of the properties of $r$. Note how we are using `pop.scale` for these steps.

__Formula for $r$__:

> $r$ is the average of the products of the two variables, when both variables are measured in standard units.

Here are the steps in the calculation. We will apply the steps to a simple table of values of $x$ and $y$.

```{r}
t <- tibble(
  x = seq(1, 6),
  y = c(2, 3, 1, 5, 2, 7)
)
t
```

Based on the scatter diagram, we expect that $r$ will be positive but not equal to 1.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(t) + 
  geom_point(aes(x = x, y = y), color = "red")
```

__Step 1.__ Convert each variable to standard units.

```{r}
t_su <- t %>%
  mutate(
    x_standard = pop.scale(x),
    y_standard = pop.scale(y))
t_su
```

__Step 2.__ Multiply each pair of standard units.

```{r}
t_product <- t_su %>%
  mutate(product_standard_units = x_standard * y_standard)
t_product
```

__Step 3.__ $r$ is the average of the products computed in Step 2.

```{r}
# r is the average of the products of standard units
r <- mean(pull(t_product, product_standard_units))
r
```

As expected, $r$ is positive but not equal to 1.

### Properties of $r$

The calculation shows that:

* $r$ is a pure number. It has no units. This is because $r$ is based on standard units.
* $r$ is unaffected by changing the units on either axis. This too is because $r$ is based on standard units.
* $r$ is unaffected by switching the axes. Algebraically, this is because the product of standard units does not depend on which variable is called $x$ and which $y$. Geometrically, switching axes reflects the scatter plot about the line $y = x$, but does not change the amount of clustering nor the sign of the association.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(t) + 
  geom_point(aes(x = y, y = x), color = "red")
```

### The `cor` function 

We are going to be calculating correlations repeatedly, so it will help to have a function that computes it by performing all the steps described above. That function might look like: 

```{r}
correlation <- function(x, y) mean(pop.scale(x) * pop.scale(y))
```

Luckily, R provides such a function for us already called `cor`. The function takes two vectors as input and returns $r$, the mean of the products of those vectors in standard units.

Let's call `cor` on the `x` and `y` columns of `t`. The function returns the same answer to the correlation between $x$ and $y$ as we got by direct application of the formula for $r$.

```{r}
cor(pull(t, x), pull(t, y))
```

As we noticed, the order in which the variables are specified doesn't matter.

```{r}
cor(pull(t, y), pull(t, x))
```

Calling `corr` on columns of the data frame `suv` gives us the correlation between price and mileage as well as the correlation between price and acceleration.

```{r}
cor(pull(suv, mpg), pull(suv, msrp))
```

```{r}
cor(pull(suv, acceleration), pull(suv, msrp))
```

These values confirm what we had observed:
* There is a negative association between price and efficiency, whereas the association between price and acceleration is positive.
* The linear relation between price and acceleration is a little weaker (correlation about 0.5) than between price and mileage (correlation about -0.67).

Correlation is a simple and powerful concept, but it is sometimes misused. Before using $r$, it is important to be aware of what correlation does and does not measure.

### Association is not Causation

Correlation only measures association. Correlation does not imply causation. Though the correlation between the weight and the math ability of children in a school district may be positive, that does not mean that doing math makes children heavier or that putting on weight improves the children's math skills. Age is a confounding variable: older children are both heavier and better at math than younger children, on average.

### Correlation Measures *Linear* Association

Correlation measures only one kind of association – linear. Variables that have strong non-linear association might have very low correlation. Here is an example of variables that have a perfect quadratic relation $y = x^2$ but have correlation equal to 0.

```{r dpi=80, fig.align="center", message = FALSE}
new_x <- seq(-4, 4.1, 0.5)
nonlinear <- tibble(x = new_x, 
                    y = new_x**2)
ggplot(nonlinear) + 
  geom_point(aes(x = x, y = y), color = "red")
```

```{r}
cor(pull(nonlinear, x), pull(nonlinear, y))
```

### Correlation is Affected by Outliers

Outliers can have a big effect on correlation. Here is an example where a scatter plot for which $r$ is equal to 1 is turned into a plot for which $r$ is equal to 0, by the addition of just one outlying point.

```{r dpi=80, fig.align="center", message = FALSE}
line <- tibble(
  x = 1:4, 
  y = 1:4
)
ggplot(line) + 
  geom_point(aes(x = x, y = y), color = "red")
```

```{r}
cor(pull(line, x), pull(line, y))
```

```{r dpi=80, fig.align="center", message = FALSE}
outlier <- tibble(
  x = 1:5, 
  y = c(1, 2, 3, 4, 0)
)
ggplot(outlier) + 
  geom_point(aes(x = x, y = y), color = "red")
```

```{r}
cor(pull(outlier, x), pull(outlier, y))
```

### Ecological Correlations Should be Interpreted with Care

Correlations based on aggregated data can be misleading. As an example, here are data on the Critical Reading and Math SAT scores in 2014. There is one point for each of the 50 states and one for Washington, D.C. The column `Participation Rate` contains the percent of high school seniors who took the test. The next three columns show the average score in the state on each portion of the test, and the final column is the average of the total scores on the test.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/sat2014.csv"
sat2014 <- read_csv(url) %>%
  arrange(State)
sat2014
```

The scatter diagram of Math scores versus Critical Reading scores is very tightly clustered around a straight line; the correlation is close to 0.985.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(sat2014) + 
  geom_point(aes(x = `Critical Reading`, y = Math), alpha = 0.5, color = "darkcyan")
```

```{r}
cor(pull(sat2014, `Critical Reading`), pull(sat2014, Math))
```

That's an extremely high correlation. But it's important to note that this does not reflect the strength of the relation between the Math and Critical Reading scores of *students*.

The data consist of average scores in each state. But states don't take tests – students do. The data in the table have been created by lumping all the students in each state into a single point at the average values of the two variables in that state. But not all students in the state will be at that point, as students vary in their performance. If you plot a point for each student instead of just one for each state, there will be a cloud of points around each point in the figure above. The overall picture will be more fuzzy. The correlation between the Math and Critical Reading scores of the students will be *lower* than the value calculated based on state averages.

Correlations based on aggregates and averages are called *ecological correlations* and are frequently reported. As we have just seen, they must be interpreted with care.

### Serious or tongue-in-cheek? 

In 2012, a [paper](http://www.biostat.jhsph.edu/courses/bio621/misc/Chocolate%20consumption%20cognitive%20function%20and%20nobel%20laurates%20%28NEJM%29.pdf) in the respected New England Journal of Medicine examined the relation between chocolate consumption and Nobel Prizes in a group of countries. The [Scientific American](http://blogs.scientificamerican.com/the-curious-wavefunction/chocolate-consumption-and-nobel-prizes-a-bizarre-juxtaposition-if-there-ever-was-one/) responded seriously whereas [others](http://www.reuters.com/article/2012/10/10/us-eat-chocolate-win-the-nobel-prize-idUSBRE8991MS20121010#vFdfFkbPVlilSjsB.97) were more relaxed. You are welcome to make your own decision! The following graph, provided in the paper, should motivate you to go and take a look.

```{r, echo=FALSE, fig.align="center", out.width='60%'}
knitr::include_graphics('images/Correlation_65_0.png')
```


## The Regression Line

The correlation coefficient $r$ doesn't just measure how clustered the points in a scatter plot are about a straight line. It also helps identify the straight line about which the points are clustered. In this section we will retrace the path that Galton and Pearson took to discover that line.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Galton's data on the heights of parents and their adult children showed a linear association. The linearity was confirmed when our predictions of the children's heights based on the midparent heights roughly followed a straight line.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/galton.csv"
galton <- read_csv(url) %>%
  transmute(midparent = midparentHeight, 
            child = childHeight)
galton
```

```{r}
predict_child <- function(mpht) {
  # Return a prediction of the height of a child 
  # whose parents have a midparent height of mpht.
  # The prediction is the average height of the children 
  # whose midparent height is in the range mpht plus or minus 0.5 inches.
  close_points <- filter(galton, between(midparent, {{ mpht }} - 0.5, {{ mpht }} + 0.5))
  return(mean(pull(close_points, child)))
}
```

```{r}
galton_with_predictions <- galton %>%
  mutate(prediction = map_dbl(midparent, predict_child))
```

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton_with_predictions %>%
  pivot_longer(c(child, prediction), names_to = "measure", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = measure), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

### Measuring in Standard Units

Let's see if we can find a way to identify this line. First, notice that linear association doesn't depend on the units of measurement – we might as well measure both variables in standard units.

```{r}
galton_su <- galton %>%
  transmute(midparent_su = scale(midparent),
            child_su = scale(child))
galton_su
```

On this scale, we can calculate our predictions exactly as before. But first we have to figure out how to convert our old definition of "close" points to a value on the new scale. We had said that midparent heights were "close" if they were within 0.5 inches of each other. Since standard units measure distances in units of SDs, we have to figure out how many SDs of midparent height correspond to 0.5 inches.

One SD of midparent heights is about 1.8 inches. So 0.5 inches is about 0.28 SDs.

```{r}
sd_midparent <- sd(pull(galton, midparent))
sd_midparent
```

```{r}
0.5/sd_midparent
```

We are now ready to modify our prediction function to make predictions on the standard units scale. All that has changed is that we are using the table of values in standard units, and defining "close" as above.

```{r}
predict_child_su <- function(mpht_su) {
  close = 0.5/sd_midparent
  # Return a prediction of the height (in standard units) of a child 
  # whose parents have a midparent height of mpht_su in standard units.
  close_points <- filter(galton_su, 
                         between(midparent_su, {{ mpht_su }} - close, {{ mpht_su }} + close))
  return(mean(pull(close_points, child_su)))
}
```

```{r}
galton_with_su_predictions <- galton_su %>%
  mutate(prediction_su = map_dbl(midparent_su, predict_child_su))
```

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton_with_su_predictions %>%
  pivot_longer(c(child_su, prediction_su), names_to = "measure", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent_su, y = height, color = measure), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon"))
```

This plot looks exactly like the plot drawn on the original scale. Only the numbers on the axes have changed. This confirms that we can understand the prediction process by just working in standard units.

### Identifying the Line in Standard Units

Galton's scatter plot has a *football* shape – that is, it is roughly oval like an American football. Not all scatter plots are football shaped, not even those that show linear association. But in this section we will pretend we are Galton and work only with football shaped scatter plots. In the next section, we will generalize our analysis to other shapes of plots.

Here is a football shaped scatter plot with both variables measured in standard units. The 45 degree line is shown in red.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
r <- 0.5
x <- rnorm(10000)
z <- rnorm(10000)
y = r*x + (sqrt(1-r**2))*z
tibble(x = x, y = y) %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
  geom_line(data = data.frame(x = c(-4,4), y = c(-4,4)),
            aes(x = x, y = y), color = "salmon", size = 1) + 
  xlab("x in standard units") + 
  ylab("y in standard units")
```

But the 45 degree line is not the line that picks off the centers of the vertical strips. You can see that in the figure below, where the vertical line at 1.5 standard units is shown in black. The points on the scatter plot near the black line all have heights roughly in the -2 to 3 range. The red line is too high to pick off the center.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
r <- 0.5
x <- rnorm(10000)
z <- rnorm(10000)
y = r*x + (sqrt(1-r**2))*z
tibble(x = x, y = y) %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
  geom_line(data = data.frame(x = c(-4,4), y = c(-4,4)),
            aes(x = x, y = y), color = "salmon", size = 1) + 
  geom_line(data = data.frame(x = c(1.5,1.5), y = c(-4,4)),
            aes(x = x, y = y), color = "black", size = 1) + 
  xlab("x in standard units") + 
  ylab("y in standard units")
```

So the 45 degree line is not the "graph of averages." That line is the yellow one shown below.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
r <- 0.5
x <- rnorm(10000)
z <- rnorm(10000)
y = r*x + (sqrt(1-r**2))*z
tibble(x = x, y = y) %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
  geom_line(data = data.frame(x = c(-4,4), y = c(-4,4)),
            aes(x = x, y = y), color = "salmon", size = 1) + 
  geom_line(data = data.frame(x = c(1.5,1.5), y = c(-4,4)),
            aes(x = x, y = y), color = "black", size = 1) + 
  geom_line(data = data.frame(x = c(-4, 4), y = c(-4*0.6,4*0.6)),
            aes(x = x, y = y), color = "khaki", size = 1) +
  xlab("x in standard units") + 
  ylab("y in standard units")
```

Both lines go through the origin (0, 0). The yellow line goes through the centers of the vertical strips (at least roughly), and is *flatter* than the red 45 degree line.

The slope of the 45 degree line is 1. So the slope of the yellow "graph of averages" line is a value that is positive but less than 1.

What value could that be? You've guessed it – it's $r$.

### The Regression Line, in Standard Units

The yellow "graph of averages" line is called the *regression line*, for reasons we will explain shortly. But first, let's simulate some football shaped scatter plots with different values of $r$, and see how the line changes. In each case, the red 45 degree line has been drawn for comparison.

The function that performs the simulation is called `regression_line` and takes $r$ as its argument.

```{r echo = FALSE}
regression_line <- function(r) {
  x = rnorm(10000)
  z = rnorm(10000)
  y = r*x + (sqrt(1-r**2))*z
  if (r >= 0) {
    y_pts <- c(-4,4)
  } else {
    y_pts <- c(4,-4)
  }
  tibble(x = x, y = y) %>%
    ggplot() +
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.8) + 
    geom_line(data = data.frame(x = c(-4, 4), y = c(-4*r,4*r)),
              aes(x = x, y = y), color = "khaki", size = 1) +
    geom_line(data = data.frame(x = c(-4, 4), y = y_pts),
              aes(x = x, y = y), color = "salmon", size = 1)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
regression_line(0.95)
```

```{r dpi=80, fig.align="center", message = FALSE}
regression_line(0.6)
```

When $r$ is close to 1, the scatter plot, the 45 degree line, and the regression line are all very close to each other. But for more moderate values of $r$, the regression line is noticeably flatter.

### The Regression Effect

In terms of prediction, this means that for a parent whose midparent height is at 1.5 standard units, our prediction of the child's height is somewhat *less* than 1.5 standard units. If the midparent height is 2 standard units, we predict that the child's height will be somewhat less than 2 standard units.

In other words, we predict that the child will be somewhat closer to average than the parents were.

This didn't please Sir Francis Galton. He had been hoping that exceptionally tall parents would have children who were just as exceptionally tall. However, the data were clear, and Galton realized that the tall parents have children who are not quite as exceptionally tall, on average. Frustrated, Galton called this phenomenon "regression to mediocrity." 

Galton also noticed that exceptionally short parents had children who were somewhat taller relative to their generation, on average. In general, individuals who are away from average on one variable are expected to be not quite as far away from average on the other. This is called the *regression effect*.

### The Equation of the Regression Line

In regression, we use the value of one variable (which we will call $x$) to predict the value of another (which we will call $y$). When the variables $x$ and $y$ are measured in standard units, the regression line for predicting $y$ based on $x$ has slope $r$ and passes through the origin. Thus the equation of the regression line can be written as:

$$
\mbox{estimate of }y ~=~ r \cdot x ~~~
\mbox{when both variables are measured in standard units}
$$

In the original units of the data, this becomes

$$
\frac{\mbox{estimate of}~y ~-~\mbox{average of}~y}{\mbox{SD of}~y}
~=~ r \times 
\frac{\mbox{the given}~x ~-~\mbox{average of}~x}{\mbox{SD of}~x}
$$

```{r, echo=FALSE, fig.align="center", out.width='60%'}
knitr::include_graphics('images/regline.png')
```

The slope and intercept of the regression line in original units can be derived from the diagram above. 

$$
\mathbf{\mbox{slope of the regression line}} ~=~ r \cdot
\frac{\mbox{SD of }y}{\mbox{SD of }x}
$$

$$
\mathbf{\mbox{intercept of the regression line}} ~=~
\mbox{average of }y ~-~ \mbox{slope} \cdot \mbox{average of }x
$$

The three functions below compute the correlation, slope, and intercept. All of them take three arguments: the name of the table, the label of the column containing $x$ , and the label of the column containing $y$.

```{r}
slope <- function(t, label_x, label_y) {
  r <- cor(pull(t, {{ label_x }}), pull(t, {{ label_y }}) )
  return (r * sd(pull(t, {{ label_y }})) / sd(pull(t, {{ label_x }})))
}

intercept <- function(t, label_x, label_y) {
  return(mean(pull(t, {{ label_y }})) - 
           slope(t, {{ label_x }}, {{ label_y }}) * mean(pull(t, {{ label_x }})))
}
```

### The Regression Line and Galton's Data

The correlation between midparent height and child's height is 0.32:

```{r}
galton_r <- cor(pull(galton, midparent), pull(galton, child))
galton_r
```

We can also find the equation of the regression line for predicting the child's height based on midparent height.

```{r}
galton_slope <- slope(galton, midparent, child)
galton_intercept <- intercept(galton, midparent, child)
c(galton_slope, galton_intercept)
```

The equation of the regression line is

$$
\mbox{estimate of child's height} ~=~ 0.64 \cdot \mbox{midparent height} ~+~ 22.64
$$

This is also known as the *regression equation.* The principal use of the regression equation is to predict $y$ based on $x$.

For example, for a midparent height of 70.48 inches, the regression equation predicts the child's height to be 67.56 inches.

```{r}
galton_slope*70.48 + galton_intercept
```

Our original prediction, created by taking the average height of all children who had midparent heights close to 70.48, came out to be pretty close: 67.63 inches compared to the regression line's prediction of 67.55 inches.

```{r}
filter(galton_with_predictions, midparent == 70.48) %>%
  head(3)
```

Here are all of the rows in Galton's table, along with our original predictions and the new regression predictions of the children's heights.

```{r}
galton_with_predictions <- galton_with_predictions %>%
  mutate(regression_prediction = galton_slope * midparent + galton_intercept)
galton_with_predictions
```

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton_with_predictions %>%
  pivot_longer(c(child, prediction, regression_prediction), 
               names_to = "attribute", values_to = "height")

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = height, color = attribute), alpha = 0.5) + 
  scale_color_manual(values = c("darkcyan", "salmon", "blue"))
```

The blue dots show the regression predictions, all on the regression line. Notice how the line is very close to the salmon pink graph of averages. For these data, the regression line does a good job of approximating the centers of the vertical strips.

### Fitted Values

The predictions all lie on the line and are known as the "fitted values". The function `fit` takes the name of the table and the labels of $x$ and $y$, and returns an array of fitted values, one fitted value for each point in the scatter plot.

```{r}
fit <- function(df, x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(df, {{ x }}, {{ y }})
  b <- intercept(df, {{ x }}, {{ y }})
  return(a * pull(df, {{ x }}) + b)
}

```

It is easier to see the line in the graph below than in the one above.

```{r dpi=80, fig.align="center", message = FALSE}
plot_df <- galton %>% 
  mutate(
    fitted = fit(galton, midparent, child)
  )

ggplot(plot_df) + 
  geom_point(aes(x = midparent, y = child), color = "darkcyan", alpha = 0.5) + 
  geom_line(aes(x = midparent, y = fitted), color = "salmon", size = 1)
```

Another way to draw the line (and perhaps easier) is to add a `geom_smooth` and setting its method argument to `lm`, which is short for *l*inear *m*odel. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(plot_df, aes(x = midparent, y = child)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", color = "salmon")
```

### Units of Measurement of the Slope

The slope is a ratio, and it worth taking a moment to study the units in which it is measured. Our example comes from the familiar dataset about mothers who gave birth in a hospital system. The scatter plot of pregnancy weights versus heights looks like a football that has been used in one game too many, but it's close enough to a football that we can justify putting our fitted line through it. In later sections we will see how to make such justifications more formal.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Maternal Height`, y = `Maternal Pregnancy Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", color = "salmon")
```

```{r}
slope(baby, `Maternal Height`, `Maternal Pregnancy Weight`)
```

The slope of the regression line is **3.57 pounds per inch**. This means that for two women who are 1 inch apart in height, our prediction of pregnancy weight will differ by 3.57 pounds. For a woman who is 2 inches taller than another, our prediction of pregnancy weight will be 
$$
2 \times 3.57 ~=~ 7.14
$$
pounds more than our prediction for the shorter woman.

Notice that the successive vertical strips in the scatter plot are one inch apart, because the heights have been rounded to the nearest inch. Another way to think about the slope is to take any two consecutive strips (which are necessarily 1 inch apart), corresponding to two groups of women who are separated by 1 inch in height. The slope of 3.57 pounds per inch means that the average pregnancy weight of the taller group is about 3.57 pounds more than that of the shorter group.

### Example

Suppose that our goal is to use regression to estimate the height of a basset hound based on its weight, using a sample that looks consistent with the regression model. Suppose the observed correlation $r$ is 0.5, and that the summary statistics for the two variables are as in the table below: 

|       |**average**    |**SD**     |   
|------:|:-------------:|:---------:|
|height |14 inches      |2 inches   |
|weight |50 pounds      |5 pounds   |

To calculate the equation of the regression line, we need the slope and the intercept.

$$
\mbox{slope} ~=~ \frac{r \cdot \mbox{SD of }y}{\mbox{SD of }x} ~=~
\frac{0.5 \cdot 2 \mbox{ inches}}{5 \mbox{ pounds}} ~=~ 0.2 ~\mbox{inches per pound}
$$


$$
\mbox{intercept} ~=~ \mbox{average of }y - \mbox{slope}\cdot \mbox{average of } x
~=~ 14 \mbox{ inches} ~-~ 0.2 \mbox{ inches per pound} \cdot 50 \mbox{ pounds}
~=~ 4 \mbox{ inches}
$$

The equation of the regression line allows us to calculate the estimated height, in inches,
based on a given weight in pounds:

$$
\mbox{estimated height} ~=~ 0.2 \cdot \mbox{given weight} ~+~ 4
$$

The slope of the line measures the increase in the estimated height per unit increase in weight. The slope is positive, and it is important to note that this does not mean that we think basset hounds get taller if they put on weight. The slope reflects the difference in the average heights of two groups of dogs that are 1 pound apart in weight. Specifically, consider a group of dogs whose weight is $w$ pounds, and the group whose weight is $w+1$ pounds. The second group is estimated to be 0.2 inches taller, on average. This is true for all values of $w$ in the sample.

In general, the slope of the regression line can be interpreted as the average increase in $y$ per unit increase in $x$. Note that if the slope is negative, then for every unit increase in $x$, the average of $y$ decreases.

### Endnote

Even though we won't establish the mathematical basis for the regression equation, we can see that it gives pretty good predictions when the scatter plot is football shaped. It is a surprising mathematical fact that no matter what the shape of the scatter plot, the same equation gives the "best" among all straight lines. That's the topic of the next section.

## The Method of Least Squares

We have retraced the steps that Galton and Pearson took to develop the equation of the regression line that runs through a football shaped scatter plot. But not all scatter plots are football shaped, not even linear ones. Does every scatter plot have a "best" line that goes through it? If so, can we still use the formulas for the slope and intercept developed in the previous section, or do we need new ones?

To address these questions, we need a reasonable definition of "best". Recall that the purpose of the line is to *predict* or *estimate* values of $y$, given values of $x$. Estimates typically aren't perfect. Each one is off the true value by an *error*. A reasonable criterion for a line to be the "best" is for it to have the smallest possible overall error among all straight lines.

In this section we will make this criterion precise and see if we can identify the best straight line under the criterion.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Our first example is a dataset that has one row for every chapter of the novel "Little Women." The goal is to estimate the number of characters (that is, letters, spaces punctuation marks, and so on) based on the number of periods. Recall that we attempted to do this in the very first lecture of this course.

```{r message = FALSE}
little_women <- read_csv("data/little_women.csv")
head(little_women, 3)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(little_women) + 
  geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5)
```

To explore the data, we will need to use the functions `slope`, `intercept`, and `fit` we defined in the previous section.

```{r}
slope <- function(t, label_x, label_y) {
  r <- cor(pull(t, {{ label_x }}), pull(t, {{ label_y }}) )
  return (r * sd(pull(t, {{ label_y }})) / sd(pull(t, {{ label_x }})))
}

intercept <- function(t, label_x, label_y) {
  return(mean(pull(t, {{ label_y }})) - 
           slope(t, {{ label_x }}, {{ label_y }}) * mean(pull(t, {{ label_x }})))
}

fit <- function(df, x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(df, {{ x }}, {{ y }})
  b <- intercept(df, {{ x }}, {{ y }})
  return(a * pull(df, {{ x }}) + b)
}
```

Recall that we can check the correlation in the data using `cor`. 

```{r}
cor(pull(little_women, Periods), pull(little_women, Characters))
```

The scatter plot is remarkably close to linear, and the correlation is more than 0.92.

### Error in Estimation

The graph below shows the scatter plot and line that we developed in the previous section. We don't yet know if that's the best among all lines. We first have to say precisely what "best" means.

```{r dpi=80, fig.align="center", message = FALSE}
lw_with_predictions <- little_women %>% 
  mutate(fitted = fit(little_women, Periods, Characters))

ggplot(lw_with_predictions) + 
  geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5) + 
  geom_line(aes(x = Periods, y = fitted), color = "salmon")
```

Corresponding to each point on the scatter plot, there is an error of prediction calculated as the actual value minus the predicted value. It is the vertical distance between the point and the line, with a negative sign if the point is below the line.

```{r}
lw_with_predictions <- lw_with_predictions %>%
  mutate(error = Characters - fitted)
lw_with_predictions
```

We can use `slope` and `intercept` to calculate the slope and intercept of the fitted line. The ggplot below shows the line (in blue). The errors corresponding to the points are shown in red. 

```{r}
lw_reg_slope <- slope(little_women, Periods, Characters)
lw_reg_intercept <- intercept(little_women, Periods, Characters)

paste('Slope of Regression Line: ', round(lw_reg_slope), ' characters per period')
paste('Intercept of Regression Line: ', round(lw_reg_intercept), ' characters')
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(lw_with_predictions) + 
  geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5) + 
  geom_line(aes(x = Periods, y = fitted), color = "blue") +
  geom_segment(aes(x = Periods, y = Characters,
                   xend = Periods, yend = fitted), color = "pink")
```

Let's package this ggplot call into a function we can play with. 

```{r}
plot_lw_errors <- function(lw_df, slope, intercept) {
  ggplot(lw_df) + 
    geom_point(aes(x = Periods, y = Characters), color = "darkcyan", alpha = 0.5) + 
    geom_line(aes(x = Periods, y = slope * Periods + intercept), color = "blue") +
    geom_segment(aes(x = Periods, y = Characters,
                     xend = Periods, yend = slope * Periods + intercept), color = "pink")
}
```

```{r dpi=80, fig.align="center", message = FALSE}
little_women %>%
  plot_lw_errors(lw_reg_slope, lw_reg_intercept)
```

Had we used a different line to create our estimates, the errors would have been different. The graph below shows how big the errors would be if we were to use another line for estimation. The second graph shows large errors obtained by using a line that is downright silly.

```{r dpi=80, fig.align="center", message = FALSE}
little_women %>%
  plot_lw_errors(50, 10000)
```

```{r dpi=80, fig.align="center", message = FALSE}
little_women %>%
  plot_lw_errors(-100, 50000)
```

### Root Mean Squared Error

What we need now is one overall measure of the rough size of the errors. You will recognize the approach to creating this – it's exactly the way we developed the SD.

If you use any arbitrary line to calculate your estimates, then some of your errors are likely to be positive and others negative. To avoid cancellation when measuring the rough size of the errors, we will take the mean of the squared errors rather than the mean of the errors themselves.

The mean squared error of estimation is a measure of roughly how big the squared errors are, but as we have noted earlier, its units are hard to interpret. Taking the square root yields the root mean square error (rmse), which is in the same units as the variable being predicted and therefore much easier to understand.

### Minimizing the Root Mean Squared Error

Our observations so far can be summarized as follows.

- To get estimates of $y$ based on $x$, you can use any line you want.
- Every line has a root mean squared error of estimation.
- "Better" lines have smaller errors.

Is there a "best" line? That is, is there a line that minimizes the root mean squared error among all lines? 

To answer this question, we will start by defining a function `lw_rmse` to compute the root mean squared error of any line through the Little Women scatter diagram. The function takes the slope and the intercept (in that order) as its arguments.

```{r}
lw_rmse <- function(slope, intercept) {
  g <- little_women %>%
    plot_lw_errors(slope, intercept)
  x = pull(little_women, Periods)
  y = pull(little_women, Characters)
  fitted = slope * x + intercept
  mse = mean((y - fitted) ** 2)
  print(g)
  print(paste("Root mean squared error:", mse ** 0.5))
}
```

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(50, 10000)
```

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(-100, 50000)
```

Bad lines have big values of rmse, as expected. But the rmse is much smaller if we choose a slope and intercept close to those of the regression line.

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(90, 4000)
```

Here is the root mean squared error corresponding to the regression line. By a remarkable fact of mathematics, no other line can beat this one. 

> The regression line is the unique straight line that minimizes the mean squared error of estimation among all straight lines.

```{r dpi=80, fig.align="center", message = FALSE}
lw_rmse(lw_reg_slope, lw_reg_intercept)
```

The proof of this statement requires abstract mathematics that is beyond the scope of this course. On the other hand, we do have a powerful tool – R – that performs large numerical computations with ease. So we can use R to confirm that the regression line minimizes the mean squared error.

### Numerical Optimization

First note that a line that minimizes the root mean squared error is also a line that minimizes the squared error. The square root makes no difference to the minimization. So we will save ourselves a step of computation and just minimize the mean squared error (mse).

We are trying to predict the number of characters ($y$) based on the number of periods ($x$) in chapters of Little Women. If we use the line 
$$
\mbox{prediction} ~=~ ax + b
$$
it will have an mse that depends on the slope $a$ and the intercept $b$. The function `lw_mse` takes a `params` vector as its argument where its first element is the slope and its second element the intercept, and returns the corresponding mse.

```{r}
lw_mse <- function(params) {
  any_slope <- params[1]
  any_intercept <- params[2]
  
  x = pull(little_women, Periods)
  y = pull(little_women, Characters)
  fitted = any_slope * x + any_intercept
  return(mean((y - fitted) ** 2))
}
```

Let's check that `lw_mse` gets the right answer for the root mean squared error of the regression line. Remember that `lw_mse` returns the mean squared error, so we have to take the square root to get the rmse.

```{r}
params <- c(lw_reg_slope, lw_reg_intercept)
lw_mse(params)**0.5
```

That's the same as the value we got by using `lw_rmse` earlier. You can confirm that `lw_mse` returns the correct value for other slopes and intercepts too. For example, here is the rmse of the extremely bad line that we tried earlier.

```{r}
params <- c(-100, 50000)
lw_mse(params)**0.5
```

And here is the rmse for a line that is close to the regression line.

```{r}
params <- c(90, 4000)
lw_mse(params)**0.5
```

If we experiment with different values, we can find a low-error pair of slope and intercept through trial and error, but that would take a while. Fortunately, there is an R function that does all the trial and error for us.

The `optim` function can be used to find the values in a vector for which the function returns its minimum value. R uses a similar trial-and-error approach, following the changes that lead to incrementally lower output values.

`optim` takes as arguments an initial vector of values, let's call it the `initial_guess` vector, and a function to be minimized using `initial_guess`. For example, the function `lw_mse` takes a vector `params` where the first element is a slope and the second element an intercept, and returns the corresponding mse.  

The call `optim(initial_guess, lw_mse)` will nudge around the values of slope and intercept inside `initial_guess` until a minimum is reached. At that point, a vector is returned consisting of the slope and intercept that minimize the mse. These minimizing values are excellent approximations arrived at by intelligent trial-and-error, not exact values based on formulas.

```{r}
initial_guess <- c(-100,50000)  # start with a very bad line! 
best <- optim(initial_guess, lw_mse)
best$par
```

These values are the same as the values we calculated earlier by using the `slope` and `intercept` functions. We see small deviations due to the inexact nature of `minimize`, but the values are essentially the same.

```{r}
paste("slope from formula: ", lw_reg_slope)
paste("slope from minimize: ", best$par[1])
paste("intercept from formula: ", lw_reg_intercept)
paste("intercept from minimize: ", best$par[2])

```


### The Least Squares Line

Therefore, we have found not only that the regression line minimizes mean squared error, but also that minimizing mean squared error gives us the regression line. The regression line is the only line that minimizes mean squared error.

That is why the regression line is sometimes called the "least squares line."


## Least Squares Regression

In an earlier section, we developed formulas for the slope and intercept of the regression line through a *football shaped* scatter diagram. It turns out that the slope and intercept of the least squares line have the same formulas as those we developed, *regardless of the shape of the scatter plot*.

We saw this in the example about Little Women, but let's confirm it in an example where the scatter plot clearly isn't football shaped. For the data, we are once again indebted to the rich [data archive of Prof. Larry Winner](http://www.stat.ufl.edu/~winner/datasets.html) of the University of Florida. A [2013 study](http://digitalcommons.wku.edu/ijes/vol6/iss2/10/) in the International Journal of Exercise Science studied collegiate shot put athletes and examined the relation between strength and shot put distance. The population consists of 28 female collegiate athletes. Strength was measured by the biggest amount (in kilograms) that the athlete lifted in the "1RM power clean" in the pre-season. The distance (in meters) was the athlete's personal best.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

To explore the data, we will need our functions `slope`, `intercept`, and `fit`.

```{r}
slope <- function(t, label_x, label_y) {
  r <- cor(pull(t, {{ label_x }}), pull(t, {{ label_y }}) )
  return (r * sd(pull(t, {{ label_y }})) / sd(pull(t, {{ label_x }})))
}

intercept <- function(t, label_x, label_y) {
  return(mean(pull(t, {{ label_y }})) - 
           slope(t, {{ label_x }}, {{ label_y }}) * mean(pull(t, {{ label_x }})))
}

fit <- function(df, x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(df, {{ x }}, {{ y }})
  b <- intercept(df, {{ x }}, {{ y }})
  return(a * pull(df, {{ x }}) + b)
}
```

### The `shotput` data frame

```{r message=FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/shotput.csv"
shotput <- read_csv(url)
shotput
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(shotput) +
  geom_point(aes(x = `Weight Lifted`, y = `Shot Put Distance`), 
             color = "darkcyan", alpha = 0.8)
```

That's not a football shaped scatter plot. In fact, it seems to have a slight non-linear component. But if we insist on using a straight line to make our predictions, there is still one best straight line among all straight lines.

Our formulas for the slope and intercept of the regression line, derived for football shaped scatter plots, give the following values.

```{r}
slope(shotput, `Weight Lifted`, `Shot Put Distance`)
intercept(shotput, `Weight Lifted`, `Shot Put Distance`)
```

Does it still make sense to use these formulas even though the scatter plot isn't football shaped? We can answer this by finding the slope and intercept of the line that minimizes the mse.

We will define the function `shotput_linear_mse` to take a `params` vector argument containing an arbirtary slope and intercept, and return the corresponding mse. Then `optim` applied to `shotput_linear_mse` will return the best slope and intercept.

```{r}
shotput_linear_mse <- function(params) {
  any_slope <- params[1]
  any_intercept <- params[2]
  
  x = pull(shotput, `Weight Lifted`)
  y = pull(shotput, `Shot Put Distance`)
  fitted = any_slope * x + any_intercept
  return(mean((y - fitted) ** 2))
}
```

```{r}
initial_guess <- c(0,0)  # start with some line with intercept 0 and slope 0
best <- optim(initial_guess, shotput_linear_mse)
best$par
```

These values are the same as those we got by using our formulas. To summarize:

>No matter what the shape of the scatter plot, there is a unique line that minimizes the mean squared error of estimation. It is called the regression line, and its slope and intercept are given by $$
\mathbf{\mbox{slope of the regression line}} ~=~ r \cdot
\frac{\mbox{SD of }y}{\mbox{SD of }x}
$$
>$$
\mathbf{\mbox{intercept of the regression line}} ~=~
\mbox{average of }y ~-~ \mbox{slope} \cdot \mbox{average of }x
$$


```{r dpi=80, fig.align="center", message = FALSE}
shotput_with_predictions <- shotput %>% 
  mutate(fitted = fit(shotput, `Weight Lifted`, `Shot Put Distance`))

ggplot(shotput_with_predictions) + 
  geom_point(aes(x = `Weight Lifted`, y = `Shot Put Distance`), 
             color = "darkcyan", alpha = 0.8) + 
  geom_line(aes(x = `Weight Lifted`, y = fitted), color = "salmon")
```

### Nonlinear Regression

The graph above reinforces our earlier observation that the scatter plot is a bit curved. So it is better to fit a curve than a straight line. The [study](http://digitalcommons.wku.edu/ijes/vol6/iss2/10/) postulated a quadratic relation between the weight lifted and the shot put distance. So let's use quadratic functions as our predictors and see if we can find the best one. 

We have to find the best quadratic function among all quadratic functions, instead of the best straight line among all straight lines. The method of least squares allows us to do this.

The mathematics of this minimization is complicated and not easy to see just by examining the scatter plot. But numerical minimization is just as easy as it was with linear predictors! We can get the best quadratic predictor by once again using `optim`. Let's see how this works.

Recall that a quadratic function has the form

$$
f(x) ~=~ ax^2 + bx + c
$$
for constants $a$, $b$, and $c$.

To find the best quadratic function to predict distance based on weight lifted, using the criterion of least squares, we will first write a function that takes the three constants as its arguments, calculates the fitted values by using the quadratic function above, and then returns the mean squared error. 

The function is called `shotput_quadratic_mse`. Notice that the definition is analogous to that of `lw_mse`, except that the fitted values are based on a quadratic function instead of linear.

```{r}
shotput_quadratic_mse <- function(params) {
  a <- params[1]
  b <- params[2]
  c <- params[3]
  
  x = pull(shotput, `Weight Lifted`)
  y = pull(shotput, `Shot Put Distance`)
  fitted = a*(x**2) + b*x + c
  return(mean((y - fitted) ** 2))
}
```

We can now use `optim` just as before to find the constants that minimize the mean squared error. 

```{r}
initial_guess <- c(0,0,0)  # start with some line with a = 0, b = 0, c = 0
best <- optim(initial_guess, shotput_quadratic_mse)
best$par
```

Our prediction of the shot put distance for an athlete who lifts $x$ kilograms is about
$$
-0.00104x^2 ~+~ 0.2829x - 1.537
$$
meters. For example, if the athlete can lift 100 kilograms, the predicted distance is 16.33 meters. On the scatter plot, that's near the center of a vertical strip around 100 kilograms.

```{r}
(-0.00104)*(100**2) + 0.2829*100 - 1.537
```

Here are the predictions for all the values of Weight Lifted. You can see that they go through the center of the scatter plot, to a rough approximation.

```{r}
a <- best$par[1]
b <- best$par[2]
c <- best$par[3]
```

```{r dpi=80, fig.align="center", message = FALSE}
shotput_with_predictions <- shotput %>% 
  mutate(fitted = a * `Weight Lifted`^2 + b * `Weight Lifted` + c)

ggplot(shotput_with_predictions) + 
  geom_point(aes(x = `Weight Lifted`, y = `Shot Put Distance`), 
             color = "darkcyan", alpha = 0.8) + 
  geom_line(aes(x = `Weight Lifted`, y = fitted), color = "salmon")
```

## Visual Diagnostics

Suppose a data scientist has decided to use linear regression to estimate values of one variable (called the response variable) based on another variable (called the predictor). To see how well this method of estimation performs, the data scientist must measure how far off the estimates are from the actual values. These differences are called *residuals*.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

The calculations in this section assume all the relevant functions we have already defined: `slope`, `intercept`, and `fit`.

```{r}
slope <- function(label_x, label_y) {
  r <- cor(label_x,  label_y)
  return (r * sd(label_y) / sd(label_x))
}

intercept <- function(label_x, label_y) {
  return(mean(label_y) - 
           slope(label_x, label_y) * mean(label_x))
}

fit <- function(x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(x, y)
  b <- intercept(x, y)
  return(a * x + b)
}
```

We will be revisiting the `galton` dataset in this section, so let's load that in as well.

```{r message=FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/galton.csv"
galton <- read_csv(url) %>%
  transmute(midparent = midparentHeight, 
            child = childHeight)
galton
```

### The residual 

$$
\mbox{residual} ~=~ \mbox{observed value} ~-~ \mbox{regression estimate}
$$

A residual is what's left over – the residue – after estimation. 

Residuals are the vertical distances of the points from the regression line. There is one residual for each point in the scatter plot. The residual is the difference between the observed value of $y$ and the fitted value of $y$, so for the point $(x, y)$,

$$
\mbox{residual} ~~ = ~~ y ~-~
\mbox{fitted value of }y
~~ = ~~ y ~-~
\mbox{height of regression line at }x
$$

The function `residual` calculates the residuals. 

```{r}
residual <- function(x, y) {
  return(y - fit(x, y))
}
```

Continuing our example of using Galton's data to estimate the heights of adult children (the response) based on the midparent height (the predictor), let us calculate the fitted values and the residuals.

```{r}
galton <- galton %>% 
  mutate(fitted = fit(midparent, child),
         residual = residual(midparent, child))
```

When there are so many variables to work with, it is always helpful to start with visualization. The function `scatter_fit` draws the scatter plot of the data, as well as the regression line.

```{r}
scatter_fit <- function(df, x, y) {
  ggplot(df) +
    geom_point(aes(x = {{x}}, y = {{y}}), color = "darkcyan", alpha = 0.8) +
    geom_line(aes(x = {{x}}, y = fit({{x}}, {{y}})), color = "salmon", size = 1)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
galton %>%
  scatter_fit(midparent, child)
```

A *residual plot* can be drawn by plotting the residuals against the predictor variable. The function `residual_plot` does just that.

```{r}
residual_plot <- function(df, x, y) {
  ggplot(df) +
    geom_point(aes(x = {{x}}, y = residual({{x}}, {{y}})), 
               color = "red", alpha = 0.8) +
    geom_hline(yintercept = 0, color = "dark blue", size = 1)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
galton %>%
  residual_plot(midparent, child)
```

The midparent heights are on the horizontal axis, as in the original scatter plot. But now the vertical axis shows the residuals. Notice that the plot appears to be centered around the horizontal line at the level 0 (shown in dark blue). Notice also that the plot shows no upward or downward trend. We will observe later that this is true of all regressions.

### Regression Diagnostics

Residual plots help us make visual assessments of the quality of a linear regression analysis. Such assessments are called *diagnostics*. The function `regression_diagnostic_plots` draws the original scatter plot as well as the residual plot for ease of comparison.

```{r}
regression_diagnostic_plots <- function(df, x, y) {
  print(scatter_fit(df, {{x}}, {{y}}))
  print(residual_plot(df, {{x}}, {{y}}))
}
```

```{r dpi=80, fig.align="center", message = FALSE}
regression_diagnostic_plots(galton, midparent, child)
```

This residual plot indicates that linear regression was a reasonable method of estimation. Notice how the residuals are distributed fairly symmetrically above and below the horizontal line at 0, corresponding to the original scatter plot being roughly symmetrical above and below. Notice also that the vertical spread of the plot is fairly even across the most common values of the children's heights. In other words, apart from a few outlying points, the plot isn't narrower in some places and wider in others.

In other words, the accuracy of the regression appears to be about the same across the observed range of the predictor variable.

> The residual plot of a good regression shows no pattern. The residuals look about the same, above and below the horizontal line at 0, across the range of the predictor variable.

### Detecting Nonlinearity

Drawing the scatter plot of the data usually gives an indication of whether the relation between the two variables is non-linear. Often, however, it is easier to spot non-linearity in a residual plot than in the original scatter plot. This is usually because of the scales of the two plots: the residual plot allows us to zoom in on the errors and hence makes it easier to spot patterns.

```{r, echo=FALSE, fig.align="center", out.width='60%'}
knitr::include_graphics('images/Dugong_dugon.jpg')
```

Our data are a [dataset](http://www.statsci.org/data/oz/dugongs.html)  on the age and length of dugongs, which are marine mammals related to manatees and sea cows (image from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Dugong_dugon.jpg)). The data are in a table called `dugong`. Age is measured in years and length in meters. Because dugongs tend not to keep track of their birthdays, ages are estimated based on variables such as the condition of their teeth.

```{r nessage=FALSE}
dugong <- read_tsv('http://www.statsci.org/data/oz/dugongs.txt')
dugong
```

If we could measure the length of a dugong, what could we say about its age? Let's examine what our data say. Here is a regression of age (the response) on length (the predictor). The correlation between the two variables is substantial, at 0.83.

```{r}
cor(pull(dugong, Length), pull(dugong, Age))
```

High correlation notwithstanding, the plot shows a curved pattern that is much more visible in the residual plot.

```{r dpi=80, fig.align="center", message = FALSE}
regression_diagnostic_plots(dugong, Length, Age)
```

While you can spot the non-linearity in the original scatter, it is more clearly evident in the residual plot.

At the low end of the lengths, the residuals are almost all positive; then they are almost all negative; then positive again at the high end of lengths. In other words the regression estimates have a pattern of being too high, then too low, then too high. That means it would have been better to use a curve instead of a straight line to estimate the ages.

> When a residual plot shows a pattern, there may be a non-linear relation between the variables.

### Detecting Heteroscedasticity

*Heteroscedasticity* is a word that will surely be of interest to those who are preparing for Spelling Bees. For data scientists, its interest lies in its meaning, which is "uneven spread".

Recall the table `hybrid` that contains data on hybrid cars in the U.S. Here is a regression of fuel efficiency on the rate of acceleration. The association is negative: cars that accelerate quickly tend to be less efficient.

```{r message=FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/hybrid.csv"
hybrid <- read_csv(url) 
hybrid
```

```{r dpi=80, fig.align="center", message = FALSE}
regression_diagnostic_plots(hybrid, acceleration, mpg)
```

Notice how the residual plot flares out towards the low end of the accelerations. In other words, the variability in the size of the errors is greater for low values of acceleration than for high values. Uneven variation is often more easily noticed in a residual plot than in the original scatter plot.

> If the residual plot shows uneven variation about the horizontal line at 0, the regression estimates are not equally accurate across the range of the predictor variable.



-->

