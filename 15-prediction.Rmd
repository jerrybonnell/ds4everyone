# (PART) Modeling {-} 

# Regression

In this chapter we turn to making guesses, or *predictions*, about the future. There are many examples in which you or an employer would want to make claims about the future -- that are accurate! For instance: 

* Can we predict the miles per gallon of a brand new car model using aspects of its design and performance?
* Can we predict the price of suburban homes in a city using information collected from towns in the city, e.g. crime levels, demographics, median income, and distance to closest employment center? 
* Can we predict the magnitude of an earthquake given temporal and geological characteristics of the area, e.g. fault length and recurrence interval? 

To answer these questions, we need a framework or *model* for understanding how the world works. Fortunately, data science has offered up many such models for addressing the above proposed questions. This chapter turns to an important one known as *regression*, which is actually a family of methods designed for modeling the value of a *response* variable given some number of *predictor* variables.

We have already seen something that resembles regression in the introduction to visualization, where we guessed by examination the line explaining the highway mileage using displacement. We build upon that intuition to examine more formally what regression is and how to use it appropriately in data science tasks. 

## Correlation

Linear regression is closely related to a statistic called the *correlation*, which refers to how tightly clustered points are about a straight line with respect to two variables. If we observe such clustering, we say that the two variables are *correlated*; otherwise, we say that there is no correlation or, put differently, that there is no association between the variables. 

In this section, we build up an understanding of what correlation is. 

### Prerequisites

We will make use of the tidyverse in this chapter, so letâ€™s load it in as usual. We will also return to some datasets from the `datasauRus`, so let us load that in as well. 

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(datasauRus)
```

To build some intuition, we will compose a tibble called `toy_df` that contains several artificial datasets. Don't worry about the internal details of how the data is generated; in fact, it would be better to think of it as real data! 

```{r}
set.seed(1)
```

```{r echo = FALSE}
N <- 500
toy_df <- tibble(t = runif(N) * 2 * pi,
                 x = cos(t), # this is why there is some clumping in the endpoints 
                 perf = x, 
                 null = rnorm(N), 
                 quadratic = jitter(x ** 2),
                 linear = x + rnorm(N) / 5, 
                 curvy = sin(t),
                 cone = 2 * x + (x + rnorm(N)/3)^2 + rnorm(N)/3) %>%
  filter(x > -0.9, x < 0.9) %>%
  pivot_longer(c(perf, null, linear, curvy, quadratic, cone), 
               names_to = "dataset", values_to = "y") %>%
  select(x, y, dataset) %>%
  arrange(dataset)
```

```{r}
toy_df
```


### Visualizing correlation with a scatter plot

Let us begin by examining the relationship between the two variables `y` versus `x` in the dataset `linear` within `toy_df`. As we have done in the past, we will use a scatter plot for such a visualization. 

```{r dpi=80,  fig.align="center", warning = FALSE}
toy_df %>%
  filter(dataset == "linear") %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan") 
```

We noted earlier that two variables are correlated if they appear to cluster around a straight line. It appears that there is a strong correlation present here, but let us confirm what we are seeing by drawing a reference line at $y = x$. 

```{r dpi=80,  fig.align="center", warning = FALSE}
toy_df %>%
  filter(dataset == "linear") %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan") +
  geom_line(data = data.frame(x = c(-1,1), y = c(-1,1)),
            aes(x = x, y = y), color = "salmon", size = 1)
```

Indeed, we can confirm a "strong" correlation between these two variables. We will define how strong momentarily. Let us turn to another dataset, `perf`. 

```{r dpi=80,  fig.align="center", warning = FALSE}
toy_df %>%
  filter(dataset == "perf") %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan") +
  geom_line(data = data.frame(x = c(-1,1), y = c(-1,1)),
            aes(x = x, y = y), color = "salmon", size = 1)
```

Neat! The points fall *exactly* on the line. We can confidently say that `y_perf` and `x` are perfectly correlated. 

How about in the dataset `null`?

```{r dpi=80,  fig.align="center", warning = FALSE}
toy_df %>%
  filter(dataset == "null") %>%
  ggplot() + 
  geom_point(aes(x = x, y = y), color = "darkcyan") +
  geom_line(data = data.frame(x = c(-1,1), y = c(-1,1)),
            aes(x = x, y = y), color = "salmon", size = 1)
```

This one, unlike the others, does not cluster well around the line $y = x$ nor does it show a trend whatsoever; in fact, it seems the points are drawn at random, much like static noise on a television. We call a plot that shows such phenomena a *null plot*.   

Let us now quantify what correlation is. 

### The correlation coefficient `r` 

We are now ready to present a formal definition for correlation, which is usually referred to as the correlation coefficient $r$. 

> $r$ is the mean of the products of two variables that are scaled to standard units. 

Here are some mathematical facts about $r$. Proving them is beyond the scope of this course, so we only state them as properties. 

* $r$ can take on a value between $1$ and $-1$. 
* $r = 1$ means perfect positive correlation; $r = -1$ means perfect negative correlation. 
* Two variables with $r = 0$ means that they are not related by a line, i.e., there is no *linear association* among them. However, they can be related by something else, which we will see an example of soon.  
* Because $r$ is scaled to standard units, it is a dimensionless quantity, i.e., it has no units.  
* __Association does not imply causation!__ Just because two variables are strongly correlated, positively or negatively, does not mean that `x` *causes* `y`. 

Let us return to the `linear` dataset and compute the coefficient by hand. First, we append two new columns, `x_su` and `y_su`, that puts `x` and `y` in standard units. 

```{r}
toy_df %>% 
  filter(dataset == 'linear') %>%
  transmute(x = x, 
            y = y,
            x_su = scale(x),
            y_su = scale(y)) 
```

Let us modify the above code to add one more column, `prod`, that takes the product of the columns `x_su` and `y_su`. We will also save the resulting tibble to a variable called `linear_df_standard`.

```{r}
linear_df_standard <- toy_df %>% 
  filter(dataset == 'linear') %>%
  transmute(x = x, 
            y = y,
            x_su = scale(x),
            y_su = scale(y),
            prod = x_su * y_su) 
linear_df_standard
```

According to the definition, we need only to calculate the mean of the products to find $r$. 

```{r}
r <- linear_df_standard %>%
  pull(prod) %>%
  mean()
r
```

So the correlation of  `y` and `x` is about $0.94$ which implies a strong positive correlation, as expected.   

Thankfully, R comes with a function for computing the correlation so we need not repeat this work every time we wish to explore the correlation between two variables. The function is called `cor` and receives two vectors as input. 

```{r}
cor(pull(linear_df_standard, x), pull(linear_df_standard, y))
```

Note that there may be some discrepancy between the two values. This is due to the $n - 1$ correction that R provides when calculating quantities like `sd`.  

### The problem of linear correlation 

We noted earlier that $r = 0$ is a special case. Let us turn to the dataset `curvy` to see why.  

```{r}
curvy <- toy_df %>% filter(dataset == "curvy")
```

We can compute the correlation coefficient as before. 

```{r}
cor(pull(curvy, x), pull(curvy, y))
```

The small value may suggest to some that there is no correlation and, therefore, the scatter diagram should resemble a null plot. Let us visualize the data.

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(curvy) + 
  geom_point(aes(x = x, y = y), color = "darkcyan")
```

A clear pattern emerges! However, the association is very much not *linear*, as indicated by the obtained $r$ value.

### Be careful with summary statistics! (revisited)

We saw before the danger of using summary statistics like mean and standard deviation without first visualizing data. Correlation is another one in the bunch to watch out for. Let us see why. 

First, let us compose the same `bullseye` and `star` datasets we examined before. 

```{r}
bullseye <- datasaurus_dozen %>%
  filter(dataset == "bullseye")
```

```{r}
star <- datasaurus_dozen %>%
  filter(dataset == "star")
```

We will compute the correlation coefficient for each dataset. 

```{r}
cor(pull(bullseye, x), pull(bullseye, y))
cor(pull(star, x), pull(star, y))
```

We observe that both datasets have almost identical coefficient values so we may be suspect that both also have seemingly identical associations as well. We may also claim there is some evidence that suggests there is a weak negative correlation between the variables. 

As before, the test of any such claim is visualization. 

```{r dpi=80, fig.align="center", message = FALSE}
datasaurus_dozen %>%
  filter(dataset == "bullseye" | dataset == "star") %>%
  ggplot() +
  geom_point(aes(x = x, y = y, color = dataset)) +
  facet_wrap( ~ dataset, ncol = 2)
```

Indeed, we would be mistaken! The variables in each dataset are not identically associated nor do they bear any kind of linear association. The lesson here, then, remains the same: __always visualize your data!__ 

## Linear Regression 

Having a better grasp of what correlation is, we are now ready to develop an understanding of linear regression. 

### Prerequisites

We will make use of the tidyverse in this chapter, so let us load it in as usual. 

```{r message = FALSE, warning = FALSE}
library(tidyverse)
```

### The `trees` data frame

The `trees` data frame contains data on the diameter, height, and volume of 31 felled black cherry trees. Note that the diameter is erroneously labeled as `Girth`. 

```{r}
head(trees)
```

Let us visualize the relationship between `Girth` and `Height`.  

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(trees) + 
  geom_point(aes(x = Height, y = Girth), color = "darkcyan")
```

There seems to be some correlation between the two -- taller trees tend to have larger diameters. Confident in the trend we are seeing, we propose a (rather bold) research question: can we *predict* the average diameter of a black cherry tree from its height? To find out, let us turn to linear regression. 

### The simple linear regression model 

*Linear regression* is a method that estimates the mean value of a *response* variable, say `Girth`, against a collection of *regressors*, say `Height` or `Volume`, so that we can express the value of the response as some combination of the regressors, where each regressor either adds to or subtracts from the response value estimation. It is customary to represent the response as $y$ and the regressors as $x_i$, where $i$ is the index for one of the regressors. 

The linear model takes the form:

\[y = c_0 + c_1x_1 + \ldots + c_n x_n \]

where $c_0$ is the intercept and the other $c_i$'s are coefficients. This form is hard to digest, so we will begin with using only one regressor. Our model, then, reduces to: 

\[y = c_0 + c_1 x\]

which is the equation of a line, just as you have seen it in high school math classes. There are some important assumptions we are making when using this model: 

* The model is of the form $y = c_0 + c_1x$, that is, the data can be modeled linearly. 
* The variance of the "errors" is more or less constant. This notion is sometimes referred to as *homoskedasticity*. 

We will return to what "errors" mean in just a moment. For now just keep in mind that the linear model may not, and usually will not, pass through all of the points in the dataset and, consequently, some amount of error is produced by the predictions it makes.    

You may be wondering how we can obtain the intercept ($c_0$) and slope ($c_1$) for this line. For that, let us return to the tree data.  

### Finding the equation of the line 

We have noted before that there exists a close relationship between correlation and linear regression. Let us see if we can distill that connection here.  

We begin our analysis by noting the correlation between the two variables. 

```{r}
cor(pull(trees, Girth), pull(trees, Height))
```

This confirms the positive linear trend we saw earlier. Recall that the correlation coefficient is a dimensionless quantity. Let us standardize `Girth` and `Height` so that they are in standard units and are also dimensionless.  

```{r}
girth_height_su <- trees %>%
  transmute(Girth_su = scale(Girth),
         Height_su = scale(Height))
girth_height_su
```

Let us plot the transformed data using a scatter plot again. Note how the axes of this plot have changed and that we can clearly identify how many SDs each point is away from the mean along each axis. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(girth_height_su) + 
  geom_point(aes(x = Height_su, y = Girth_su), color = "darkcyan")
```

How can we find an equation of a line that "best" passes through this collection of points? We can start with some trial and error -- say, the line $y = x$. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(girth_height_su) + 
  geom_point(aes(x = Height_su, y = Girth_su), color = "darkcyan") + 
  geom_abline(aes(slope = 1, intercept = 0), color = "salmon", size = 1) +
  geom_segment(aes(x = Height_su, y = Girth_su,
                   xend = Height_su, yend = 1 * Height_su + 0), color = "blue", alpha = 0)
```

We can see that the line closely passes through some points while completely missing the mark for others.  Unfortunately, this description is useless as it doesn't *quantify* the amount of error for each point -- can we do better? 

For this, we introduce the notion of a *residual*, which we define as the vertical distance from the point to the line (which can be positive or negative depending on the location of the point relative to the line). More formally, we say that a residual $e_i$ has the form: 

\[e_i = y_i - (c_0 + c_1 x_i) \]

where plugging into $c_0 + c_1 x_i$, the equation of the line, returns the *predicted* value for some observation $i$. We usually call this the *fitted* value. 

We can annotate the above plot with such residuals. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
plot_errors <- function(df, y, x, slope, intercept) {
  ggplot(df) + 
  geom_point(aes(x = {{ x }}, y = {{ y }}), color = "darkcyan", alpha = 0.5) + 
  geom_abline(aes(slope = slope, intercept = intercept), color = "salmon", size = 1)  +
  geom_segment(aes(x = {{ x }}, y = {{ y }},
                   xend = {{ x }}, yend = slope * {{ x }} + intercept), color = "blue", alpha = 0.4)
}
plot_errors(girth_height_su, Girth_su, Height_su, 1, 0)
```

The vertical blue lines show the residual for each point; note how some are small while others are quite large. How do the residuals for this line compare with those for, say, a horizontal line at $y = 0$?

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
plot_errors(girth_height_su, Girth_su, Height_su, 0, 0)
```

If we compare the two plots side-by-side, it becomes clear that the equation $y = x$ has smaller residuals overall than the horizontal line that passes through $y = 0$. This observation is key: to get an overall sense of the error in the model, we can *sum* the residuals for each point. However, there is a problem with such an approach. Some residuals can be positive while others can be negative, so a straightforward sum of the residuals will total to 0! Thus, we take the *square* of each residual and then issue the sum. 

Our goal can now be phrased as follows: we would like to find the line that *minimizes* the **r**esidual **s**um of **s**quares. We normally refer to this quantity as $RSS$. Let us write a function that returns the $RSS$ for a given linear model. This function receives a vector called `params` where the first element is the slope and the second the intercept, and returns the corresponding $RSS$. 

```{r}
line_rss <- function(params) {
  slope <- params[1]
  intercept <- params[2]
  
  x = pull(girth_height_su, Height_su)
  y = pull(girth_height_su, Girth_su)
  fitted <- slope * x + intercept
  return(sum((y - fitted) ** 2))
}
```

We can retrieve the $RSS$ for the two lines we played with above. First, for the line $y = x$. 

```{r}
params <- c(1, 0)
line_rss(params)
```

Next, for the horizontal line passing through $y = 0$. Note how the larger $RSS$ for this model confirms what we saw graphically when plotting the vertical distances.  

```{r}
params <- c(0, 0)
line_rss(params)
```

We could continue experimenting with values until we found a configuration that seems "good enough". While that would hardly be acceptable to our peers, solving this rigorously involves methods of calculus which are beyond the scope of this course. Fortunately, we can use something called numerical optimization which allows R to do the trial-and-error work for us, "nudging" the above line until it minimizes $RSS$. 

### Numerical optimization 

The `optim` function can be used to accomplish the task of finding the linear model that yields the smallest $RSS$. `optim` takes as arguments an initial vector of values, let us call it the `initial_guess` vector, and a function to be minimized using the initial guess as a starting point -- this is our function `line_rss`. 

The call `optim(initial_guess, line_rss)` will nudge around the values of slope and intercept inside `initial_guess` until a minimum is reached. 

```{r}
initial_guess <- c(1,0)  # start with our first guess, the y = x line from above 
best <- optim(initial_guess, line_rss)
```

We can now examine the obtained slope and intercept. 

```{r}
best$par
```

This means that the best linear model follows the equation 

\[\text{Girth} = 0.52 * \text{Height} \]

where the intercept is essentially 0. We can overlay this line on our scatter plot. Compare this line with our original guesses, which are shown in gray. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(girth_height_su) + 
  geom_point(aes(x = Height_su, y = Girth_su), color = "darkcyan") + 
  geom_abline(aes(slope = best$par[1], intercept = best$par[2]), color = "salmon", size = 1) +
  geom_abline(aes(slope = 1, intercept = 0), color = "gray", alpha = 1) +
  geom_abline(aes(slope = 0, intercept = 0), color = "gray", alpha = 1) +
  geom_segment(aes(x = Height_su, y = Girth_su,
                   xend = Height_su, yend = 1 * Height_su + 0), color = "blue", alpha = 0)
```

Compare the line we have just found (in orange) with the line $y = x$. For trees with heights that are *less* than 0 $SD$s away from the mean, the orange line predicts a *larger* girth than the $y = x$ line. And, for trees with heights that are *more* than 1 $SD$ away from the mean, the orange line predicts a *smaller* girth than the $y = x$ line. The effect should become clearer in the following plot. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(girth_height_su) + 
  geom_point(aes(x = Height_su, y = Girth_su), color = "darkcyan") + 
  geom_abline(aes(slope = best$par[1], intercept = best$par[2]), color = "salmon", size = 1) +
  geom_abline(aes(slope = 1, intercept = 0), color = "gray", alpha = 1) +
  geom_abline(aes(slope = 0, intercept = 0), color = "gray", alpha = 1) +
  geom_vline(xintercept = -1, color = "blue") + 
  geom_vline(xintercept = 1, color = "blue") + 
  geom_point(aes(x = -1, y = 0), size = 2, color = "blue") + 
  geom_point(aes(x = -1, y = -0.5), size = 2, color = "blue") + 
  geom_point(aes(x = -1, y = -1), size = 2, color = "blue") + 
  geom_point(aes(x = 1, y = 0), size = 2, color = "blue") + 
  geom_point(aes(x = 1, y = 0.5), size = 2, color = "blue") + 
  geom_point(aes(x = 1, y = 1), size = 2, color = "blue") + 
  geom_segment(aes(x = Height_su, y = Girth_su,
                   xend = Height_su, yend = 1 * Height_su + 0), color = "blue", alpha = 0)
```

Observe the position of the orange line relative to $y = x$ at each vertical blue line. Note how the orange line approaches the horizontal line that passes through $y = 0$. The phenomena we are observing is called __"regression towards the mean"__, as the regression line prefers to predict extreme points closer to the mean. We can summarize the effect on the data as follows: 

> Shorter trees will tend to have larger girths on average, and taller trees will tend to have smaller girths on average.  

Also recall the value of the correlation coefficient we calculated earlier.  

```{r}
cor(pull(trees, Girth), pull(trees, Height))
```

This is the same value as the slope of the line. Thus, we discover a connection: 

> When $x$ and $y$ are scaled to standard units, the slope of the regression of $y$ on $x$ equals the correlation coefficient $r$ and the line passes through the origin $(0,0)$. 

Let us summarize what we have learned and compare it against what the theory tells us. 

### Equation of the Regression Line

Mathematical theory tells us, in general, that the __slope__ of the regression line follows: 

\[ \text{slope} = r * \frac{SD_y}{SD_x}\]

It should be clear that when we put $x$ and $y$ in standard units, the slope is simply $r$. This confirms what we just saw above. 

The __intercept__ of the line has the form:

\[ \text{intercept} = \bar{y} - \text{slope} * \bar{x} \]

where $\bar{x}$ and $\bar{y}$ is the mean of $x$ and $y$, respectively. 


## Using Linear Regression

The last section developed a theoretical foundation for understanding linear regression. This section will involve linear regression more practically, and introduce some functions that R provides for doing regression analysis. 

### Prerequisites

We will make use of the tidyverse in this chapter, so let us load it in as usual. 

```{r message = FALSE, warning = FALSE}
library(tidyverse)
```

For the running example in this section, we will appeal to the [The MLB Height Weight dataset](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) from [UCLA SOCR Statistics](http://ww.socr.ucla.edu/), which is an excellent resource for statistical concepts and tools. The dataset contains 1,034 Major League Baseball players among 30 teams and 9 positions. Let us load this dataset and remove any missing values that may be present. 

```{r message = FALSE}
mlb <- read_csv("data/mlb.csv") %>%
  drop_na()
```

### The `lm` function 

Let us visualize the relationship between `Weight` and `Height`.  

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(mlb) + 
  geom_point(aes(x = Height, y = Weight), color = "darkcyan")
```

How much correlation is present between these two variables?

```{r}
cor(pull(mlb, Weight), pull(mlb, Height))
```

The high positive correlation suggests that linear regression can be a good fit for the data. Let us run the linear regression, this time making use of R. 

The function is `lm`, which is the short-hand for "linear model". Running the linear regression function requires two parameters. One parameter is the data set, which takes the form `data = DATA_SET_NAME`. The other is the response and the regressors, which takes a formula of the form `response ~ regressor_1 + ... + regressor_k`.

For a regression of `Weight` on `Height`, we have the following call. 

```{r}
lmod <- lm(Weight ~ Height, data = mlb)
```

The resulting linear model is stored in a variable `lmod`. We can inspect it by simply typing its name into the console. 

```{r}
lmod
```

The resulting values point us to the equation: 

\[\text{Weight} = 4.841 * \text{Height} - 155.09 \]

Let us overlay the linear model atop the scatter plot. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(mlb) + 
  geom_point(aes(x = Height, y = Weight), color = "darkcyan") + 
  geom_abline(aes(slope = lmod$coefficients[2], intercept = lmod$coefficients[1]), 
              size = 1, color = "salmon")
```

We sometimes call the line a "graph of averages", as it tells us the average weight of a MLB player for any given height.  

An equivalent, and perhaps more direct, way of plotting this is to use a smooth geom using the `lm` method. 

```{r dpi=80, fig.align="center", eval = FALSE, message = FALSE}
ggplot(mlb, aes(x = Height, y = Weight)) + 
  geom_point(color = "darkcyan") + 
  geom_smooth(method = "lm", color = "salmon", size = 1, se = FALSE)
```


### Making Predictions about Unknown Observations

The power of a linear model lies not in its status of being the "best" fitted line that passes through a collection of points, but because we can use such a model to make predictions about observations that do not exist in our dataset. Put more specifically, we can use `lmod` to make predictions about the expected weight of a MLB baseball player using just one piece of information: their height.  

Making a prediction is easy: just plug into the equation of the linear model! For example, what is the expected weight of a MLB baseball player who is 72 inches tall? 

\[\text{Weight} = 4.841 (72) - 155.09 = 193.46\]

So a MLB baseball player that is 72 inches tall has an expected weight of about 193.46 pounds. 

R can do the work for us with the function `predict`. Note that this function receives a dataset, in the form of a data frame or tibble, with the required inputs. 

```{r}
my.pred <- predict(lmod, newdata = tibble(Height = c(72)))
my.pred
```

The prudent reader may have some suspicions about this result -- *that's it*? Is this something you can take to the bank? 

Indeed, there is a key element missing from the analysis so far: *confidence*. How confident are we in our predictions? We learned in the statistics unit that data scientists rarely report singular (or pointwise) values because so often they are working with samples of data. The regression line could have turned out differently depending on the sample at hand and, consequently, the predicted value can change as well. 

Instead, data scientists provide *confidence intervals* that quantify the amount of uncertainty in a prediction. We learned one way of obtaining such intervals in the previous chapters: the bootstrap.  

### Bootstrapping a Confidence Interval

We will use the bootstrap method we learned earlier to estimate a confidence interval for the prediction we just made. To accomplish this, we will: 

* Bootstrap the samples that are present in `mlb` by means of sampling with replacement. 
* Fit a linear model to the bootstrapped samples. 
* Generate a prediction from this linear model. 
* Repeat the process a large number of times, say, 5,000 or 10,000 times. 

Let us define a function that performs one round of the bootstrap. 

```{r}
one_bootstrap <- function(df, x, y, x.new) {
  # bootstrap as many samples as the size of the dataset at hand
  samples <- slice_sample(df, n = nrow(df), replace = TRUE)
  # fit a linear model
  fm <- as.formula(paste(y, "~", x))
  lmod <- lm(fm, data = samples)
  # generate and return the prediction 
  pred <- predict(lmod, newdata = x.new)
  return(pred)
}
```

Run the following cell a few times and observe the variability in the prediction. 

```{r}
one_bootstrap(mlb, "Height", "Weight", tibble(Height = c(72)))
```

Let us now repeat the bootstrap a large number of times, say 10,000 times. 

```{r}
bstrap_predictions <- replicate(n = 10000, one_bootstrap(mlb, "Height", "Weight", tibble(Height = c(72))))
```

As before, we will identify a 95% confidence interval. Here it is: 

```{r}
desired_area <- 0.95
middle <- quantile(bstrap_predictions, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle
```

Let us plot an empirical histogram of the predictions and annotate the confidence interval on this histogram.

```{r dpi=80, fig.align="center", message = FALSE}
df <- tibble(bstrap_predictions)
ggplot(df, aes(x = bstrap_predictions, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 13) +
  geom_segment(aes(x = middle[1], y = 0, xend = middle[2], yend = 0), 
                   size = 2, color = "salmon") +
  geom_point(aes(x = my.pred, y = 0), size = 3, color = "red")
```

We find that the distribution is approximately normal. Our prediction interval spans between 192.2 and 194.7 pounds. The red dot shows where our original prediction occurs, which falls squarely in the center of the histogram.  

We can use R to give us the prediction interval by amending the call to `predict`. 

```{r}
lmod <- lm(Weight ~ Height, data = mlb)
predict(lmod, newdata = tibble(Height = c(72)), interval = "confidence")
```

`predict` obtains the interval by means of statistical theory, and we can see that the result is very close to what we found using the the bootstrap. Formally, these intervals go by a special name: *confidence intervals for the mean response*. That's a mouthful!


### How significant is the slope? 

We showed that the predictions generated by a linear model can vary depending on the samples we have at hand. The slope of the linear model can also turn out differently for the same reasons. But, by how much?

In the same way we used bootstrapping to estimate a confidence interval for the mean response, we can apply bootstrapping to estimate the true slope. Fortunately, we need only to make small modifications to the code we wrote earlier to perform this experiment. The change being that instead of generating a prediction from the linear model, we will return its slope.    

```{r}
one_bootstrap <- function(df, x, y) {
  # bootstrap as many samples as the size of the dataset at hand
  samples <- slice_sample(df, n = nrow(df), replace = TRUE)
  # fit a linear model
  fm <- as.formula(paste(y, "~", x))
  lmod <- lm(fm, data = samples)
  # return the slope of the model
  return(lmod$coefficients[2])
}
```

Run the following cell a few times and observe how the value of the slope changes.  

```{r}
one_bootstrap(mlb, "Height", "Weight")
```

As before, we will repeat the process 10,000 times. 

```{r}
bstrap_slopes <- replicate(n = 10000, one_bootstrap(mlb, "Height", "Weight"))
```

Here is the 95% confidence interval.

```{r}
desired_area <- 0.95
middle <- quantile(bstrap_slopes, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle
```

Following is the empirical histogram of the bootstrapped slopes with the annotated confidence interval. We also plot a red dot representing the original slope, which we found to be $4.841$.   

```{r dpi=80, fig.align="center", message = FALSE}
df <- tibble(bstrap_slopes)
ggplot(df, aes(x = bstrap_slopes, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 13) +
  geom_segment(aes(x = middle[1], y = 0, xend = middle[2], yend = 0), 
                   size = 2, color = "salmon") +
  geom_point(aes(x = 4.841, y = 0), size = 3, color = "red")
```

As before, we observe that the distribution is roughly normal. We also see that our original slope value sits right in the middle of the confidence interval, which ranges from about 4.3 to 5.3. 

The normality of this distribution is important since it is the basis for the statistical theory that regression analysis builds up from. This means that our interval should be extremely close to what R reports, which is calculated using such theory. The function `confint` gives us the answer.    

```{r}
confint(lmod)
```

Indeed, they are quite close! As a bonus, we also get a confidence interval for the intercept. 

<!-- as a lab or assignment, frame the confidence interval as a hypothesis test to check if slope is 0 -->


### The `summary` function 

An important function for doing regression analysis in R is the function `summary`, which we have not covered so far and will likely not cover in the remainder of the course. The function reports a wealth of useful information about the linear model, such as significance of the intercept and slope coefficients. However, to truly appreciate and understand what is presented requires a deeper understanding of statistics than what we have developed so far -- if this sounds at all interesting, we encourage you to take a course in statistical analysis! 

Nevertheless, for the curious reader, we include the incantation to use. 

```{r eval = FALSE}
summary(lmod)
```


## Graphical Diagnostics 

An important part of using regression analysis well is understanding how to apply it. Many situations will present itself to you as what seems a golden opportunity for applying linear regression only to find out that the data does not meet any of the assumptions. Do not despair -- there are many transformation techniques and diagnostics available that can render linear regression suitable. However, using any of them first requires a realization that there is a problem at hand with the current linear model. 

Detecting problems with a regression model is the job of diagnostics. We can broadly categorize them as two kinds -- numerical and graphical. We find graphical diagnostics easier to digest because they are visual and, hence, is what we will cover in this section. 

### Prerequisites 

We will make use of the tidyverse in this chapter, so let us load it in as usual. 

```{r message = FALSE, warning = FALSE}
library(tidyverse)
```

To develop the discussion, we will make use of the toy dataset we artificially created when studying correlation. We will study three datasets contained in this tibble: `linear`, `cone`, and `quadratic`. 

```{r}
diagnostic_examples <- toy_df %>%
  filter(dataset %in% c("linear", "cone", "quadratic"))
diagnostic_examples
```

### A reminder on assumptions 

To begin, let us recap on the assumptions we have made about the linear model. 

* The model is of the form $y = c_0 + c_1x$, that is, the data can be modeled linearly. 
* The variance of the residuals is constant, i.e., it fulfills the property of *homoskedasticity*. 


### Some instructive examples 

Let us now examine the relationship between `y` and `x` across three different datasets. As always, we start with visualization.    

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(diagnostic_examples) + 
  geom_point(aes(x = x, y = y, color = dataset)) +
  facet_wrap( ~ dataset, ncol = 3)
```

Assuming we have been tasked with performing regression on these three datasets, does it seem like a simple linear model of `y` on `x` will get the job done? One way to tell is by using something we have already learned: look at the correlation between the variables for each dataset. 

We use `dplyr` to help us accomplish the task. 

```{r}
diagnostic_examples %>%
  filter(dataset == "cone") %>%
  summarize(cor(x, y))
```

```{r}
diagnostic_examples %>%
  filter(dataset == "linear") %>%
  summarize(cor(x, y))
```

```{r}
diagnostic_examples %>%
  filter(dataset == "quadratic") %>%
  summarize(cor(x, y))
```

Even without computing the correlation, it should be clear that something is very wrong with the `quadratic` dataset. We are trying to fit a line to something that follows a curve -- a clear violation of our assumptions! The correlation coefficient also confirms that `x` and `y` do not have a linear relationship in that dataset. 

The situation with the other datasets is more complicated. The correlation coefficients are roughly the same and signal a strong positive linear relationship in both. However, we can see a clear difference in how the points are spread in each of the datasets. Something looks "off" about the `cone` dataset.  

Notwithstanding our suspicions, let us proceed with fitting a linear model for each. 

```{r}
cone <- diagnostic_examples %>% filter(dataset == "cone")
linear <- diagnostic_examples %>% filter(dataset == "linear")
quadratic <- diagnostic_examples %>% filter(dataset == "quadratic")

lmod_cone <- lm(y ~ x, cone)
lmod_linear <- lm(y ~ x, linear)
lmod_quadratic <- lm(y ~ x, quadratic)
```

Before we proceed any further, we need some new machinery for understanding the appropriateness of our linear model. For that, we turn to the residual plot.

### The residual plot 

One of the main diagnostic tools we use for studying the fit of a linear model is the residual plot. A *residual plot* can be drawn by plotting the residuals against the fitted values. 

Let us look at the residual plot for `lmod_linear`. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(lmod_linear) + 
  geom_point(aes(x = .fitted, y = .resid), color = "darkcyan")
```

This residual plot tells us that the linear model has a good fit. The residuals are distributed roughly the same around the horizontal line at 0, and the width of the plot is not wider in some parts while narrower in others. The resulting shape is "blobbish nonsense" with no tilt.  

Thus, our first observation: 

> A residual plot corresponding to a good fit shows no pattern. The residuals are distributed roughly the same around the horizontal line passing through 0.  

### Detecting a lack of homoskedasticity 

Let us now turn to the residual plot for `lmod_cone`, which we suspected had a close resemblance to `lmod_linear`.  

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(lmod_cone) + 
  geom_point(aes(x = .fitted, y = .resid), color = "darkcyan")
```

Observe how the residuals fan out at both ends of the residual plot. Meaning, the variance in the size of the residuals is higher in some places while lower in others -- a clear violation of the linear model assumptions. Note how this problem would be harder to detect with the scatter plot or correlation coefficient alone. 

Thus, our second observation: 

> A residual plot corresponding to a fit with nonconstant variance shows uneven variation around the horizontal line passing through 0. The resulting shape of the residual plot usually resembles a "cone" or a "funnel".


### Detecting nonlinearity 

Let us move onto the final model, `lmod_quadratic`. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(lmod_quadratic) + 
  geom_point(aes(x = .fitted, y = .resid), color = "darkcyan")
```

This one shows a striking pattern, which bears the shape of a quadratic curve. That is a clear violation of the model assumptions, and indicates that the variables likely do not have a linear relationship. It would have been better to use a curve instead of a straight line to estimate `y` using `x`. 

Thus, our final observation:

> When a residual plot presents a pattern, there may be nonlinearity between the variables. 

### What to do from here? 

The residual plot is helpful in that it is a tell-tale sign of whether a linear model is appropriate for the data. The next question is, of course, what comes next? If a residual plot indicates a poor fit, data scientists will likely look to techniques like transformation to see if they can provide quick remedies. While such methods are certainly beyond the scope of this course, we can demonstrate what the residual plot will look like after the problem is addressed. 

Let us return to the model `lmod_quadratic`. The residual plot signaled a problem of nonlinearity, and the shape of the plot (as well as the original scatter plot) gives a hint that we should try a quadratic curve.  

We will modify our model to include a new regressor, which contains the term $x^2$. Our model will then have the form:

\[y = c_0 + c_1x + c_2x^2 \]

which is the standard form of a quadratic curve, as you have likely seen it before. Amending the call to `lm` is straightforward. 

```{r}
lmod_quadratic_revised <- lm(y ~ x + I(x^2), quadratic)
```

Let us draw again the residual plot.  

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(lmod_quadratic_revised) + 
  geom_point(aes(x = .fitted, y = .resid), color = "darkcyan")
```

The residual plot shows no pattern whatsoever and, therefore, we can be confident in our new model :-)


<!--- homework problem for playing with diagnostics  
```{r dpi=80, fig.align="center", eval = FALSE, message = FALSE}
library(gapminder)
a <- filter(gapminder, continent == "Africa")
ggplot(a) +
   geom_point(aes(x = log(gdpPercap), y = log(lifeExp)))
ggplot(a) +
   geom_point(aes(x = log(gdpPercap), y = log(lifeExp)))
ggplot(a) +
  geom_histogram(aes(x = log(gdpPercap), y = ..density..))
ggplot(a) +
  geom_histogram(aes(x = log(lifeExp), y = ..density..))

lmod <- lm(log(lifeExp) ~ log(gdpPercap), data = a)
summary(lmod)
ggplot(lmod, aes(x = .fitted, y = .resid)) +
  geom_point()
```


## More than one Regressor 

We can find a model that has less error when fitting the data. the hope is that we can take advantage of all the information in the regressors by including all of them together in one linear model. 

We would like for a way to account for all of the information in one model. An extension of linear regression is something called *multiple linear regression* which allows us to specify more than one regressor to be used in the model. The theory for this is beyond the scope of the course, but R allows you to do this using the same `lm` function. 


As usual, our exploration starts with loading the library `tidyverse`.

```{r eval = FALSE}
library(tidyverse)
library("reshape2")
```

Our previous exploration of gas mileage used the `mpg` data set. This time, we use `mtcars`. By typing `?mtcars` you get the information of the data set. 

As you can see, all the attributes are numerical.
By `glimpse(mtcars)` you can inspect the attributes.

```{r eval = FALSE}
glimpse(mtcars)
```

```{r eval = FALSE, fig.height=6, fig.width=4}
#melt your data
df_melt <- melt(mtcars,"mpg")

ggplot(df_melt,aes(value,mpg)) +
  geom_point() +
  facet_wrap(.~variable, ncol = 2, scales = "free")
```

It looks like there is a strong association between `mpg` and `disp` so let's pick these two out for a linear regression. 

```{r eval = FALSE}
lmod <- lm(mpg ~ disp + I(disp^2), data = mtcars)
summary(lmod)
```

```{r eval = FALSE}
ggplot(mtcars) + 
  geom_point(aes(x = disp, y = resid(lmod)))
```

Now let us try a regression on all of the continuous variables. 

```{r eval = FALSE}
lmod <- lm(mpg ~ disp + I(disp^2) + cyl + hp + drat + wt + qsec, data = mtcars)
summary(lmod)
```


You can see that the multiple R-squared has increased as expected when accounting for all these variables. However, deeper analysis is required. The final numerical column contains a quantity known as the p-value. These are derived from statistical theory which is beyond the scope of this course. The interpretation is simple, however. The smaller the p-value the more significant we consider the coefficient. p-values of less than 0.1 are considered to have some significance. This interpretation is done for us in the final column where the amount of dots represents the level of significance. According to this metric, none of the coefficients are significant except for `wt`. A reason for this can be that the other variables themselves depend on the weight of the car. 

Let us run a regression with only the two most significant regressors, `wt` and `hp`. 

```{r eval = FALSE}
lmod <- lm(mpg ~ wt + hp, data = mtcars)
summary(lmod)
```

We see that they both have a high level of significance and the model contains a near identical R-squared value. Because this model appears just as good, we prefer this (simpler) model over the more complete and complex model. occam's razor?


---> 


