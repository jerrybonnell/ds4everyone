---
output:
  html_document: default
  pdf_document: default
---

# Hypothesis Testing

In the previous chapters, we learned about randomness and sampling. Quite often, a data scientist receives some data and must make some assertion about it. There are typically two kinds of situations:

* She has one dataset and a model that the data should have followed. She needs to decide if it is likely that the dataset indeed follows the model?
* She has two datasets. She needs to decide if it is possible to explain the two datasets using a single model.

Here are examples of the two situations.

* A company making dice is testing the fairness of a die. Using some machine, the company throws the die 100,000 times. They record the face that turns up. By examining the record of the 100,000 throws, can you tell how likely it is that the die is fair?
* You are evaluating the performance data of National Football League (NFL) players. Are the receivers in 2019 better than those in 2017?

For both situations, an important consideration for you to make is in terms of what you compare the differences.

## Evaluating a Model

Suppose 10,000 throws of a die generate the following counts for the faces 1 through 6:

```{r}
face_counts <- c(1620, 1618, 1633, 1658, 1802, 1669)
face_counts
```

By dividing each number by 10,000, we get the *proportion* of the occurrence of each face.

```{r}
empirical <- face_counts / 10000
empirical
```

The six values comprise the empirical distribution representing the behavior of the die. We notice that the values are not very close to the ideal 0.1667 (= 1/6). How far away is that from what we know about the true distribution?

We know that the probability of each face is $1/6$. By subtracting $1/6$ from each and obtaining the absolute difference values from them by removing any negative sign we have: 

```{r}
abs(empirical - 1/6)
```

We then compute the sum and take one half of this value. We will call this the *observed value*. 

```{r}
observed_tvd <- sum(abs(empirical - 1/6)) * 1/2
observed_tvd
```

In statistics, this value has a special name: the *total variation distance* (or, for short, TVD).  The total variation distance serves as the measure for the difference between two distributions, namely, the difference between a true and empirical distribution. 

Now that we have the technical term referring to the difference, we can state the question as follows:

> Is the total variation distance 0.0446 big or small?

It is hard to tell without knowing the differences we get by chance.

Let us conduct some simulation to obtain an empirical distribution of the absolute difference of a fair dice. As mentioned earlier, the proportion that each face turns up is not constant, even for a fair die. So by simulating throws of a fair dice, we must expect to see a range of absolute differences.

### Prerequisites

Before starting, let's load the `tidyverse` as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```


### The `rmultinom` function

We saw before that we can generate an empirical distribution by putting in place some sampling strategy. Perhaps the most straightforward is simple random sampling with replacement. This will be the approach we continue to make use of here, as well as throughout the rest of the course.  

We also learned about two different ways to sample with replacement. `sample` samples items from a *vector*, which we used when simulating the number of expected heads in 100 coin tosses or the expected amount of grains a minister receives after some number of days. `slice_sample` functions identically, but instead samples from rows of a *data frame* or tibble. To generate the empirical distribution for this experiment, we could just use `sample` again. But there is a quicker way, using a function called `rmultinom`, which is tailored for sampling at random from categorical distributions. We introduce it here and will use it several times this chapter.

Here is how we can use it to generate an empirical distribution of 100 tosses of a fair coin.

```{r}
fair_coin <- c(1/2, 1/2)
sample_vector <- rmultinom(n = 1, size = 100, prob = fair_coin)
sample_vector
```

For an empirical distribution, we are more interested in the *proportion* of resulting heads and tails. Thus, we should divide by the number of tosses. Note how the probability of heads and tails is about equal. 

```{r}
sample_vector / 100
```

So we can just as easily simulate proportions instead of counts.

The classic interpretation of `rmultinom` is that you have some marbles to put into boxes of size `size`, each with some probability `prob`; the length of `prob` determines the number of boxes. The result shows the number of marbles that end up in each box. Thus, the function takes the following arguments:

* `size`, the total number of marbles that are put into the boxes. 
* `prob`, the distribution of the categories in the population, as a vector of proportions that add up to 1.
* `n`, the number of samples to draw from this distribution. We will typically leave this at 1 to make things easier to work with later on.  

It returns a vector containing the number of marbles in each category in a random sample of the given size taken from the population. Because this distribution is so special, statisticians have given it a name: the *multinomial distribution*. 

Let us see how we can use it to assess the model for 10,000 throws of dice. 

### A model for 10,000 die throws 

Generating an empirical distribution for the dice throw is an extension of the coin toss we saw just earlier. As before, note how the probability of each face is about equal. 

```{r}
fair_die_probs <- rep(1/6, 6)
sample_vector <- rmultinom(n = 1, size = 10000, prob = fair_die_probs) / 10000
sample_vector
```

We can compute the total variation distance (TVD) of this sample. 

```{r}
sum(abs(sample_vector - fair_die_probs)) / 2
```

Let's wrap this up into a function we can call. 

```{r}
one_simulated_tvd <- function() {
  fair_die_probs <- rep(1/6, 6)
  sample_vector <- rmultinom(n = 1, size = 10000, prob = fair_die_probs) / 10000
  return(sum(abs(sample_vector - fair_die_probs)) / 2)
}
```

Next, we create a vector `sample_tvds` containing 10,000 simulated TVD values. As before, we will use `replicate` to do the work. 

```{r echo=FALSE}
set.seed(1)
```

```{r}
num_repetitions <- 10000
sample_tvds <- replicate(n = num_repetitions, one_simulated_tvd())
```

```{r}
max(sample_tvds)
```

### The Prediction

To interpret the results of our simulation, we start by visualizing the results using an empirical histogram.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_tvds)) +
  geom_histogram(aes(x = sample_tvds, y = ..density..), fill = "darkcyan", color = 'gray')
```

Where does the observed value fall in this histogram? We can plot it easily. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_tvds)) +
  geom_histogram(aes(x = sample_tvds, y = ..density..), fill = "darkcyan", color = 'gray') +
  geom_point(aes(x = observed_tvd, y = 0), size = 3, color = "salmon")
```


Let us look at the proportion of the elements in the list `sample_tvds` that are at least the observed TVD value 0.01376, whose value we have stored in `observed_tvd`. We simply count the elements matching the requirement and then divide the count by the length of the vector.

```{r}
sum(sample_tvds >= observed_tvd) / length(sample_tvds)
```


The value we get is 0.0638, or about 6.4%. <!-- how do we adjust according to the outcome? -->
We interpret the value as stating the *chance* the difference achieves 0.0659 is no more than 6.6%. We can thus conclude that the chances that the die at hand is a fair one is no more than 6.6%. This value, computed through simulation, is what we call a *p-value*.

To compute a p-value, we take the following steps:

1. Establish some model that is possibly describing a quantifiable phenomenon (e.g., the absolute distance from the ideal distribution).
2. Run a simulation to obtain a histogram of the quantifiable phenomenon under the model.
3. Compare the observation and examine where in the histogram the observation stands.

More specifically, we estimate how far the observation is from the central portion of the histogram by splitting the histogram at the point of observation. As shown in the following figure, the portion less than or equal to the observation (in dark cyan) and the portion greater than or equal to the observation (in orange). Note that we can include the equality, when the sampled value equals the observed value, on either side. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
colors <- c(rep("darkcyan",17), rep("salmon",13))

ggplot(tibble(sample_tvds)) +
  geom_histogram(aes(x = sample_tvds, y = ..density..), fill = colors, color = 'gray') +
  geom_point(aes(x = observed_tvd, y = 0), size = 3, color = "red")
```

The closer these two portions are in size, i.e. the number of cyan and orange bars, the higher the likelihood of the observation. Conversely, the farther the two portions are in size, the lower the likelihood of the observation. We usually take the smaller of the two portions and designate the smaller portion a special name: __the p-value__.

Also, when we compute a p-value, we have in mind two possible interpretations.
We call them the *Null Hypothesis* and the *Alternative Hypothesis*.

* __Null Hypothesis (NH):__ The hypothesis we use to create the model for simulation. For example, we assume that the die we have is a fair die. 
* __Alternative Hypothesis (AH):__ The opposite, or counterpart, of the Null Hypothesis. For example, the AH states that the die we have is not fair.  

Using simulations we learn that the Null Hypothesis has a p-value of 6.6% (or 0.0659) using the absolute distance between distributions as the distance measure.

## Case Study: Entering Harvard

Harvard University is one of the most, if not the most, prestigious universities in the United States. A recent lawsuit by [Students for Fair Admissions (SFFA)](https://studentsforfairadmissions.org/) led by Edward Blum against Harvard University alleges that Harvard has used soft racial quota to keep the numbers of Asian-Americans low.
Put differently, the allegation claims that, from the pool of Harvard-admissible American applicants, Harvard uses a racial discriminatory score that allows the college to choose disproportionately less Asian-Americans. As of this writing, the lawsuit has been appealed and is to appear before the Supreme Court.

This section examines the "Harvard model" to assess, at a basic level, the claim put forward by the lawsuit.  

### Prerequisites

Before starting, let's load the `tidyverse` as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

### Students for Fair Admissions

Harvard University publishes [some statistics](https://college.harvard.edu/admissions/admissions-statistics) on the class of 2024.
According to the data, the proportions of the class for Whites, Blacks, Hispanics, Asians, and Others (International and Native Americans) are respectively 46.1%, 14.7%, 12.7%, 24.4%, and 2.1%.

We do not have the data of the students admissible to enter Harvard so, in lieu of this, we refer to student demographics enrolled in a four-year college.
According to [Chronicle of Higher Education](http://www.chronicle.com) 2020-2021 Almanac, the racial demographics of full-time students in 4-year private non-profit institutions -- Harvard is one of them -- in Fall 2018 are: 63.6% White, 11.5% Black, 12.3% Hispanic, 8.1% Asian, and 4.5% Other.

Let's compile this information into a tibble.

```{r}
class_props <- tribble(~Race, ~Harvard, ~Almanac,
                             "White", 46.1, 63.6, 
                             "Black", 14.7, 11.5,
                             "Hispanic", 12.7, 12.3,
                             "Asian", 24.4, 8.1,
                             "Other", 2.1, 4.5)
class_props
```
The distributions may look quite different from each other. Of course, the demographics from the Almanac includes students who did not apply to Harvard, those who might not have got into Harvard, those applied and did not get in, and international students. Moreover, the Almanac data covers all full-time students in 2018 but not students who entered college in 2020.

Notwithstanding these differences, let us conduct an experiment to see how the demographics from Harvard look different from those given by the Almanac in terms of an empirical distribution.

As we will be handling proportions, let us scale the numbers down (by dividing each element by 100) so that they are expressed as percentages.

```{r}
class_props <- class_props %>% 
  mutate(Harvard = Harvard / 100,
         Almanac = Almanac / 100)
class_props
```

We will also write a function that computes the total variation distance (TVD) between two vectors.  

```{r}
compute_tvd <- function(x, y) {
  return(sum(abs(x - y)) / 2)
}
```

In this study, our observed value is the TVD between the distribution of students in the Harvard class and the Almanac. 

```{r}
harvard_diff <- compute_tvd(pull(class_props, Harvard), pull(class_props, Almanac))
harvard_diff
```

The Harvard class of 2024 has 2015 students. We can think of the process of sampling 2015 people to fill the "Harvard class" from those who "were attending" a four-year non-profit college in Fall 2018 and then examining their racial distribution. Of course, we cannot reach out to those individuals specifically; but we know the distribution of the entire population from which we want to sample. Therefore, we can simulate a great number of times what this "Harvard class" and compare it with what we know about the true Harvard class distribution, available in `harvard_diff`. 

Our sampling plan can be framed as a "boxes and marbles" problem, as we saw in the previous section. There are five "boxes" to choose from, where each corresponds to a race: White, Black, Hispanic, Asian, and Other. The goal is to place marbles (which correspond to students) in each of the boxes, where the probability of ending up in any of the boxes is given by the Almanac. 

This is an excellent fit for the `rmultinom` function. For example, here is one simulation of the proportion of races found in a "Harvard class." 

```{r}
sample_vector <- rmultinom(n=1, size=2015, prob = pull(class_props, Almanac)) / 2015
sample_vector
```

How far is our simulated class from the Almanac? We can compute the TVD to find out. 

```{r}
compute_tvd(sample_vector, pull(class_props, Almanac))
```

We wrap our work into a function.  

```{r}
one_simulated_class <- function(props) {
  sample_vector <- rmultinom(n=1, size=2015, prob = pull(props, Almanac)) / 2015
  return(compute_tvd(sample_vector, pull(props, Almanac)))
}
```

Let us simulate what 10,000 classes could look like. This will be contained in a vector called `sample_class_tvds`. Also, as before, we will use `map_dbl()` to do the work. 

```{r}
num_repetitions <- 10000
sample_class_tvds <- replicate(n = num_repetitions, one_simulated_class(class_props))
```

We can now visualize the result. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_class_tvds)) +
  geom_histogram(aes(x = sample_class_tvds, y = ..density..), 
                 bins = 30, fill = "darkcyan", color = 'gray')
```

Where does the true Harvard class lie on this histogram? We can plot a point geom to find out. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_class_tvds)) +
  geom_histogram(aes(x = sample_class_tvds, y = ..density..), 
                 bins = 70, fill = "darkcyan", color = 'gray') +
  geom_point(aes(x = harvard_diff, y = 0), size = 3, color = "salmon")
```

The orange dot shows the distance value of the Harvard value from the Almanac value. What we see is that the proportion of the races at Harvard is nothing like the national proportion.


### Asian Students Versus Others

The prior experiment looked at the proportion of all races. However, the claim given by the lawsuit is specifically about Harvard-admissible students who are Asian. We can now address this by refining our model to include only Asian students and non-Asian students. 

```{r}
class_props_asian <- tribble(~Race, ~Harvard, ~Almanac,
                             "Asian", 24.4, 8.1,
                             "Other", 75.6, 91.9)
class_props_asian
```

As before, we transform the data to be in terms of proportions.

```{r}
class_props_asian <- class_props_asian %>% 
  mutate(Harvard = Harvard / 100,
         Almanac = Almanac / 100)
class_props_asian
```

The proportion of Asian in private non-profit 4-year colleges is just 8.1% while the race occupies 24.4% of the Harvard freshman class. Let's recompute our observed TVD value. 

```{r}
harvard_diff <- compute_tvd(pull(class_props_asian, Harvard), pull(class_props_asian, Almanac))
harvard_diff
```

Re-running the simulation is easy. Note that instead of passing `class_props` as an argument to the function `one_simulated_class`, we pass the new tibble `class_props_asian`.

```{r}
num_repetitions <- 10000
sample_class_tvds <- replicate(n = num_repetitions, one_simulated_class(class_props_asian))
```


We again visualize the result. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_class_tvds)) +
  geom_histogram(aes(x = sample_class_tvds, y = ..density..), 
                 binwidth=0.002, fill = "darkcyan", color = 'gray')
```

Where does the observed value fall in this histogram? 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_class_tvds)) +
  geom_histogram(aes(x = sample_class_tvds, y = ..density..), 
                 binwidth=0.002, fill = "darkcyan", color = 'gray') +
  geom_point(aes(x = harvard_diff, y = 0), size = 3, color = "salmon")
```

We find that the result is the same; the Harvard proportion of Asian students is not at all like the national value. We can state, with great confidence, that Harvard enrolls much more Asian students than the national average. However, the reader should be cautioned not to accept these results as direct evidence against the suit's case. As noted at the outset of this section, we do not know the proportion of Harvard-admissible students and must instead rely on a national Almanac for reference. The base population of students can be very much different which is, in fact, something we anticipated.


## Significance Level

So far we have evaluated models by comparing some observation to a prediction made by a model. For instance, we compared:

* Racial demographics at Harvard University with the national Almanac at four-year non-profit private colleges. 
* 10,000 throws of an unknown six-sided die at hand with a fair six-sided die. 

Sometimes the observed statistic, e.g. the sample total variation distance (TVD), ended up close to the predictions; sometimes it ended up very far away. But how do we define what "close" or "far" is? And at what point does an observed statistic transition from being "close" to "far"? 

This section examines the *significance* of an observed statistic. To set up the discussion, we introduce another example still on the topic of academics: midterm scores in a computer science course at the University of Miami. 

### Prerequisites

Before starting, let's load the `tidyverse` as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

```{r message = FALSE, warning = FALSE, echo = FALSE}
set.seed(1)
```


### A midterm grumble? 

A Computer Science course at the University of Miami had 40 enrolled students and was divided into 3 lab sections. A Teaching Assistant (TA) leads the section. After a midterm exam was given, the students in one section noticed that their midterm scores were lower compared to students in the other two lab sections. They complained that their performance was due to the TA's teaching. The professor faced a dilemma: is it the case that the TA is at fault for poor teaching or are the students from that section more vocal about their grumbles following a exam?

If we were to fill that lab section with randomly selected students from the class, it is possible that their average midterm grade will look a lot like the score the grumbling students are unhappy about. It turns out that what we have stated here is a chance model that we can simulate. 

Let's have a look at the data from each (anonymized) student in the course. The following tibble `csc_labs` contains midterm and final scores, and the lab section the student is enrolled in. 

```{r message = FALSE, warning = FALSE}
csc <- read_csv("data/csc_labs.csv")
csc
```

We can use the dplyr verb `group_by` to examine the mean midterm grade as well as the number of students in each section. 

```{r message = FALSE}
lab_stats <- csc %>%
  group_by(section) %>%
  summarize(midterm_avg = mean(midterm),
            count = n())
lab_stats
```

Indeed, it seems that the section H students fared the worst, albeit by a small margin, among the three sections. Our statistic then is the mean grade of students in the lab section. Thus, our observed statistic is the mean grade from section H, which is about 68.56.  

```{r}
observed_statistic <- 68.56
```

We can also formally state our null and alternative hypothesis. 

__Null Hypothesis:__ The mean midterm grades of students in lab section H looks like the mean grades of a "section H" that is generated by randomly sampling the same number of students from the class.  

__Alternative Hypothesis:__ The section H midterm grades are too low. 

To form a random sample, we will need to sample *without* replacement 9 students from the course to fill up the theoretical "lab section H".  

```{r}
random_sample <- csc %>%
  select(midterm) %>%
  slice_sample(n = 9, replace = FALSE)
random_sample
```

We can look at the mean midterm score for this randomly sampled section. 

```{r}
mean(pull(random_sample, midterm))
```

Now that we know how to simulate one value, we can wrap this into a function. 

```{r}
one_simulated_mean <- function() {
  random_sample <- csc %>%
    select(midterm) %>%
    slice_sample(n = 9, replace = FALSE)
  return(mean(pull(random_sample, midterm)))
}
```

We will simulate the "section H lab" 10,000 times. Let us run the simulation! 

```{r}
num_repetitions <- 10000
sample_means <- replicate(n = num_repetitions, one_simulated_mean())
```

As before, we visualize the resulting distribution of grades.  

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_means)) +
  geom_histogram(aes(x = sample_means, y = ..density..), 
                 bins = 15, fill = "darkcyan", color = 'gray')
```

It seems that the grades cluster around 72. Where does the actual section H section lie? Recall that this value is available in the variable `observed_statistic`. We overlay a point geom to our above plot.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(sample_means)) +
  geom_histogram(aes(x = sample_means, y = ..density..), 
                 bins = 15, fill = "darkcyan", color = 'gray') + 
  geom_point(aes(x = observed_statistic, y = 0), size = 3, color = "salmon")
```

It seems that the observed statistic is "close" to the center of randomly sampled scores. 

### Choosing a significance level

Let's sort the sample means generated. We will examine in particular the value at 10% and 5% from the bottom. We will first turn to the value at 10%. 

```{r}
sorted_samples <- sort(sample_means)
sorted_samples[length(sorted_samples)*0.1]
```

We will plot this value on our empirical histogram, as well as the area to the left of it. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
colors <- c(rep("salmon",6), rep("darkcyan",9))

ggplot(tibble(sample_means)) +
  geom_histogram(aes(x = sample_means, y = ..density..), bins = 15,
                 fill = colors, color = 'gray') +
  geom_point(aes(x = sorted_samples[length(sorted_samples)*0.1], y = 0), size = 3, color = "red")
```

This means that in our simulation the mean midterm score is as low as about 63 90% of the time. Put differently, the chance of a midterm score *lower* than 63 occurring is 10%. 

The situation is not much different if we look at the value 5% from the bottom, which we find to be about 61. We redraw the situation in on our empirical histogram. 

```{r}
sorted_samples[length(sorted_samples)*0.05]
```

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
colors <- c(rep("salmon",5), rep("darkcyan",10))

ggplot(tibble(sample_means)) +
  geom_histogram(aes(x = sample_means, y = ..density..), bins = 15,
                 fill = colors, color = 'gray') +
  geom_point(aes(x = sorted_samples[length(sorted_samples)*0.05], y = 0), size = 3, color = "red")
```

This time, the chance of a mean midterm score at least as low as about 61 occurring is 5%.   

We say that the threshold point 63 is the 90% *significance level* and the threshold point 61 is at the 95% significance level. If we prefer an even higher level of significance, we could also look at the 99% significance level or even the 99.9% level; these are commonly referred to in the area of physics which rely on high significance to prove something axiomatic. It is important to note that these cut-off points are conventional only and do not have a strong theoretical backing. They were first established by the statistician Ronald Fisher in his seminal work [*Statistical Methods for Research Workers*](https://en.wikipedia.org/wiki/Statistical_Methods_for_Research_Workers). 

Regardless of the cut-off point, a significance level states that if an observed statistic remains above it the discovery is not significant. The same holds true for the flip-side, i.e., if we are looking for too large a value.   

### The verdict: is the TA guilty? 

We can set a modest significance level at 95% for the course case study. Of course, judgment is needed if the decision resulting from this study will cause the TA to be reprimanded -- we may tend towards a much more conservative significance level if so. 

We overlay the observed statistic on the empirical histogram. As before, the orange bars show the 95% significance region.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
colors <- c(rep("salmon",6), rep("darkcyan",9))

ggplot(tibble(sample_means)) +
  geom_histogram(aes(x = sample_means, y = ..density..), bins = 15,
                 fill = colors, color = 'gray') +
  geom_point(aes(x = observed_statistic, y = 0), size = 3, color = "red")
```

We see that the point does not fall within this region. We can check numerically how much area is to the left of the observed statistic. 

```{r}
sum(sample_means <= observed_statistic) / num_repetitions
```

Therefore, we conclude that the resulting low grades are likely not due to the TA's teaching. It may be wise to check in with the TA to get his take on the story.   


## Permutation Testing

In the previous section, we study the use of hypothesis testing. In this section we learn a simple method to compare two distributions using a method we call *permutation testing*. This allows us to decide if the two distributions come from the same underlying distribution.

<!--
* We have two sets of numerical data A and B, which respectively have the mean values $a$ and $b$, with difference $d = a - b$.
* We want to know, assuming that the two datasets have the same origin, how large (or small) chances are that the difference of $d$ emerges when we redivide the join of A and B into two groups, only retaining the sizes of A and B.
* For each such random split, we compute the difference in the mean values.
* By repeating such random splits many times, we can obtain distributions of the means.
--> 

### Prerequisites

Let us begin by loading the `tidyverse`.

```{r message = FALSE, warning = FALSE}
library(tidyverse)
```

### The effect of a tutoring program 

The tibble `finals` contains final exam grades in a Computer Science course for 105  students. They are divided into two groups, based on two different offerings of the course labeled `A` and `B`. The more recent offering `B` featured a tutoring program for students to receive help on assignments and exams. The course instructor is interested in finding out if the tutoring program boosted overall performance in the class, measured by a final exam. This could help the instructor and department decide if the program should continue or even be expanded. The dataset is collected over two semesters from the same Computer Science course taught at the University of Miami.

Let's first load the dataset. 

```{r message = FALSE, echo = FALSE}
finals <- read_csv("data/final_exams.csv")
finals
```

We can examine the number of enrolled students in each of the two offerings. 

```{r}
finals %>%
  group_by(class) %>%
  count()
```

It appears they are about equal. Let's now turn to a distribution of the students in the offering that featured the tutoring program (class `B`) compared to those in the offering without the program (class `A`). To generate an overlaid histogram, we use the positional adjustment argument `identity` and set an `alpha` so that the bars are drawn with slight transparency. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(finals) + 
  geom_histogram(aes(x = grade, y = ..density.., fill = class),
                 bins = 10, color = "gray",
                 alpha = 0.7, position = "identity")
```

By observation alone, it seems that the final scores of students in the offering where the tutoring program was available (`B`) is slightly to the right of the distribution corresponding to scores when the program did not exist (`B`). Could this be chalked up to chance?

As we have done throughout this chapter, we can address this question by means of a hypothesis test. We will state a null and alternative hypothesis that arise from the problem. 

__Null hypothesis:__ In the population, the distribution of final exam scores where the  tutoring program was available is the same as those when the service did not exist. The difference seen in the sample is because of chance. 

__Alternative hypothesis:__ In the population, the distribution of final exam scores when the tutoring program was available are, on average, *higher* than the scores when the program was not.  

According to the alternative hypothesis, the average final score in offering `B` should be higher than the average final score in offering `A`. Therefore, a natural test statistic we can use is the difference in the mean between the two groups. That is,

\[
\text{test statistic} = \mu_B - \mu_A
\]

where $\mu$ denotes the mean of the group. 

First, we form two vectors `finalsA` and `finalsB` that contain final scores with respect to the course offering. 

```{r}
finalsA <- finals %>%
  filter(class == 'A') %>% pull(grade)
finalsB <- finals %>%
  filter(class == 'B') %>% pull(grade)
```

The observed value of the statistic can be computed as the following. 

```{r}
observed_statistic <- mean(finalsB) - mean(finalsA)
observed_statistic
```

We can write a function that computes the statistic for us. We call it `mean_diff`. 

```{r}
mean_diff <- function(a, b) {
  return(mean(a) - mean(b))
}
```

Observe how it returns the same value for the observed statistic. 

```{r}
mean_diff(finalsB, finalsA)
```

To predict the statistic under the null hypothesis, we defer to an idea called the *permutation test*. 

### The Permutation Test 

Suppose that we are given the following vector of integers. 

```{r}
1:10
```

We can interpret these numbers as indices that refer to an element inside a vector. We imagine that the first half of indices belong to a group `A`, and the second half group `B`. 

Under the assumption of the null hypothesis, there should be no difference between the two distributions `A` and `B` with respect to the underlying population. For example, whether a final exam score belongs to the course offering `A` or `B` should have no effect on the mean final score. If so, there should be no consequences if we place both groups into a pot, shuffle them around, and compute the mean difference from the result. The resulting value we get from this process is one simulated value of the test statistic under the null hypothesis. 

The first bit of machinery we need is a function that shuffles a sequence of integers. We actually already know one: `sample`. 

```{r}
shuffled <- sample(1:10)
shuffled
```

In this example, `sample` receives a vector of numbers 1 through 10 and returns the result after shuffling them. We might also call the result a *permutation* of the original sequence -- hence, its namesake. 

If we again interpret the resulting vector as indices, we take the first half to be the indices of the shuffled group A and the second half the shuffled group B. 

```{r}
shuffled[1:5]  # shuffled group A
shuffled[6:10] # shuffled group B
```

The remaining work then is to compute the difference in means between the shuffled groups. 

The function `one_mean_difference` puts everything together. It receives two vectors, `a` and `b`, puts them together in a pot, and deals out two shuffled vectors with the same size as `a` and `b`, respectively. The function returns the value of the simulated statistic by calling the functional `compute_statistic`. For this example, we use `mean_diff`. 

```{r}
one_difference <- function(a, b, compute_statistic) {
  pot <- c(a, b)
  sample_indices <- sample(1 : length(pot))
  shuffled_a <- pot[sample_indices[1 : length(a)]]
  shuffled_b <- pot[sample_indices[(length(a) + 1) : length(pot)]]
  return(compute_statistic(shuffled_a, shuffled_b))
}
```

We are now ready to perform a permutation test for the tutoring program example. We would like to simulate the test statistic under the null hypothesis multiple times and collect the values into a vector. As before, we can use `replicate`. We will simulate 10,000 values. 

```{r}
differences <- replicate(n = 10000, one_difference(finalsA, finalsB, mean_diff))
```

### Conclusion of the Test

Let's visualize the results. 

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(tibble(differences)) + 
  geom_histogram(aes(x = differences, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 20) +
  geom_point(aes(x = observed_statistic, y = 0), color = "salmon", size = 3)
```

First observe how the distribution is centered around 0. Under the assumption of the null hypothesis, there is no difference between the final exam averages in the two course offerings and, therefore, the difference clusters around 0. 

Also observe that the observed test statistic is quite far from the center. To get a better sense of how far, we compute the P-value. 

```{r}
sum(differences >= observed_statistic) / 10000
```

This means that the chance of obtaining a mean difference at least as large as $6.10$ is around 8%. By standards of the conventional cut-off points we have discussed, we would have enough evidence to refute the null hypothesis at a 90% significance level. Would this be enough to convince us that the tutoring program is indeed effective? Let us consider for a moment what it would mean if it does not. 

If we were to demand a higher significance level, say 95%, our observed statistic is no longer significant. The logical next step would be to conclude that the null hypothesis is true, bearing the implication that the tutoring program is ineffective. This would be a statistical fallacy! Even if our results are not significant at the desired level, we do *not* take the null hypothesis to be true. Put another way, we fail to reject the null hypothesis. That is a mouthful! 

The problem here is a lack of evidence. A lack of evidence does not prove that something *does not* exist, e.g., the tutoring program is *not* effective; it very well could be, but our study missed it. Indeed, our permutation test only evaluated one criteria -- that is, difference in final exam scores -- as a measure for improvement. There are other criteria we could have considered, like class participation, which may have benefited from the program. It would be up to the judgment of the department on how to use these results in deciding the merit of the tutoring program. 

### Another Permutation Test: NFL and NCAA players 

We end this section with one more example of a permutation test: comparing the yardage of NFL and NCAA players. We load in both datasets and select out the column containing year and yardage information. We also add a new column to each tibble containing which league (NFL or NCAA) the yardage pertains to, so we can refer to it later. 

```{r message = FALSE}
ncaa_with_yards <- read_csv("data/ncaa-receivers-2005-2011.csv") %>%
  select(Yards, Year) %>%
  mutate(league = "NCAA")
nfl_with_yards <- read_csv("data/nfl_receivers.csv") %>% 
  select(Yds, Year) %>%
  rename(Yards = Yds) %>%
  mutate(league = "NFL")
```

Let us use NCAA data for 2011 and NFL data from 2019 and 2018. 

```{r}
ncaa_with_yards <- ncaa_with_yards %>%
  filter(Year == 2011)
nfl_with_yards <- nfl_with_yards %>%
  filter(Year == 2019 | Year == 2018)
```

It will be easier if we merge these two tibbles together. 

```{r}
all_yards <- bind_rows(ncaa_with_yards, nfl_with_yards)
```

We can glance at how much yardage information we have in each league. 

```{r}
all_yards %>%
  group_by(league) %>%
  count()
```

We observe that they are roughly the same. Before proceeding any further, we should visualize the yardage with an overlaid histogram. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(all_yards) + 
  geom_histogram(aes(x = Yards, y = ..density.., fill = league),
                 bins = 30, color = "gray",
                 alpha = 0.7, position = "identity")
```

We state our hypothesis tests. 

__Null hypothesis:__ In the population, the distribution of yardage information in the NFL is, on average, the same as in the NCAA. 

__Alternative hypothesis:__ In the population, the distribution of yardage information in the NFL is, on average, different from the NCAA. 

Note that the alternative hypothesis, unlike the tutoring program example, does not care whether the yardage information in the NFL is *higher* or *less* than that of the NCAA. It only states that some *difference* exists.  

We can apply the same test statistic before but we take the absolute value of the difference to account for this change. 

\[
\text{test statistic} = | \mu_B - \mu_A | 
\]

Note how it does not matter which group ends up as A and likewise for B. Let's write a function to compute this statistic; it is a slight variation of the `mean_diff` we saw before. 

```{r}
mean_abs_diff <- function(a, b) {
  return(abs(mean(a) - mean(b)))
}
```

### The Test 

We form two vectors `ncaa_yardage` and `nfl_yardage` that contain the yardage information with respect to the league. 

```{r}
ncaa_yardage <- ncaa_with_yards %>% pull(Yards)
nfl_yardage <- nfl_with_yards %>% pull(Yards)
```

The observed value of the statistic can be computed as the following. 

```{r}
observed_statistic <- mean_abs_diff(ncaa_yardage, nfl_yardage)
observed_statistic
```

We are now ready to perform the permutation test. As before, let us simulate the test statistic under the null hypothesis 10,000 times. 

```{r}
differences <- replicate(n = 10000, one_difference(ncaa_yardage, nfl_yardage, mean_abs_diff))
```

### Conclusion of the Test

We are ready to visualize the results. 

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(tibble(differences)) + 
  geom_histogram(aes(x = differences, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 15) +
  geom_point(aes(x = observed_statistic, y = 0), color = "salmon", size = 3)
```

The observed statistic is quite far away from the distribution of simulated test statistics. Let's do a quick numerical check. 

```{r}
sum(differences >= observed_statistic) / 10000
```

The chance of obtaining a mean absolute difference of $53.9$ is practically 0. We can safely reject the null hypothesis at a significance level over 99.9%. This confirms that the yardage information between NCAA and NFL players are likely, on average, to be different. 
