---
output:
  pdf_document: default
  html_document: default
---
# Sampling

Sampling is an important concept in data science.
When a data scientist receives a data to analyze, the first question she asks is "how was this data generated?"
The question she asks is about:
* the original source of data (e.g., is it through an experiment, is it a combination of multiple data sources, etc.)
* any processing applied to data (e.g., data conversion, filling in blanks, etc.)
The "processing" part of the question often includes, "if the data collector chose to exclude some data rows or columns, why and how she did it?"
The data selection question becomes important particularly when it is impossible to include every possible "subject" in the data.
In this chapter, we explore methods for sampling.


## Situations Demanding Sampling 

Here are examples in which "selections" become important.
* You want to conduct research on the height and weight on the entire population in U.S.
* You want to conduct research on the buying and selling prices of the cars in a large online used car retailer in light of the demographics of the people who bought and sold the cars, but you want it very quick.
* You need to analyze patient trends in a large hospital network, and the hospitals in the network are able to provide information for their inpatients and outpatients.

The first one is an impossible problem because we cannot account the entire population in U.S.
The second one is different in nature than the first because the retailer has the entire data set at hand, but the time is of essence, and so there is no time to examine all the cars.
In the last case, the hospitals in the network are able to provide information for their inpatients and outpatients.
However, if there are tens of hospitals with diverse sizes of patients on their records, does it make sense to combine all the data?
If the most populous hospital has 250000 patients while the least one 5000, can we join the two or do have to include only a small fraction from the first so that the two extremes contribute the same number of patients?

This is where the selection of data comes in to play.
The selection can be systematic.
For example, you may choose to select every 50th patient in the most populous hospital top reduce the number to 5000 to make the data size equal to the size of the smallest.
We can apply this systematic reduction to the other hospitals to reduce the size equal to 5000 for all of them.
Now every hospital will contribute the same number, 5000, of patients!

But wait a second, what if the least populous is in Wyoming, the one with the least people in the U.S. and the most populous one is California, the most populous one in the U.S.
Will selecting the same number of people from each hospital do justice?
Shouldn't we select patients from the hospitals according to the states, or more precisely, the number of population they cover?
There is no an obvious answer to the question.
The data scientist in charge will have to decide on the adjustments to make.


## Sample or Not Sample

We will study how sampling can help produce the results without forcing you to examine the entire data.

We will again make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

Here is a data of the receivers in the National Football League in the 2017, 2018, and 2019 seasons.
The data comes from [The Football Database](http://www.footballdb.com/index.html).

If you are not familiar with (American) football, in each play of the game (excluding the kick-off), one team is offending and the other is defending.
The ultimate goal of the offending team is to carry the ball beyond the goal line of the opponent team, which we call the "end zone".
The offending team has four chances in its turn.
If they cannot bring the ball beyond the goal line, an alternate option is to advance the ball position at least 10 yards forward from the starting position.
If they cannot accomplish neither options, the defensive team will take over the ball and the role switches between the defensive and the offensive teams.
There are two ways to move the ball forward.
One is by running, where a player carries the ball and runs forward.
When the player cannot move forward in the play, the last position of the ball becomes the next starting position.
The other is by passing, where the quarterback player, who is responsible for initiating the play, throws a ball towards a player of the same team.
If the player that the quarterback targets catches the ball, the player can continue the play by running forward with the ball.
The idea in the running play is the runner knits through the regions that the defensive players cover and runs as far as possible.
The idea in the passing play is the receiver runs to a place beyond the reach of the defensive team and the quarterback throws the ball so the receiver can catch it.

The rule book of the National Football League is thick and contains so many rules.
An important point here is that in each play there are multiple players who are eligible to receive the ball that the quarterback throws, but not everyone in the offensive team.
In a rare event, the ball that the quarterback throws hits a helmet of some player, bounces back to the quarterback, the quarterback catches the ball, and runs with it.
Anyhow, in one game of a football, multiple players may become targets of the same quartterback.
Sometimes the target catches the ball and runs after catching, and sometimes they drop the ball with the passing play being unsuccessful.
Important information about players who are eligible to receive balls, *receivers*, consists of the following:

* how many times the quarterback targeted the receiver (Target);
* after receiving the ball how many yards the receiver ran (Yards After Catch);
* when the receiver caught the ball how many yards the ball advanced forward from the position that the play started (Receiving Yards); Note that the quarterback may have to run backwards before throwing the ball and so the distance can be greater than the advance distance of the ball towards the determination of the 10 yard requirement;
* whether the receiver carried the ball beyond the goal line (Touchdown)
* if not touch down, whether the receiver's catching resulted in the accomplishment of the 10-yard advance (First down)

Let's start by loading the data into R.


```{r}
path <- "data/nfl_receiving0.csv"
receivers0 <- read_csv(path)
receivers0
```
The attributes of the data are:
* The name of receiver (Player)
* The year of data (Year)
* The team of receiver (Team)
* The number of games the receiver became a target (Gms)
* The number of times the receiver caught the ball (Rec)
* The total receiving yards (Yds)
* The average yards per receiving (Avg)
* The average yards per game (YPG)
* The longest yards received where the attachment of "t" indicates "touchdown" (Lg)
* The number of touchdowns (TD)
* The number of first downs (FD)
* The number of times the receiver became the target (Yac)
* The total yards after catch (YAC)

The data has a bit of a problem:
* the same name may appear multiple times because the data is over three seasons
* the longest yard information may have a "t" attached.

For the former, we can solve the problem by creating a column to combine Player and Year.
For the latter, we can use the following series of editing to create two new columns in which the two values in the longest yard are split into the yardage and the value indicating whether the longest yard is also a touchdown (1 for "yes" and 0 for "no").

We will show how to do the process.
An impatient reader may just do this to download the data after cleaning.

```{r}
path <- "data/nfl_receivers.csv"
receivers_data <- read_csv(path)
```

The function below takes the longest yard string and returns a two element list whose first entry is the actual number and the second is the flag of "t".
It uses `str_length` to obtain the number of characters in a string and `str_sub` to obtain substrings.
Basically, it checks if the substring consisting only of the last character of `s` is equal to "t", and then if so, return the substring preceding the "t" and 1; otherwise, it returns s and 0.
The function combines the two values as a list using `c` and converts both values to integers.
To generate the integer from a string representing (e.g., going from "49" to 49) is possible using the `strtoi` function.

```{r}
split_lg <- function(s) {
  if (str_length(s) == 0) {
    return(c(0,0))
  }
  else {
    l <- str_length(s)
    suffix <- str_sub(s,l,l)
    if (suffix != "t") {
      return(c(strtoi(s),0))
    }
    else {
      return(c(strtoi(str_sub(s,1,l-1)),1))
    }
  }
}
```


With the function `split_lg`, we add three new columns to the data using the following code.
The function `cbind` add a new column to an existing data.
The first argument is the database to which to add the new column and the second argument is the new column.
The last line of the code saves the result as a new data. 

```{r}
lgall <- receivers0$Lg
length(lgall)
lg_yd0 <-vector("integer",length(lgall))
lg_td0 <-vector("integer",length(lgall))
names <- receivers0$Player
year <- receivers0$Year
names_year <- 1:length(lgall)
for (i in 1:length(lgall)) {
  parts <- split_lg(lgall[i])
  lg_yd0[i] <- parts[1]
  lg_td0[i] <- parts[2]
  names_year[i] <- paste(names[i], year[i])
}
receivers1 <- cbind(receivers0, lg_yd0)
receivers2 <- cbind(receivers1, lg_td0)
receivers_data <- cbind(receivers2, names_year)
print(sum(lg_yd0))
write_csv(receivers_data,'data/nfl_receivers.csv')
```

So, here we reload from the data set after cleaning.

```{r}
path <- "data/nfl_receivers.csv"
receivers_data <- read_csv(path)
```

Let us say we will study the performance of the receivers with respect to their yardage (Yds).

How many yards did the best receiver gain and how many receivers are there?
Also, are there are receivers with 0 gains?

```{r}
col <- receivers_data$Yds
max(col)
length(col)
min(col)
```
The largest value is 1725, there are about 1511 rows in the data set, and wow, there is a player with negative gain.
How could the negative total be possible?
The yardage is the distance from the point of throwing to the point where the ball ended and so it can be negative.

What is the average of the total yardage value among the 1511 entries?
Computing the average in question is simple:
obtain the column for the attribute, sum the entries, and divide it by the number of rows.

```{r}
col <- receivers_data$Yds
Yds_avg <- sum(col)/length(col)
```

So the average is around 252.7.

Let us look at the histogram of the yardage values.
We locate the position of the average, which we have calculated into `Yds_avg` in red.


```{r dpi=80,  fig.align="center", message = FALSE}
bins <- seq(0,1750,20)
ggplot(receivers_data, aes(x = Yds)) +
  geom_histogram(aes(y = ..density..), color = "grey", breaks = bins) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 2)

```

It is hard to tell, just by looking at the histogram, that 252.72 is the average.

Let us see how sampling qill takes us close to the ideal value.
Let us we will sample 10% of the rows; that is, 151 rows.

```{r}
colSub <- sample(col,size=151,replace=FALSE)
sum(colSub)/length(colSub)
```

How close do we get close to the true average if we are to repeat the 10% sampling many times?

Let's do with 1000 repetitions.

```{r}
num_reps = 1000
outcomes <- vector("double",num_reps)
for (i in 1:num_reps) {
  colSub <- sample(col,size=151,replace=FALSE)
  outcomes[i] <- sum(colSub)/length(colSub)
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```

Now this is 10000 repetitions.

```{r}
num_reps = 10000
outcomes <- vector("double",num_reps)
for (i in 1:num_reps) {
  colSub <- sample(col,size=151,replace=FALSE)
  outcomes[i] <- sum(colSub)/length(colSub)
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```

How about we increase the percentage to 20%?

```{r}
num_reps = 10000
outcomes <- vector("double",num_reps)
for (i in 1:num_reps) {
  colSub <- sample(col,size=302,replace=FALSE)
  outcomes[i] <- sum(colSub)/length(colSub)
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```


```{r}
sum(outcomes)/length(outcomes)
```

How does the histogram look like if we make systematic choices?
There are many possibilities for mechanically selecting 151 positions.
Suppose we will select "every Xth position starting from W".
The following code does the job.
It uses four parameters in the calculation: `size` for the size of the length of the list in the original data, `need` for the length of the index list, `start` for the starting position, and `gap` for the distance between the two position.
For example, to execute every 3rd position starting from 2, the value of `start` is 3 and the value of `gap` is 2.
The program generates a vector of length `need`.
The elements of the vector serve as the place holders for the index values.
Then the program generates the sequence `start + gap * (i - 1)` for `i` ranging from 1 to `need` and store the value in the position `i` in the index value vector.
The value of the index the program generates by adding `gap` may exceed the value of `size` at some point (actually, it may occur many times).
Each time the value exceeds, the program makes an adjustment by subtracting the value of `size`.
For example, if `start` is 1 and `gap` is 100, then after 16 additions, the value becomes 1601.
If the value of `size` is 1511, we subtract 1511 from 1601 to make it 90.

Because of the adjustmnet process, the value that the program generates go up and then down.
Since the elements of the vector simply state the places from which the samples come, we can safely reorder the elements.
So, we execute `sort` on the positions and update the vector with the result of sorting.

```{r}
size <- 1511
need <- 151
start <- 1
gap <- 20
pos <- vector("integer",need)
x <- start
for (i in 1:need) {
  pos[i] <- x
  x <- x + gap
  if (x > size) x <- x - size
}
pos <- sort(pos)
pos
```

The list the program shows at the end is the result from generating index values with the starting point 1 and the gap of 20.
We can write a function for the index generation into a function that takes the four valus as arguments.

```{r}
mechanical_selection <- function(size, need, start, gap) {
  pos <- vector("integer",need)
  x <- start
  for (i in 1:need) {
    pos[i] <- x
    x <- x + gap
    if (x > size) x <- x - size
  }
  pos <- sort(pos)
  return(pos)
}
mechanical_selection(1000,10,1,27)
```

Using the method `mechanical_selection`, let us see how the average yardage turns up in the same number of repetitions with 151 samples.
In the following program, we create gap sequences of 100 elements each and start position sequences of 100 elements.
The elements of the former are 14 * i, where i ranges from 1 to 100.
The elements of the latter are 13 * i, where i ranges from 1 to 100.
Using all possible combinations of the elements produce 100 * 100 sequences with 1511 as `size` and 151 as `need`.
Using two iterations, where the second occurs for each choice of the first element, we generate 10000 cases.

```{r}
gaps <- 1:100
gaps <- gaps * 14
starts <- 1:100
starts <- starts * 13
num_reps = 10000
size <- 1511
need <- 151
outcomes <- vector("double",num_reps)
for (i in 1:100) {
  for (j in 1:100) {
    p <- (i-1) * 100 + j
    pos <- mechanical_selection(size, need, starts[i], gaps[j])
    colSub <- col[pos]
    outcomes[p] <- sum(colSub)/need
  }
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```


```{r}
sum(outcomes)/length(outcomes)
```

As you can see, the mechanical generations generate a histogram that is much narrower than the one we derive from random position selections.

We may think of ourselves as lucky. If we use 1000 start points and 10 different gaps, the histogram looks much flatter.

```{r}
gaps <- 1:10
gaps <- gaps
starts <- 1:1000
starts <- starts
num_reps = 10000
size <- 1511
need <- 151
outcomes <- vector("double",num_reps)
for (i in 1:1000) {
  for (j in 1:10) {
    p <- (i - 1) * 10 + j
    pos <- mechanical_selection(size, need, starts[i], gaps[j])
    colSub <- col[pos]
    outcomes[p] <- sum(colSub)/need
  }
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```

```{r}
sum(outcomes)/length(outcomes)
```
So, the use of mechanical index selections requires a careful treatment.
Depending on how mechanically we select the index values, we may obtain different results.


## Probabilities of Things Happening

A probability is the likelihood of a thing happening.
In our favorite topic of coin tossing, we have said previously that the the coin shows "Heads" as likely as it does "Tails".
We have also said that the chances are 50-50 that the coin shows "Heads".
The numbers 50 represent the probability in percentage that the coin shows "Heads" (or "Tails").
A fair coin has no memory.
When one turns "Heads" in a throws, it informs us nothing about what will happen in the next throw.
Suppose a referee of football has thrown "Heads" three times in a row, what do you think will happen in the next throw.
Many people think that the next throw must be "Tails" because "Tails" has not turned up in any of the past three throws.
However, noting that the coin is fair, and has no memory, the next throw has equal probability of 50% for both "Heads" and "Tails".

Now, suppose you, as an officiate member of NFL, have received a new coin (because a coin probably gets wear and tear, and of course, a new shiny coin looks more spiffy than an old, dull coin).
How can you be sure that the coin is fair?
Say you have thrown it three times and seen "Heads" three times in a row.
Do you believe that it is a fair coin?

In the program below, we simulate the process of throwing a fair coin a number of times.
The value of `num_throws` represents the number of throws.
To simulate the action of throwing a fair coin, we use `sample` and select from a two-element list [1,2] that we represent using a vector `counts`.
We assume that 1 represents "Heads" and 2 "Tails", but these assignments are exchangeable.
For each throw, we get 1 or 2 as a random sample.
We then examine the results appearing in the list `sample` has generated and compute the number of times 1 appears and the number of times 2 appears, and we record the numbers in a two-element list `counts`.
By dividing the quantities in `counts` by the number of throws, we get the probabilities of "Heads" and "Tails" of the fair coin.


```{r}
counts <- vector("double",2)
choices <- 1:2
num_throws <- 1000
throws <- sample(choices,size=num_throws,replace=TRUE)
for (i in 1:num_reps) {
  counts[throws[i]] <- counts[throws[i]] + 1
}
counts[1] <- counts[1] / num_throws
counts[2] <- counts[2] / num_throws
counts
```

Let us turn the program into a function that takes the number of throws as its argument and returns the two-element probabilities.

```{r}
throw_est <- function(x) {
  counts <- vector("double",2)
  choices <- 1:2
  throws <- sample(choices,size=x,replace=TRUE)
  for (i in 1:x) {
    counts[throws[i]] <- counts[throws[i]] + 1
  }
  counts[1] <- counts[1] / x
  counts[2] <- counts[2] / x
  return(counts)
}
```

Here we use 50 throws.

```{r}
coin <- c("Heads", "Tails")
counts <- throw_est(50)
outcome_df <- data.frame(throw=coin,prob=counts)
# bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = coin, y=counts), color = "grey") +
  geom_bar(stat="identity",fill="steelblue", width=0.5) +
  geom_text(aes(label=counts), vjust=-0.3, size=3.5)
```

Now with 100 throws.

```{r}
coin <- c("Heads", "Tails")
counts <- throw_est(100)
outcome_df <- data.frame(throw=coin,prob=counts)
# bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = coin, y=counts), color = "grey") +
  geom_bar(stat="identity",fill="steelblue", width=0.5) +
  geom_text(aes(label=counts), vjust=-0.3, size=3.5)
```

Here we use 1000 throws.

```{r}
coin <- c("Heads", "Tails")
counts <- throw_est(1000)
outcome_df <- data.frame(throw=coin,prob=counts)
# bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = coin, y=counts), color = "grey") +
  geom_bar(stat="identity",fill="steelblue", width=0.5) +
  geom_text(aes(label=counts), vjust=-0.3, size=3.5)
```

As we did for the average yardage, let us observe when we execute the simulation of 1000 throws multiple times how the probability of 1 (="Heads") turns up.

```{r}
outcomes <- vector("double",1000)
for (i in 1:num_reps) {
  result <- throw_est(1000)
  outcomes[i] <- result[1]
}
# outcomes
outcome_df <- tibble(outcomes)
bins <- seq(0.44,0.56,0.010)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=0.001)
```

Even with 1000 repetitions, the shape is not smooth.

_The Law of Average_

The law of average is the term that refers to the understanding that for an event occurring with no relation to when in the past the event occurred, the ratio that we compute as
\[
\frac{\mathrm{the~number~of~times~the~event~occurred}}{\mathrm{the~number~of~times~the~event~could~have~occurred}}
\]
reaches the inherent ratio in which the event occurs as we make more observations.
To put differently, if there is an experiment that
* we can repeat as many time as we want,
* the experiment has a number of possible outcomes, and
* the outcomes of the previous events do not affect the outcome of any future event
then, we can assume that
* for each outcome, there is a non-negative constant quantity, which we call the *probability* of the outcome,
* the sume of the probability of all the outcomes of the experiment is equal to 1, and
* for each outcome $e$, if we are toi execute the experiment some $N$ times, count how many times $e$ occurs, and compute the ratio $e/N$, then the larger $N$ we use, the ratio $e/N$ gets closer to the probability of $e$ appears as the outcome.




## Population and Statistics

Recall that in our prior exploration of the receiver data, we saw that the average total yards is a little over 211 yards and that the distribution shows a drastic decline.
A static that is popular along with average is *median*, which is the value at the middle position in the data after reordering of the values.
Let's load the data and examine the median of the yardage value.

But first, let's load the library.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

As before, we load the edited data into `receivers_data`.

```{r}
path <- "data/nfl_receivers.csv"
receivers_data <- read_csv(path)
```

We can ask for the median of the yardage values, which appears in the attribute `Yds`.

```{r}
median(receivers_data$Yds)
```

So there is a big gap between the median and the average.
A synonym for average is *mean*.
The statistical literature prefers "mean" over "average".
Our library `tidyverse` has the same reference.
We can get the "mean" of the `Yds` values with `mean` in place of `median`.
Let us look at the ratio of the mean to the average.


```{r}
mean(receivers_data$Yds)/median(receivers_data$Yds)
```

The ratio is close to 2; that is, the mean is almost as twice as much of the median.
What does this mean?
Noting that the gradually decline histogram of the yardage values, we guess that there are so many large values.

As we mentioned earlier, a receiving yardage value can be negative, occurring when the receiving position is behind the position at the play started.
So, the total yardage value may have a negative entry, especially when someone who is not regularly in a receiving position (such as a quarterback) received the ball in one game.

Using `filter` we can find all the data with negative yardage.


```{r}
filter(receivers_data, Yds < 0)
```
There are 18 of them. You may notice that some of them are indeed quarterbacks, e.,g., Andy Dalton, Derek Carr, and Kirk Cousins.

What is the minimum of the negative values?

```{r}
min(receivers_data$Yds)
```

Negative 11 it is.

We have found that the median is 131 and the minimum is $-11$.
The difference between the two is 142.
Of the 1511 data values we have, the median sits right in the middle.
There are $(1511-1)/2 = 755$ values below the median and the same number of values above the median.
The value that is 142 away from the median on the opposite side of $-11$ is 273.
To make the mean equal to the average, the data values above the median only have to be around 273.
The deviation of the mean from the median indicates that the values above the median can be much higher than 273.
Let up look at the top 100 (about 6.6%) of the yardage values.
The `sort` function can be in the decreasing order.
You have only to append `decreasing=TRUE` in the argument.

```{r}
y <- sort(receivers_data$Yds, decreasing=TRUE)
y[1:100]
```

The highest is a breathtaking 1725 and the 100th one is 828.
Here is a list of top 100 (i.e., the yardage value greater than or equal to 828) in the descending order of the yardage value.
Note that Mike Thomas and Julio Jones appear in the top 10.

```{r}
filter(receivers_data, Yds >= 828) %>% arrange(desc(Yds))
```
There are cases, by the way, where two median exists.
Such an event occurs exactly when the number of values is an even number.
If there are 10 values, the 5th and the 6th values in the ascending order are the two medians.
The one at the lower position in the order is the *odd median* and the other one, i.e., the one at the higher position in the order, is the *even median*.
Som additional terminology is worth defining.
The *population* is a group of objects from which we draw samples.
In the case of the receivers data, the American football players who were targets in any play in the 2017, 2018, and 2019 seasons form the population.
The *population size* is the number of objects in the population.
1511 is the population size.
The football players that we drew from the data set are *samples*.
Each football player we included (implictly or explicitly) in our analysis is among the samples.
The *maximum*, the *minimum*, and the *median* are *order statistics*, referring to their values from a list after reordering in ascending order.
The *mean* (or *average*) is a statistic, but is not an order statistic.




<!-- 
## Use of samples

Let's start by drawing some samples. Our examples are based on the `mpg` data set.

```{r}
mpg
```

__Sampling Rows of a Table__

Each row of a data table represents an individual; in `mpg`, each individual is a car model. Sampling individuals can thus be achieved by sampling the rows of a table.

The contents of a row are the values of different variables measured on the same individual. So the contents of the sampled rows form samples of values of each of the variables.

__Deterministic Samples__

When you simply specify which elements of a set you want to choose, without any chances involved, you create a *deterministic sample*.

You have done this many times, for example by using `slice()`:

```{r}
slice(mpg, c(3, 25, 100))
```

You have also used `filter()`:

```{r}
filter(mpg, manufacturer == "land rover")
```

While these are samples, they are not random samples. They don't involve chance.

__Probability Samples__

For describing random samples, some terminology will be helpful.

A *population* is the set of all elements from whom a sample will be drawn.

A *probability sample* is one for which it is possible to calculate, before the sample is drawn, the chance with which any subset of elements will enter the sample.

In a probability sample, all elements need not have the same chance of being chosen.

__A Random Sampling Scheme__

For example, suppose you choose two people from a population that consists of three people A, B, and C, according to the following scheme:

* Person A is chosen with probability 1.

* One of Persons B or C is chosen according to the toss of a coin: if the coin lands heads, you choose B, and if it lands tails you choose C.

This is a probability sample of size 2. Here are the chances of entry for all non-empty subsets:

```
A: 1 
B: 1/2
C: 1/2
AB: 1/2
AC: 1/2
BC: 0
ABC: 0
```

Person A has a higher chance of being selected than Persons B or C; indeed, Person A is certain to be selected. Since these differences are known and quantified, they can be taken into account when working with the sample.

__A Systematic Sample__

Imagine all the elements of the population listed in a sequence. One method of sampling starts by choosing a random position early in the list, and then evenly spaced positions after that. The sample consists of the elements in those positions. Such a sample is called a *systematic sample*.

Here we will choose a systematic sample of the rows of `mpg`. Let's start by preparing a version of the data frame that includes the row index for better visual inspection.

```{r}
mpg_with_index <- mpg %>%
  mutate(row_index = 1:nrow(mpg)) %>%
  select(row_index, everything())
mpg_with_index
```

We will now pick one of the first 10 rows at random, and then we will select every 10th row after that.

```{r}
# Choose a random start among rows 0 through 9; then take every 10th row.
start <- sample(1:10, size = 1)
slice(mpg_with_index, seq(start, nrow(mpg_with_index), by = 10))
```

Run the cell a few times to see how the output varies.

This systematic sample is a probability sample. In this scheme, all rows have chance $1/10$  of being chosen. For example, Row 23 is chosen if and only if Row 3 is chosen, and the chance of that is $1/10$.

But not all subsets have the same chance of being chosen. Because the selected rows are evenly spaced, most subsets of rows have no chance of being chosen. The only subsets that are possible are those that consist of rows all separated by multiples of 10. Any of those subsets is selected with chance 1/10. Other subsets, like the subset containing the first 11 rows of the table, are selected with chance 0.

__Random Samples Drawn With or Without Replacement__

In this course, we will mostly deal with the two most straightforward methods of sampling.

The first is random sampling with replacement, which (as we have seen earlier) is the  behavior of `sample(replace = TRUE)` when it samples from an vector.

The other, called a "simple random sample", is a sample drawn at random *without* replacement. Sampled individuals are not replaced in the population before the next individual is drawn. This is the kind of sampling that happens when you deal a hand from a deck of cards, for example.

In this chapter, we will use simulation to study the behavior of large samples drawn at random with or without replacement.

Drawing a random sample requires care and precision. It is not haphazard, even though that is a colloquial meaning of the word "random". If you stand at a street corner and take as your sample the first ten people who pass by, you might think you're sampling at random because you didn't choose who walked by. But it's not a random sample – it's a *sample of convenience*. You didn't know ahead of time the probability of each person entering the sample; perhaps you hadn't even specified exactly who was in the population.



<!--

In the previous chapter, we learned the concepts of randomness, iteration, and simulation.

An important part of data science consists of making conclusions based on the data in random samples. To make correct interpretations of their results, data scientists must possess first-hand knowledge of exactly what random samples are.

In this chapter we will take a more careful look at sampling, with special attention to the properties of large random samples.

__Prerequisites__

We will again make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```





## Empirical Distributions

In data science, the word "empirical" means "observed". Empirical distributions are distributions of observed data, such as data in random samples.

In this section we will generate data and see what the empirical distribution looks like.

### Prerequisites

Let's load in the tidyverse as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

Our setting is a simple experiment: rolling a die multiple times and keeping track of which face appears. The table `die` contains the numbers of spots on the faces of a die. All the numbers appear exactly once, as we are assuming that the die is fair.

```{r}
die <- tibble(face = 1:6)
die
```

### A Probability Distribution

The histogram below helps us visualize the fact that every face appears with probability 1/6. We say that the histogram shows the *distribution* of probabilities over all the possible faces. Since all the bars represent the same percent chance, the distribution is called *uniform on the integers 1 through 6*.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(die) + 
  geom_histogram(aes(x = face, y =  ..density..), bins = 6, color = "gray")
```

Variables whose successive values are separated by the same fixed amount, such as the values on rolls of a die (successive values separated by 1), fall into a class of variables that are called *discrete*. The histogram above is called a *discrete* histogram. Its bins are specified by the vector `die_bins` and ensure that each bar is centered over the corresponding integer value.

It is important to remember that the die can't show 1.3 spots, or 5.2 spots – it always shows an integer number of spots. But our visualization spreads the probability of each value over the area of a bar. While this might seem a bit arbitrary at this stage of the course, it will become important later when we overlay smooth curves over discrete histograms.

Before going further, let's make sure that the numbers on the axes make sense. The probability of each face is 1/6, which is 16.67% when rounded to two decimal places. The width of each bin is 1 unit. So the height of each bar is 16.67% per unit. This agrees with the horizontal and vertical scales of the graph.

### Empirical Distributions

The distribution above consists of the theoretical probability of each face. It is not based on data. It can be studied and understood without any dice being rolled.

*Empirical distributions*, on the other hand, are distributions of observed data. They can be visualized by *empirical histograms*.

Let us get some data by simulating rolls of a die. This can be done by sampling at random with replacement from the integers 1 through 6. We have used `sample(replace = TRUE)` for such simulations before. But now we will introduce a `dplyr` method for doing this with data frames. 

The `dplyr` function is called `slice_sample()`. It draws at random from the rows of a data frame, with an argument to sample with replacement. It also gives an an argument `n` for the sample size, and it returns a data frame consisting of the rows that were selected. 

Here are the results of 10 rolls of a die.

```{r}
slice_sample(die, n = 10, replace = TRUE)
```

We can use the same method to simulate as many rolls as we like, and then draw empirical histograms of the results. Because we are going to do this repeatedly, we define a function empirical_hist_die that takes the sample size as its argument, rolls a die as many times as its argument, and then draws a histogram of the observed results.

```{r}
empirical_hist_die <- function(n) {
  die_sample <- slice_sample(die, n = n, replace = TRUE)
  ggplot(die_sample, aes(x = face, y = ..density..)) + 
    geom_histogram(bins = 6, color = "gray")
}
```

### Empirical Histograms

Here is an empirical histogram of 10 rolls. It doesn't look very much like the probability histogram above. Run the cell a few times to see how it varies.

```{r dpi=80,  fig.align="center", message = FALSE}
empirical_hist_die(10)
```

When the sample size increases, the empirical histogram begins to look more like the histogram of theoretical probabilities.

```{r dpi=80,  fig.align="center", message = FALSE}
empirical_hist_die(100)
```

```{r dpi=80,  fig.align="center", message = FALSE}
empirical_hist_die(1000)
```

As we increase the number of rolls in the simulation, the area of each bar gets closer to 16.67%, which is the area of each bar in the probability histogram.

### The Law of Averages 

What we have observed above is an instance of a general rule.

If a chance experiment is repeated independently and under identical conditions, then, in the long run, the proportion of times that an event occurs gets closer and closer to the theoretical probability of the event.

For example, in the long run, the proportion of times the face with four spots appears gets closer and closer to 1/6.

Here "independently and under identical conditions" means that every repetition is performed in the same way regardless of the results of all the other repetitions.


## Sampling from a Population

The law of averages also holds when the random sample is drawn from individuals in a large population.

As an example, we will study a population of flight delay times. The data frame `flights` contains all 336,776 flights that departed from New York City in 2013. It is drawn from the [Bureau of Transportation Statistics](http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0) in the United States. The help page `?flights` contains more documentation about the dataset.  

### Prerequisites

We will continue to make use of the tidyverse in this chapter. Moreover, we will load the nycflights13 package which has the `flights` table we will be using.  

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

There are 336,776 rows in `flights`, each corresponding to a flight. Note that some delay times are negative; those flights left early.

```{r}
flights
```

One flight departed 43 minutes early, and one was 1301 minutes late. The other delay times were almost all between -10 minutes and 200 minutes, as the histogram below shows.

```{r}
slice_min(flights, dep_delay)
```

```{r}
slice_max(flights, dep_delay)
```

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = seq(-50, 200, 10))
```

For the purposes of this section, it is enough to zoom in on the bulk of the data and ignore the 1.83% of flights that had delays of more than 150 minutes. This restriction is just for visual convenience; the table still retains all the data.

```{r}
nrow(filter(flights, dep_delay > 150)) / nrow(flights)
```

```{r warning=FALSE}
delay_bins <- seq(-50, 150, 10)
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```

The height of the (-10, 0] bar is just under 6%, which means that just under 60% of the flights had delays between 0 and 10 minutes. That is confirmed by counting rows:

```{r}
nrow(filter(flights, dep_delay > -10 & dep_delay <= 0)) / nrow(flights)
```

### Empirical Distribution of the Sample

Let us now think of the 336,776 flights as a population, and draw random samples from it with replacement. It is helpful to package our code into a function. The function `empirical_hist_delay()` takes the sample size as its argument and draws an empiricial histogram of the results.

```{r}
empirical_hist_delay <- function(n) {
  flights_sample <- slice_sample(flights, n = n, replace = TRUE)
  ggplot(flights_sample, aes(x = dep_delay, y = ..density..)) + 
    geom_histogram(breaks = delay_bins, color = "gray")
}
```

As we saw with the dice, as the sample size increases, the empirical histogram of the sample more closely resembles the histogram of the population. Compare these histograms to the population histogram above.

```{r dpi=80,  fig.align="center", warning = FALSE, message = FALSE}
empirical_hist_delay(10)
```

```{r dpi=80,  fig.align="center", warning = FALSE, message = FALSE}
empirical_hist_delay(100)
```

The most consistently visible discrepancies are among the values that are rare in the population. In our example, those values are in the right hand tail of the distribution. But as the sample size increases, even those values begin to appear in the sample in roughly the correct proportions.

```{r dpi=80,  fig.align="center", warning = FALSE, message = FALSE}
empirical_hist_delay(1000)
```

### Convergence of the Empirical Histogram of the Sample

What we have observed in this section can be summarized as follows:

For a large random sample, the empirical histogram of the sample resembles the histogram of the population, with high probability.

This justifies the use of large random samples in statistical inference. The idea is that since a large random sample is likely to resemble the population from which it is drawn, quantities computed from the values in the sample are likely to be close to the corresponding quantities in the population.


## Empirical Distribution of a Statistic

The Law of Averages implies that with high probability, the empirical distribution of a large random sample will resemble the distribution of the population from which the sample was drawn.

The resemblance is visible in two histograms: the empirical histogram of a large random sample is likely to resemble the histogram of the population.

### Prerequisites

We will continue to make use of the tidyverse in this section. Moreover, we will load the nycflights13 package which has the `flights` table we will be using.  

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

As a reminder, here is the histogram of the delays of all the flights in `flights`, and an empirical histogram of the delays of a random sample of 1,000 of these flights.

```{r warning=FALSE}
delay_bins <- seq(-50, 150, 10)
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```

```{r warning=FALSE}
sample_1000 <- slice_sample(flights, n = 1000, replace = TRUE)
ggplot(sample_1000, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```

The two histograms clearly resemble each other, though they are not identical.


### Parameter

Frequently, we are interested in numerical quantities associated with a population.

* In a population of voters, what percent will vote for Candidate A?
* In a population of Facebook users, what is the largest number of Facebook friends that the users have?
* In a population of United flights, what is the median departure delay?

Numerical quantities associated with a population are called *parameters*. For the population of flights in `flights`, we know the value of the parameter "median delay":

```{r}
median(flights$dep_delay, na.rm = TRUE)
```

Note the argument `na.rm` (read: "NaN remove") used in this function call. An interesting property of the `flights` dataset is that it has missing values, e.g., not all rows have a value in the `dep_delay` column. We toggle a flag `na.rm` so that R knows to drop these rows before computing the median.  

The function `median()` returns the median (half-way point) of a column or vector. Among all the flights available, the median delay was -2 minute. This isn't surprising: more than 50% of flights in the population had early departures of 2 minutes or more:

```{r}
nrow(filter(flights, dep_delay <= -2)) / nrow(flights)
```

Neat!

__Note.__ The percent isn't exactly 50 because of "ties," that is, flights that had early departures of exactly 2 minutes. There were 21,516 such flights. Ties are quite common in data sets, and we will not worry about them in this course.

```{r}
nrow(filter(flights, dep_delay == -2))
```

### Statistic

In many situations, we will be interested in figuring out the value of an unknown parameter. For this, we will rely on data from a large random sample drawn from the population.

A statistic (note the singular!) is any number computed using the data in a sample. The sample median, therefore, is a statistic.

Remember that `sample_1000` contains a random sample of 1000 flights from united. The observed value of the sample median is:

```{r}
median(sample_1000$dep_delay, na.rm = TRUE)
```

Our sample – one set of 1,000 flights – gave us one observed value of the statistic. This raises an important problem of inference:

__The statistic could have been different.__ A fundamental consideration in using any statistic based on a random sample is that *the sample could have come out differently*, and therefore the statistic could have come out differently too.

```{r}
a_new_sample <- slice_sample(flights, n = 1000, replace = TRUE)
median(a_new_sample$dep_delay, na.rm = TRUE)
```

Run the cell above a few times to see how the answer varies. Often it is equal to 2, the same value as the population parameter. But sometimes it is different.

__Just how different could the statistic have been?__ One way to answer this is to simulate the statistic many times and note the values. A histogram of those values will tell us about the distribution of the statistic.

Let's recall the main steps in a simulation.

### Simulating a Statistic 

We will simulate the sample median using the steps we set up in an earlier chapter when we started studying simulation. You can replace the sample size of 1,000 by any other sample size, and the sample median by any other statistic.

__Step 1: Decide which statistic to simulate.__ We have already decided that: we are going to simulate the median of a random sample of size 1,000 drawn from the population of flight delays.

__Step 2: Write the code to generate one value of the statistic.__ Draw a random sample of size 1000 and compute the median of the sample. We did this in the code cell above. Here it is again, encapsulated in a function.

```{r}
random_sample_median <- function(x) {
  a_new_sample <- slice_sample(flights, n = 1000, replace = TRUE)
  return(median(a_new_sample$dep_delay, na.rm = TRUE))
}
```

__Step 3: Decide how many simulated values to generate.__ Let's do 5,000 repetitions.

__Step 4: Write the code to generate an array of simulated values.__ As in all simulations, we start by creating a vector with length 5,000 where all the simulated values will be stored. The map will call the `random_sample_median()` function for each of its elements and store the calculated median in the corresponding element of the vector. Upon completion, the vector will contain all the simulated values, and we call this vector `medians`. 

The simulation takes a noticeable amount of time to run. That is because it is performing 5,000 repetitions of the process of drawing a sample of size 1,000 and computing its median. That's a lot of sampling and repeating!

```{r}
num_repetitions <- 5000
medians <- map_dbl(1:num_repetitions, random_sample_median)
```

The simulation is done. All 5,000 simulated sample medians have been collected in the array medians. Now it's time to visualize the results.

### Visualization

Here are the simulated random sample medians displayed in the data frame `simulated_medians`.

```{r}
simulated_medians <- tibble(sample_median = medians)
simulated_medians
```

We can also visualize the simulated data using a histogram. The histogram is called an *empirical histogram of the statistic*. It displays the *empirical distribution* of the statistic. Remember that *empirical* means *observed*.

```{r}
ggplot(simulated_medians) + 
  geom_histogram(aes(x = sample_median, y = ..density..), color = "gray", bins = 3)
```

You can see that the sample median is very likely to be about -2, which was the value of the population median. Since samples of 1,000 flight departure delays are likely to resemble the population of delays, it is not surprising that the median delays of those samples should be close to the median delay in the population.

This is an example of how a statistic can provide a good estimate of a parameter.

### The Power of Simulation

If we could generate all possible random samples of size 1,000, we would know all possible values of the statistic (the sample median), as well as the probabilities of all those values. We could visualize all the values and probabilities in the probability histogram of the statistic.

But in many situations including this one, the number of all possible samples is large enough to exceed the capacity of the computer, and purely mathematical calculations of the probabilities can be intractably difficult.

This is where empirical histograms come in.

We know that by the Law of Averages, the empirical histogram of the statistic is likely to resemble the probability histogram of the statistic, if the sample size is large and if you repeat the random sampling process numerous times.

This means that simulating random processes repeatedly is a way of approximating probability distributions *without figuring out the probabilities mathematically or generating all possible random samples*. Thus computer simulations become a powerful tool in data science. They can help data scientists understand the properties of random quantities that would be complicated to analyze in other ways.


-->
