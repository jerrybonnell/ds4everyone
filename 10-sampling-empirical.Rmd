# Sampling and Empirical Distributions

An important part of data science consists of making conclusions based on the data in random samples. In order to correctly interpret their results, data scientists have to first understand exactly what random samples are.

In this chapter we will take a more careful look at sampling, with special attention to the properties of large random samples.

__Prerequisites__

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

Let's start by drawing some samples. Our examples are based on the `mpg` data set.

```{r}
mpg
```

__Sampling Rows of a Table__

Each row of a data table represents an individual; in `mpg`, each individual is a car model. Sampling individuals can thus be achieved by sampling the rows of a table.

The contents of a row are the values of different variables measured on the same individual. So the contents of the sampled rows form samples of values of each of the variables.

__Deterministic Samples__

When you simply specify which elements of a set you want to choose, without any chances involved, you create a *deterministic sample*.

You have done this many times, for example by using `slice()`:

```{r}
slice(mpg, c(3, 25, 100))
```

You have also used `filter()`:

```{r}
filter(mpg, manufacturer == "land rover")
```

While these are samples, they are not random samples. They don't involve chance.

__Probability Samples__

For describing random samples, some terminology will be helpful.

A *population* is the set of all elements from whom a sample will be drawn.

A *probability sample* is one for which it is possible to calculate, before the sample is drawn, the chance with which any subset of elements will enter the sample.

In a probability sample, all elements need not have the same chance of being chosen.

__A Random Sampling Scheme__

For example, suppose you choose two people from a population that consists of three people A, B, and C, according to the following scheme:

* Person A is chosen with probability 1.

* One of Persons B or C is chosen according to the toss of a coin: if the coin lands heads, you choose B, and if it lands tails you choose C.

This is a probability sample of size 2. Here are the chances of entry for all non-empty subsets:

```
A: 1 
B: 1/2
C: 1/2
AB: 1/2
AC: 1/2
BC: 0
ABC: 0
```

Person A has a higher chance of being selected than Persons B or C; indeed, Person A is certain to be selected. Since these differences are known and quantified, they can be taken into account when working with the sample.

__A Systematic Sample__

Imagine all the elements of the population listed in a sequence. One method of sampling starts by choosing a random position early in the list, and then evenly spaced positions after that. The sample consists of the elements in those positions. Such a sample is called a *systematic sample*.

Here we will choose a systematic sample of the rows of `mpg`. Let's start by preparing a version of the data frame that includes the row index for better visual inspection.

```{r}
mpg_with_index <- mpg %>%
  mutate(row_index = 1:nrow(mpg)) %>%
  select(row_index, everything())
mpg_with_index
```

We will now pick one of the first 10 rows at random, and then we will select every 10th row after that.

```{r}
# Choose a random start among rows 0 through 9; then take every 10th row.
start <- sample(1:10, size = 1)
slice(mpg_with_index, seq(start, nrow(mpg_with_index), by = 10))
```

Run the cell a few times to see how the output varies.

This systematic sample is a probability sample. In this scheme, all rows have chance $1/10$  of being chosen. For example, Row 23 is chosen if and only if Row 3 is chosen, and the chance of that is $1/10$.

But not all subsets have the same chance of being chosen. Because the selected rows are evenly spaced, most subsets of rows have no chance of being chosen. The only subsets that are possible are those that consist of rows all separated by multiples of 10. Any of those subsets is selected with chance 1/10. Other subsets, like the subset containing the first 11 rows of the table, are selected with chance 0.

__Random Samples Drawn With or Without Replacement__

In this course, we will mostly deal with the two most straightforward methods of sampling.

The first is random sampling with replacement, which (as we have seen earlier) is the  behavior of `sample(replace = TRUE)` when it samples from an vector.

The other, called a "simple random sample", is a sample drawn at random *without* replacement. Sampled individuals are not replaced in the population before the next individual is drawn. This is the kind of sampling that happens when you deal a hand from a deck of cards, for example.

In this chapter, we will use simulation to study the behavior of large samples drawn at random with or without replacement.

Drawing a random sample requires care and precision. It is not haphazard, even though that is a colloquial meaning of the word "random". If you stand at a street corner and take as your sample the first ten people who pass by, you might think you're sampling at random because you didn't choose who walked by. But it's not a random sample – it's a *sample of convenience*. You didn't know ahead of time the probability of each person entering the sample; perhaps you hadn't even specified exactly who was in the population.

## Empirical Distributions

In data science, the word "empirical" means "observed". Empirical distributions are distributions of observed data, such as data in random samples.

In this section we will generate data and see what the empirical distribution looks like.

Our setting is a simple experiment: rolling a die multiple times and keeping track of which face appears. The table die contains the numbers of spots on the faces of a die. All the numbers appear exactly once, as we are assuming that the die is fair.

```{r}
die <- tibble(face = 1:6)
die
```

### A Probability Distribution

The histogram below helps us visualize the fact that every face appears with probability 1/6. We say that the histogram shows the *distribution* of probabilities over all the possible faces. Since all the bars represent the same percent chance, the distribution is called *uniform on the integers 1 through 6*.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(die) + 
  geom_histogram(aes(x = face, y =  ..density..), bins = 6, color = "gray")
```

Variables whose successive values are separated by the same fixed amount, such as the values on rolls of a die (successive values separated by 1), fall into a class of variables that are called *discrete*. The histogram above is called a *discrete* histogram. Its bins are specified by the vector `die_bins` and ensure that each bar is centered over the corresponding integer value.

It is important to remember that the die can't show 1.3 spots, or 5.2 spots – it always shows an integer number of spots. But our visualization spreads the probability of each value over the area of a bar. While this might seem a bit arbitrary at this stage of the course, it will become important later when we overlay smooth curves over discrete histograms.

Before going further, let's make sure that the numbers on the axes make sense. The probability of each face is 1/6, which is 16.67% when rounded to two decimal places. The width of each bin is 1 unit. So the height of each bar is 16.67% per unit. This agrees with the horizontal and vertical scales of the graph.

### Empirical Distributions

TBA

## Sampling from a Population

TBA 

## Empirical Distribution of a Statistic

TBA 



