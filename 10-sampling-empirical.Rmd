---
output:
  pdf_document: default
  html_document: default
---

# The Power of Sampling

In the previous chapter we learned that we can make selections by chance using randomness.
In this chapter we will use sampling more aggressively and learn to exploit the power of sampling.
Specifically, we will learn that: 

* By drawing samples, we are able to observe what an unknown distribution might look like
* By conducting experiments using sampling we can assess how unlikely or likely a phenomenon is at hand. 

In both cases, the number of samples we draw plays an important role.

## Introduction

This chapter introduces some sampling preliminaries that we need for building our experiments. 

### Prerequisites

Before starting, let's load the `tidyverse` as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

We will familiarize ourselves with the use of sampling from tibbles, or data frames.

In this chapter specifically, we will use the `mpg` data set that comes packaged with the tidyverse. Here is a preview of it: 

```{r}
mpg
```

Each row of the tibble represents an individual; in the `mpg` tibble, each individual is a car model. Sampling individuals can thus be achieved by sampling the rows of a table.

The contents of a row are the values of different variables measured on the same individual. So the contents of the sampled rows form samples of values for each of the variables.

### Deterministic Samples

Recall that when working with data frames, we often anticipate the columns of the table to represent the properties we can obtain from an individual object. A collection of the properties representing an individual is a *record* or a *data object*. The rows of a data frame are the data records. The row $X$ and column $Y$ of the data frame is the property $Y$ of the data object $X$.

By "sampling" we mean to select rows from the data frame. If we want to select distinct rows of the data frame, there is a convenient function `slice` for the action, which you may recall. The first argument of the function call specifies the source for sampling, and the other specifies the rows we draw using vector representation. The following generates a new tibble with rows 3, 25, and 100 of `mpg`. 

```{r}
mpg_sub <- slice(mpg, c(3, 25, 100))
mpg_sub
```

Note that `slice` does not care if the row numbers appearing in the second argument are all different or if the row numbers are given in non-decreasing order. For instance, we can create a tibble containing four repeats of Audi A4's and Chevrolet Corvette's. 

```{r}
mpg_sub <- slice(mpg, c(3, 25, 3, 25, 3, 25, 3, 25))
mpg_sub
```

In the above examples, we knew beforehand which rows would appear in the sampled data frame because we specified explicitly the corresponding index of rows to include (e.g., four repeats of row 3 and row 25). We call such a sampling process with a selection vector a *deterministic sample*. The determinism refers to the non-existence of chance during the selection process.  

An alternative to directly specifying the row numbers is specifying a condition on a variable for a record to be in the sample data frame. We have also seen this before. This is the `dplyr` verb `filter`. 

```{r}
mpg_sub <- filter(mpg, manufacturer == "land rover")
mpg_sub
```

The condition on the variable of a record either puts it in the subset or it does not. Exactly one of the two must occur. Like `slice`, `filter` also selects the records deterministically. If you are not convinced, you could run through each of the 234 rows of `mpg` by hand and manually check whether the manufacturer is indeed a `land rover`. Those that pass the check will end up in the sample data frame and, by the end of it all, you would end up with the same result as `filter` -- no manual effort needed!  

### Probabilistic Sampling

The antonym of "deterministic" is "non-deterministic", which is a concept that plays an important role in computer science. The meaning of it is quite obvious: it refers to a process that is not deterministic. However, it is quite vague in that it does not state *how* to draw the non-determinism. Data science prefers a more concrete form of non-determinism called *randomness*, which we have seen before. We call the process of sampling that uses randomness to make its draws *probabilistic sampling* or *random sampling*. 

The objects from which we draw samples are called the *population*. There are multiple types of populations and the determining factors of the types is its quantity and the way the samples are generated. Following are some (non-comprehensive) examples of populations and how we sample from the data.

* *Accessible Data with Succinct Definition* We want to study the choice of major at a college. The registrar's office can generate a complete list of all the students currently attending the college. Using filtering, we can narrow down the population to a specific group (e.g., the full-time sophomore students). We can then sample from the group or we can choose to include all the students in the group in our study.
* *Continuous Population Requiring Discretization* We want to study the quality of air based on how ``blue'' the skies are. There is no clear definition of the population. The geographical location of measurement equipment, the area to cover in the sky, and the time of measurement can be factors in determining the samples. The determination of these factors essentially *discretize* the continuous data. After determining all these factors succinctly, a technician can make a measurement.
* *Data with Succinct Definition Beyond Reach* We want to study the relationship between the height and weight of the people living in the United States. The population appears to have a clear definition, but it is difficult to determine who gets in the population because the population is transient (due to babies coming to life, people moving out of the country, etc.). By specifying at which point of time the person must be living in the United States, the definition can be succinct. The problem is that it is impossible to include everyone in the population. There are more than 300 million people, and we have no way of knowing who those people are.
* *Non-existing Population Requiring Active Generation* We want to examine the fairness of a coin. Each time we throw the coin, we generate a record. Throwing the coin N times you get a population of N records, but you may want to use the entire population for the analysis.

### Systematic or Random, Replace or not Replace, Uniform or Non-uniform

When we get down to the business of sampling, some critical decisions must be decided before starting to sample.

* Is the sampling to be done systematically or not?
* Can the same record appear more than once in the sampled data frame? 
* Do the records have a equal chance of becoming a sample? 

### Systematic Sampling

In systematic samples, we choose samples in a systematic manner. For example, we can select all students whose university-issued ID number has an odd number in the second-to-last position from the population of all students in a university. Also, we could select, from the entire population of the United States, all voters whose mailing address has a postal code ending in 1, whose street address has a digit 2, owns a car, and whose license plate has either the letter T or the numeral 6.

Here is an example of making systematic sampling on the `mpg` data set using `dplyr` verbs. Let's start by preparing a version of the data frame that includes the row index for better visual inspection.

```{r}
mpg_with_index <- mpg %>%
  mutate(row_index = 1:nrow(mpg)) %>%
  select(row_index, everything())
mpg_with_index
```

We will now pick one of the first 10 rows at random, and then we will select every 10th row after that.

```{r}
# Choose a random start among rows 0 through 9; then take every 10th row.
start <- sample(1:10, size = 1)
slice(mpg_with_index, seq(start, nrow(mpg_with_index), by = 10))
```

Run the code a few times to see how the output varies.

This is a combination of systematic sampling and random sampling. The starting point is random and there are 10 possibilities, and we pick one from these 10 possibilities with probability 10% (i.e., one in ten chances). The selection after determining the starting point is deterministic; we select every tenth element from the starting point. Therefore, there are just ten different sample data sets out of the original data set.

There are many other ways of selecting one out of ten records from the data set. We could expand the systematic selection to assembling every 10 rows into groups and selecting exactly one from each group. This selection would open up more possibilities. 

However, note that there exist combinations that can never be generated. For instance, in both of the schemes we just described, it is impossible to generate a sampled data frame that contains the very first two records. Can you see why? 

### To Replace or Not To Replace?

When we do random sampling, there are two main strategies. One is to prohibit any record from appearing more than once in the sampled data frame. The other is, of course, not to impose such a restriction.  

We call the first strategy *sampling without replacement* and the second *sampling with replacement*.
Most of the sampling we conduct in this course is sampling *with* replacement. The reasons for this will follow in the next section.   

### Uniform or Non-Uniform?

There are situations in which the population we want to observe is a mixture of several groups, but the representation is not equal among them. For example, a university may have a 50:50 ratio among male and female students but a dataset covering some of the student population has a 20:80 ratio instead. We may want to sample four male students for every female student sampled in order to remain true to the gender distribution in the student population. 


## Empirical Distributions

One of the most important applications of sampling is to obtain an approximation for a true distribution, which we often do not know. Think of a question like: "what percent of people in the United States are taller than six feet?" To attempt an answer, we would need to define who are "the people in the United States" and then engage in an enormous effort to collect the heights for all qualifying individuals -- a task that is not only tedious, but impossible to accomplish. 

Enter sampling. Instead of trying to account for every individual in the country, we can *sample* people from the population and approximate the height of people in the United States from the samples we collect. Our hope is that any histogram we construct from our sampled population will be close enough to the one we would have if we could record the heights from the entire population. All the heights from the U.S. population follows what we call the *true distribution*, and those from our sampled population the *empirical distribution*.   

<!-- Keep in mind that the human height carries inaccuracies because the way a person stands affects height measurement.
We might say beyond a certain level of granularity, such as one tenth of an inch, it would be impossible to produce consistent measurements.
According the [Guiness World Record](https://www.guinnessworldrecords.com/news/2019/2/a-history-of-record-breaking-giants-100-years-after-the-tallest-man-in-the-world-511577), the tallest person has ever lived with a irrefutable height measurement is Robert Wadlow (February 22, 1918 - July 15, 1940), whose record is stunning 8 feet and 11.1 inches (2.72 meters).
Assuming none will beat him, we can assume that an active range of height is from 0 feet 0 inches to 9 feet 0 inches with the increment of 0.1 inches.
With the active range and the granularity, there are $9 * 12 * 10 = 1080$ bins.
We can thus imagine having a histogram with 1080 bins with the entire qualifying population and think of the histograms we generate from sampling is an approximation of this 1080-bin histogram.
We refer to the one with the entire population as the *true distribution* and the one we generate from samples *empirical distributions* (or *observed distributions*).
With the view that the 1080 bins are nonsplittable, we cannot increase the number of bins.
Alternatively, assuming that the values can be further specific (e.g., by temporary freezing a person and then putting him/her on a height scale), the distributions are *continuous distributions*. --> 

Let's examine what an empirical distribution looks like with an example of throwing die.  

### Prerequisites

As before, let's load in the tidyverse as usual. 

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

### Throwing a 6-sided die

There are six faces to a (fair) die. The outcome of throwing a die is the face that turns up. 

```{r}
die <- tibble(face = 1:6)
die
```

Since the die is fair, the probability of the faces are all equal to each other and each is exactly $1/6$. The *true distribution* then is the probability where each of the faces is exactly $1/6$. Here is what that distribution looks like. Observe that the height is equal for all bars, at the level of $1/6$.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(die) + 
  geom_histogram(aes(x = face, y =  ..density..), bins = 6, color = "gray")
```

Does that mean that if you throw the die six times, you would see each of the faces exactly once? Not at all! 

Here is a quick counterargument: Assume that your intuition is correct. After throwing the die five times you observed five different faces. You could then predict the face of the sixth one to be the one that appears next. In fact, you could apply the same logic to each consecutive five throws to predict the next one. What have we done here? Our observation is leading to a realization that, after the first five throws, the remaining throws actually become *deterministic*. This contradicts the *randomness* we are expecting from the die. 

Therefore, the proportion of faces you see after throwing a die multiple times can be substantially different from the expected "$1/6$ for each face".

Note that the sampling we are about to conduct is different from the example of sampling heights from the U.S. population on three counts.

* The fair die may not exist in the real world, so we use a tibble that represents a fair die.
* The population exists only in our throwing of the die; that is, each time we throw the die, the throw and its outcome becomes a new member of the population.
* Because we can generate a sample any number of times, the population is actually infinite in size.

Nevertheless, we know what the true distribution looks like. It is our ggplot histogram just above. 

#### An Empirical Distribution

We now generate an empirical distribution using simulation. We used previously `sample` for generating samples from a vector. Here we will use the `dplyr` function `slice_sample` for sampling from a data frame. It draws at random from the rows of a data frame, with an argument to sample with replacement. It also receives an argument `n` for the sample size, and it returns a data frame consisting of the rows that were selected. 

Here are the results of 10 rolls of a die.

```{r}
slice_sample(die, n = 10, replace = TRUE)
```

Run the cell above a few times and observe how the faces selected changes. 

We can adjust the sample size by changing the number given to the `n` argument. Let's generalize the call by  writing a function that receives a sample size `n` and generates the empirical histogram for this sample size. 

```{r}
empirical_hist_die <- function(n) {
  die_sample <- slice_sample(die, n = n, replace = TRUE)
  ggplot(die_sample, aes(x = face, y = ..density..)) + 
    geom_histogram(bins = 6, color = "gray")
}
```


Here is an empirical histogram of 10 rolls. Note how it does not look anything like the true distribution from above. Run the cell a few times to see how it varies.

```{r dpi=80,  fig.align="center", message = FALSE}
empirical_hist_die(10)
```

When we increase the sample size, we see that the distribution gets closer to the true distribution.

```{r dpi=80,  fig.align="center", message = FALSE}
empirical_hist_die(100)
```

```{r dpi=80,  fig.align="center", message = FALSE}
empirical_hist_die(1000)
```

The phenomenon we have observed - the empirical distribution growing closer and closer to the true distribution as the sample size is increased - is an important concept in data science. In fact, it is so important that it has a name: we call it the *law of averages*. 

A critical requirement for the law of averages to be applicable is that the samples come from the same underlying distribution and do not depend on the drawing of other samples, e.g. the rolling of a 2 does not make the rolling of a 6 more likely on the next roll. This idea is sometimes called sampling "independently and under identical conditions", where the resulting distribution is "independently and identically distributed".

### Pop quiz: why sample with replacement? 

The careful reader may have noticed that in the calls to `slice_sample` in this section, the `replace` argument has been set to `TRUE`, i.e. the sampling is done with replacement. Why not sample without? 

You may have already guessed at an answer: if we sample without replacement, we would not be able to make more than 6 draws since we would have run out of faces on the die to choose from! Here is what happens when trying to sample 10 times without replacement.  

```{r}
slice_sample(die, n = 10, replace = FALSE)
```

We simply get back the 6-sided die! So sampling with replacement is a requirement for the dice roll example. But, there is another problem when sampling without replacement. Let's look at the true distribution again. 

```{r}
ggplot(die) + 
  geom_histogram(aes(x = face, y =  ..density..), bins = 6, color = "gray")
```

When we sample *without* replacement, we are effectively removing the possibility of that event happening from future draws. Put another way, this would be the same as somehow erasing or "deleting" a face from the die before drawing again. For instance, let's check the true distribution after rolling a 2. 

```{r}
selected_face <- 2  # assume a 2 was rolled
slice(die, -selected_face) %>%
  ggplot() + 
  geom_histogram(aes(x = face, y =  ..density..), bins = 6, color = "gray")
```

If we were to sample again from this die, the probability of rolling any of the faces is no longer the same as when we rolled the 2. For one thing, the probability of rolling the faces $1, 3, 4, 5, 6$ has increased to 20% (or $1/5$) and it is impossible to roll a 2. The underlying distribution has fundamentally changed, confirmed by the above ggplot. 

Recall that a prerequisite for the law of averages to work is that the drawing of samples must be done "independently and under identical conditions". Sampling without replacement is a clear violation of this assumption. And yet, the story does not end there. In our example, there were only 6 individuals to choose from -- the faces of a 6-sided die. If we were to increase the number of individuals that we could sample from, say the entire U.S. population, then the effect observed here actually becomes negligible. Sampling without replacement remains an important method for sampling, especially in drug studies with treatment/control groups where it is physically not possible to sample with replacement -- cloning people remains the stuff of science fiction, at least for now :-) Therefore, our hope is that both sampling plans will generate similar results.  


## Sampling from a Population

The Law of Averages can be useful when the population from which to draw samples is very large.

As an example, we will study a population of flight delay times. The tibble `flights` contains all 336,776 flights that departed from New York City in 2013. It is drawn from the [Bureau of Transportation Statistics](http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0) in the United States. The help page `?flights` contains more documentation about the dataset.  

### Prerequisites

We will continue to make use of the tidyverse in this section. Moreover, we will load the nycflights13 package, which has the `flights` table we will be using.  

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

There are 336,776 rows in `flights`, each corresponding to a flight. Note that some delay times are negative; those flights left early.

```{r}
flights
```

One flight departed 43 minutes early, and one was 1301 minutes late. The other delay times were almost all between -10 minutes and 200 minutes, as the histogram below shows.

```{r}
slice_min(flights, dep_delay)
```

```{r}
slice_max(flights, dep_delay)
```

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = seq(-50, 200, 1))
```

We are interested in the bulk of the data here, so we can ignore the 1.83% of flights with delays of more than 150 minutes.
We will only use the core part of the data excluding the 1.83% using the `filter` method as we show below.
Note that the table `flight` still has all the data.

```{r}
nrow(filter(flights, dep_delay > 150)) / nrow(flights)
```

```{r warning=FALSE}
delay_bins <- seq(-50, 150, 1)
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```

We group together delay values in size 10 intervals, so we can see better.

```{r warning=FALSE}
delay_bins <- seq(-50, 150, 10)
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```
The tallest bar in the histogram is the (-10, 0] bar is just below 0.06, which is equal to 6%.
There are ten values of delay minutes in the bin, we multiply the below-0.06% by 10 to assess that the delays in the interval occupy slightly below 60%.
We can confirm the visual assessment by counting rows:

```{r}
nrow(filter(flights, dep_delay > -10 & dep_delay <= 0)) / nrow(flights)
```

### Empirical Distribution of the Sample

Let us now think of the 336,776 flights as a population, and draw random samples from it with replacement.
As we may try using various values for the number of samples, let us define a function for sampling and plotting.
The function `empirical_hist_delay()` takes the sample size as its argument, which we call `nn`, and draws an empirical histogram of the results.
The reason that we use `nn` instead of the most natural single-letter name `n` is that the function `slice_sample` has an argument whose name is `n`. 

```{r}
empirical_hist_delay <- function(nn) {
  flights_sample <- slice_sample(flights, n = nn, replace = TRUE)
  ggplot(flights_sample, aes(x = dep_delay, y = ..density..)) + 
    geom_histogram(breaks = delay_bins, color = "gray")
}
```

As we saw in the simulation for throwing a die, the more samples we have, the closer the empirical histogram of the sample becomes to the histogram of the population.
Let us compare two samle sizes 10 and 100.

```{r dpi=80,  fig.align="center", warning = FALSE, message = FALSE}
empirical_hist_delay(10)
```

```{r dpi=80,  fig.align="center", warning = FALSE, message = FALSE}
empirical_hist_delay(100)
```

We may notice various differences between the two plots.
The plots may vary each time we execute the code.
Of all the differences that emerge when we execute the two codes and compare the plots, the most notable are the number of bars and how far the bars reach to the right without a gap.
We notice that in the case of 100 samples, there are always bars far to the right and there almost is also some presence in each bar between the tallest bar and the farthest one.

Here is a plot with 1000 samples.
We see the plot is much closer to the one with the entire flight data.

```{r dpi=80,  fig.align="center", warning = FALSE, message = FALSE}
empirical_hist_delay(1000)
```

### The Empirical Histogram at the Ultimum

From the experiments we have conducted with 10, 100, and 1000 samples, we have observed that:

> The larger the sample gets, the closer the empirical distribution becomes to the true distribution, which we obtain from the true population.

> At some large size, with high probability, the approximation of the true distribution we produce through sampling becomes almost indistinguishable from the true one.

Because of the two properties, we can use random samples in statistical inference.
The indistinguishable nature of empirical distributions is quite powerful.
When the population is large, we can turn to sampling to generate a sample data set of a reasonably large size, which saves computation time.

## Parameters, statistics, and simulation

There are quantitative evaluations that we commonly use in statistical analysis.
For instance, in a population of flights that departed New York City (NYC) in 2013, the *median* departure delay can tell us something about the central tendency of the data -- are most departure flights delayed, on time, or are they ahead of schedule?
Similarly, in baseball, the ability of a player to produce a hit takes the *average* (or *mean*) hit rate as its measure.
When a baseball player takes a stand as a hitter in a game, the result is one of four-ball, dead-ball, hit, and out.
In the first two, the hitter advances to the first base due to the pitcher either throws four balls (balls outside the strike zone for which the hitter did not swing the bat) or hits the batter with a ball where the ball would have been outside the strike zone.
In the third one, the player hits a ball within a zone spreading in front of him and reaches the first base before being the defensive player touches him.
In this case, the player advances to the first base, or depending on what transpires after hitting of the ball, may advances to a base beyond the first one.
In the last one, the hitter does not advance to the first base.
The hitting rate is the number of hits divided by the number of times the player stood in a batter box.

Median and average (or mean) are *parameters* of the population.
Also, median is an *order static*, meaning that the value emerges when the data undergoes reordering.
Note that there are two medians exist when there are an even number of data.
If there are 10 values, the 5th and the 6th values in the ascending order are the two medians.
The one at the lower position in the order is the *odd median* and the other one, i.e., the one at the higher position in the order, is the *even median*.


In many situations we do not know the parameters of the distribution.
Sampling offers estimates of the parameters.
Since large-scale sampling produces an empirical distribution close to the true population, we anticipate that the parameters we obtain from empirical distributions are close to the true ones when the sample size is large.


### Prerequisites

We will continue to make use of the tidyverse in this section. Our running examples in this section will be flights data from `nycflights13` and National Football League (NFL) data on football players. First, let's load in these datasets.  

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

```{r, message = FALSE, warning = FALSE}
path <- "data/nfl_receiving0.csv"
receivers0 <- read_csv(path)
```

### The variability of statistics

One thing we need to keep in mind is that the empirical distribution can look quite different between runs. Recall the histogram of departure delays in `flights`. 

```{r warning=FALSE}
delay_bins <- seq(-50, 150, 10)
ggplot(flights, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```

Here is an empirical histogram of the delays in a random sample of 1,000 such flights. 

```{r warning=FALSE}
sample_1000 <- slice_sample(flights, n = 1000, replace = TRUE)
ggplot(sample_1000, aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(col="grey", breaks = delay_bins)
```

Since the population of all departed NYC flights in 2013 is available to us in `flights`, we know the value of the parameter "median flight delay."
We consider ourselves very lucky.
We can check the value we obtain through sampling against the value we obtain from the entire population.

```{r}
flights %>%
  pull(dep_delay) %>%
  median(na.rm = TRUE)
```

Note the argument `na.rm` (read as "NaN remove") used in this function call.
An interesting property of the `flights` dataset is that it has missing values, e.g., not all rows have a value in the `dep_delay` column.
We toggle a flag `na.rm` so that R knows to drop these rows before computing the median.  

The function `median` returns the half-way point of a column or vector after reordering the values (the even median when the number of data is even).
For `flights`, the returned value means that the median delay was -2 minutes so about 50% of the flights in the population had early departures of 2 minutes or more.

```{r}
nrow(filter(flights, dep_delay <= -2)) / nrow(flights)
```

Now let's compute the sample median from the empirical distribution in `sample_1000`. 

```{r}
sample_1000 %>%
  pull(dep_delay) %>%
  median(na.rm = TRUE)
```

Let us check what happens if we tried another random sample of 1,000 flights.

```{r}
slice_sample(flights, n = 1000, replace = TRUE) %>%
  pull(dep_delay) %>%
  median(na.rm = TRUE)
```

You can run the code a few times to see that the value that appears as the result is not consistent, but always either -1 or -2.
While the two values are close, you cannot assert that the median is -1 or it is -2.
Is there any way out of this?
Can we say anything about how likely the median is -1 and how likely the median is -2?
We thus turn to simulation.
We obtain the statistic of median many times using sampling and then see a histogram of the numbers we obtain.

### Simulating a Statistic 

Before getting down to the business of estimating the median distribution, let us set up the steps for obtaining a distribution of a statistic.

__Step 1: Select the Statistic to Estimate.__ Two questions are at hand. What statistic do we want to estimate? How many samples do we use in each estimate?

__Step 2: Write the Code for Estimation.__ We need to write the code for sampling and then computing an estimate from the samples. Typically we encapsulate such steps in a function that we can use as a functional in a `purrr` map. 

__Step 3: Generate Estimations and Examine.__ A question at hand is how many times do we repeat the experiment? We may not have an answer to the question.
We will write the code for repeating experiments, collecting the results in a vector whose length is equal to the number of repetitions, and then generating a visualization of the results we have collected in the vector.
The code we write may be a function that takes the number of repetitions as an argument.

### Example: Median flight delay in `flights`

Let us turn to simulating the median flight delay in the `flights` data frame. Recall that we know the value of the parameter the statistic is trying to estimate. It is the value $-2$. 

### Step 1: Select the Statistic to Estimate

We will draw random samples of size 1,000 from the population of flights and simulate the median.  

### Step 2: Write the Code for Estimation

We know how to generate a random sample of 1,000 flights. 

```{r}
sampled <- flights %>%
  slice_sample(n = 1000, replace = TRUE)
```

We also know how to compute the median of this sample. 

```{r}
sampled %>%
  pull(dep_delay) %>%
  median(na.rm = TRUE)
```

Let's wrap this up into a function we can use in our map. 

```{r}
one_sample_median <- function(x) {
  sample_median <- flights %>%
    slice_sample(n = 1000, replace = TRUE) %>%
    pull(dep_delay) %>%
    median(na.rm = TRUE)
  return(sample_median)
}
```

### Step 3: Generate Estimations and Examine

We will issue 5,000 repetitions of our simulation.
Here is the map that makes use of the functional `one_sample_median`.
Note that this simulation takes a bit more time to run: we are repeating an experiment where we draw 1,000 random samples a total of 5,000 times! 

```{r}
num_repetitions <- 5000
medians <- map_dbl(1:num_repetitions, one_sample_median)
```

Here are what some of the sample medians look like. 

```{r}
medians_df <- tibble(medians)
medians_df
```

Of course, it would be much better to visualize these results using a histogram. This histogram displays the *empirical distribution* of the statistic.

```{r}
ggplot(medians_df) + 
  geom_histogram(aes(x = medians, y = ..density..), color = "gray", bins = 3)
```

The sample median is very likely to be about -2, which is exactly the value of the population median.
This is because the empirical distributions of 1,000 flight departure delays are close to the true ones.
We know that we generated before looked a lot like the true distribution of flight delays.
We can guess that in running the experiment, each empirical distribution with 1,000 flights would have liked like the true one.

### Example: Touchdown percentage in the `receivers2019` data frame

We end this section with an example from football. The `receivers0` data frame contains data of receivers in the National Football League in the 2017, 2018, and 2019 seasons. The data comes from [The Football Database](http://www.footballdb.com/index.html). The dataset has 1,511 rows and 13 columns. We note some of the most important variables:

* The name of receiver (Player)
* The year of data (Year)
* The team of receiver (Team)
* The number of games the receiver became a target (Gms)
* The number of times the receiver caught the ball (Rec)
* The total receiving yards (Yds)
* The average yards per receiving (Avg)
* The average yards per game (YPG)
* The longest yards received where the attachment of "t" indicates "touchdown" (Lg)
* The number of touchdowns (TD)
* The number of first downs (FD)
* The number of times the receiver became the target (Yac)
* The total yards after catch (YAC)

The data requires some tidying: 

* the same name may appear multiple times because the data is over three seasons
* the longest yard information may have a "t" attached.

We can solve the former by creating a new column `player_year` that combines `Player` and `Year`. The latter can be solved by forming two new columns in which the two values in the longest yard are split into the yardage and the value indicating whether the longest yard is also a touchdown (`TRUE` for "yes" and `FALSE` for "no").

We can accomplish the work using `mutate` and some `stringr` functions.  

```{r}
receivers <- receivers0 %>%
  mutate(player_year = str_c(Player, " ", Year),
         is_lg_td = str_detect(Lg, "t$"),
         Lg = str_remove(Lg, "t"), 
         ) 
receivers
```

For this example, we will study the percentage of players whose longest yardage ended in a touchdown for the 2019 season. Since we have access to the population, we know the value of the parameter. Let's record it. 

To begin, let's filter the data frame to include only the data from 2019. We will call this data frame `receivers2019`. 

```{r}
receivers2019 <- filter(receivers, Year == 2019)
```

The percentage of longest yards is the fraction of `TRUE`'s in the `is_lg_td` column. Recall that Boolean variables are really just `1`'s and `0`'s. Thus, we can sum the number of `TRUE`'s and divide by the total number of players. This is equivalent to computing the mean for the `is_lg_td` column.  

```{r}
percent_td <- receivers2019 %>%
  pull(is_lg_td) %>%
  mean()
percent_td
```

So about 18.5% of the longest yardages of players ended in a touchdown. How good would our statistic be? 

### Step 1: Select the Statistic to Estimate

We will draw random samples of size 1,000 from the population of players and simulate the percentage of player yardages that end in a touchdown.   

### Step 2: Write the Code for Estimation

We already know how to generate a random sample of 1,000 players.  

```{r}
sampled <- receivers2019 %>%
  slice_sample(n = 1000, replace = TRUE)
```

We also know how to compute the sample percentage of this sample. 

```{r}
sampled %>%
  pull(is_lg_td) %>%
  mean()
```

As before, let's wrap this up into a function we can use in our map. 

```{r}
one_sample_percentage <- function(x) {
  sample_percent <- receivers2019 %>%
    slice_sample(n = 1000, replace = TRUE) %>%
    pull(is_lg_td) %>%
    mean()
  return(sample_percent)
}
```

### Step 3: Generate Estimations and Examine

We will issue 10,000 repetitions of our simulation. Here is the map that makes use of the functional `one_sample_percentage`. 

```{r}
num_repetitions <- 10000
percentages <- map_dbl(1:num_repetitions, one_sample_percentage)
```

We can now visualize the empirical distribution of the statistic using a histogram. 

```{r}
ggplot(tibble(percentages)) + 
  geom_histogram(aes(x = percentages, y = ..density..), 
                 fill = "darkcyan", color = "gray", bins = 20) + 
  geom_point(aes(x = percent_td, y = 0), color = "salmon", size = 3)
```

Observe how the sample percentages cluster around the value 0.185, which is the value of the population percentage given by `percent_td` (see the light orange dot). 

Once again, we see that the statistic provides a good estimate of a parameter. 

## Sampling Plans

The story goes that we can derive meaningful conclusions about a population using empirical distributions. Such distributions are formed by the application of some sampling strategy. The  one we have used so far has been drawing random samples with (or without) replacement. We refer to such a scheme as *simple random sampling*. However, simple random sampling is not the only way to generate an empirical distribution. We briefly discussed another method at the start of this chapter: *systematic sampling*. In this section, we examine systematic sampling in greater detail and its suitability for statistical analysis.   


### Prerequisites

We will continue to make use of the tidyverse in this section. Our example in this section will be data about New York City flight delays in 2013 from the package `nycflights13`, which we have worked with before. Let's load both of these packages. 

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

Recall that the median flight delay in the true distribution of flights is $-2$. 

```{r}
median_delay <- flights %>%
  pull(dep_delay) %>%
  median(na.rm = TRUE)
median_delay
```


### Systematic selection

In systematic selection, we pick some random pivot (say, the 4th row), and then select every $i$-th row after that. This method of sampling is quite popular because of its sheer simplicity. For example, if we wanted to sample a student population at a university, we could select all students based on the digits in their university-issued ID number, e.g., selecting all students who have an odd number in the second-to-last position of their ID. 

<!-- HIDDEN (for generating mystery flights)
```{r}
early_delays <- sample(filter(flights, dep_delay < 0)$dep_delay)

my_func <- function(ID, dep_delay) {
  return(if_else(ID %% 2 == 0, 
                 early_delays[ID],
                 dep_delay))
}

flights_new <- flights %>% 
  mutate(ID = row_number(),
         dep_delay = pmap(list(ID, dep_delay), my_func)
         ) %>%
  unnest()
flights_new
write_csv(flights_new, "data/mystery_flights.csv")
```
--> 

We can explore this sampling strategy using the `flights` tibble. We will select a random row in the tibble as the pivot, and then select every 100th row after that (recall that the dataset is quite large, with ~330K rows!). 

```{r}
start <- sample(1:nrow(flights), size = 1)  # pick a random row, any one! 
start
```

Does the generated distribution mirror what we know about the true distribution? 

```{r}
selected_rows <- seq(start, nrow(flights), 100)  # select every 100th row after start

slice(flights, selected_rows) %>% 
  ggplot(aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(fill = "darkcyan", color = "gray", breaks = seq(-50, 150, 10)) +
  ggtitle(str_c("starting row = ", start))
```

Looks good! The empirical distribution still looks a whole lot like the true distribution of flight delays. However, the number of samples that appear in the tail varies greatly across runs -- why might that be? Run the cell a few times and observe the effect of the selected `start` variable on the resulting distribution. 

From our initial exploration, it looks like systematic sampling is a good bet. Any statistics we compute using this sampling strategy is likely to provide a good estimate of the population parameter in question. 

Now suppose that we reorganized the `flights` data a bit into a tibble called `mystery_flights`.

```{r message = FALSE, error = FALSE}
mystery_flights <- read_csv("data/mystery_flights.csv") %>% select(ID, everything())
mystery_flights 
```

Notice anything problematic? Don't worry if you can't spot it; the problem doesn't jump at first glance. Let's keep with our sampling scheme, but with one key modification. We will assert that the randomly generated `start` is 2. 

```{r}
start <- 2
```

We can now perform the same systematic selection as before. 

```{r message = FALSE, warning = FALSE}
selected_rows <- seq(start, nrow(mystery_flights), 100)  # select every 100th row after start

slice(mystery_flights, selected_rows) %>% 
  ggplot(aes(x = dep_delay, y = ..density..)) + 
  geom_histogram(fill = "darkcyan", color = "gray", breaks = seq(-50, 150, 10)) +
  ggtitle(str_c("starting row = ", start))
```

All of the sampled flights have early departures! What happened? 

Let's break down the steps we took. The first row selected is at index 2, as told by `start`, and each row after increases by increments of 100. If we write out some of these indices, we would select rows: 

```{r}
tibble(row_index = seq(2, nrow(mystery_flights), 100))
```

There are several patterns that can be gleaned from this listing, but we will direct your attention to one in particular: these row numbers are all even! If we pick out some of these rows from `mystery_flights`, we find something revealing. 

```{r}
slice(mystery_flights, c(2, 102, 202, 302, 402))
```

Compare this with the rows just before these. 

```{r}
slice(mystery_flights, c(1, 101, 201, 301, 401))
```

It turns out that the flights with even row numbers all have early departures. By fixing `start` to be an even value, our systematic sampling scheme was fooled into always choosing flights that are ahead of schedule. Under such circumstances, we conclude that our random sample is necessarily *biased*. 

### Beware: the presence of patterns 

The `mystery_flights` tibble is a contrived example that required careful reorganization of the rows to create a setup where *every* flight with an even row index among the approximately 330K flights present in the dataset had an early departure. While it is quite unlikely that a real-world dataset would contain such an anomaly, the example points to valuable lessons that can occur in practice. 

Real-world datasets are rife with patterns. Manufacturing errors due to a particular malfunctioning machine that assemble every $n$-th product; software engineering teams where every tenth member is designated the product manager; university-issued ID's where ID numbers ending in 0 are reserved for faculty members. Any systematic sampling scheme is much more prone to selecting samples that follow (unexpected) patterns than a simple random scheme would be. Not unlike like the story with `mystery_flights`, a biased sample can result and lead to misleading (and likely erroneous) findings. Even more, some may be willing to exploit such patterns for the sole possibility of increasing the significance of their results -- we would hardly call them data scientists!  

Thus, regardless whether simple random or systematic sampling is used, sampling strategies demand prudence on the part of the data scientist.  






<!--
What is the average of the total yardage value among the 1511 entries?
Computing the average in question is simple:
obtain the column for the attribute, sum the entries, and divide it by the number of rows.

```{r}
col <- receivers$Yds
Yds_avg <- sum(col)/length(col)
```

So the average is around 252.7.

Let us look at the histogram of the yardage values.
We locate the position of the average, which we have calculated into `Yds_avg` in red.


```{r dpi=80,  fig.align="center", message = FALSE}
bins <- seq(0,1750,20)
ggplot(receivers, aes(x = Yds)) +
  geom_histogram(aes(y = ..density..), color = "grey", breaks = bins) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 2)

```

It is hard to tell, just by looking at the histogram, that 252.72 is the average.

Let us see how sampling qill takes us close to the ideal value.
Let us we will sample 10% of the rows; that is, 151 rows.

```{r}
colSub <- sample(col,size=151,replace=FALSE)
sum(colSub)/length(colSub)
```

How close do we get close to the true average if we are to repeat the 10% sampling many times?

Let's do with 1000 repetitions.

```{r}
num_reps = 1000
outcomes <- vector("double",num_reps)
for (i in 1:num_reps) {
  colSub <- sample(col,size=151,replace=FALSE)
  outcomes[i] <- sum(colSub)/length(colSub)
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```

Now this is 10000 repetitions.

```{r}
num_reps = 10000
outcomes <- vector("double",num_reps)
for (i in 1:num_reps) {
  colSub <- sample(col,size=151,replace=FALSE)
  outcomes[i] <- sum(colSub)/length(colSub)
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```

How about we increase the percentage to 20%?

```{r}
num_reps = 10000
outcomes <- vector("double",num_reps)
for (i in 1:num_reps) {
  colSub <- sample(col,size=302,replace=FALSE)
  outcomes[i] <- sum(colSub)/length(colSub)
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```


```{r}
sum(outcomes)/length(outcomes)
```

How does the histogram look like if we make systematic choices?
There are many possibilities for mechanically selecting 151 positions.
Suppose we will select "every Xth position starting from W".
The following code does the job.
It uses four parameters in the calculation: `size` for the size of the length of the list in the original data, `need` for the length of the index list, `start` for the starting position, and `gap` for the distance between the two position.
For example, to execute every 3rd position starting from 2, the value of `start` is 3 and the value of `gap` is 2.
The program generates a vector of length `need`.
The elements of the vector serve as the place holders for the index values.
Then the program generates the sequence `start + gap * (i - 1)` for `i` ranging from 1 to `need` and store the value in the position `i` in the index value vector.
The value of the index the program generates by adding `gap` may exceed the value of `size` at some point (actually, it may occur many times).
Each time the value exceeds, the program makes an adjustment by subtracting the value of `size`.
For example, if `start` is 1 and `gap` is 100, then after 16 additions, the value becomes 1601.
If the value of `size` is 1511, we subtract 1511 from 1601 to make it 90.

Because of the adjustmnet process, the value that the program generates go up and then down.
Since the elements of the vector simply state the places from which the samples come, we can safely reorder the elements.
So, we execute `sort` on the positions and update the vector with the result of sorting.

```{r}
size <- 1511
need <- 151
start <- 1
gap <- 20
pos <- vector("integer",need)
x <- start
for (i in 1:need) {
  pos[i] <- x
  x <- x + gap
  if (x > size) x <- x - size
}
pos <- sort(pos)
pos
```

The list the program shows at the end is the result from generating index values with the starting point 1 and the gap of 20.
We can write a function for the index generation into a function that takes the four valus as arguments.

```{r}
mechanical_selection <- function(size, need, start, gap) {
  pos <- vector("integer",need)
  x <- start
  for (i in 1:need) {
    pos[i] <- x
    x <- x + gap
    if (x > size) x <- x - size
  }
  pos <- sort(pos)
  return(pos)
}
mechanical_selection(1000,10,1,27)
```

Using the method `mechanical_selection`, let us see how the average yardage turns up in the same number of repetitions with 151 samples.
In the following program, we create gap sequences of 100 elements each and start position sequences of 100 elements.
The elements of the former are 14 * i, where i ranges from 1 to 100.
The elements of the latter are 13 * i, where i ranges from 1 to 100.
Using all possible combinations of the elements produce 100 * 100 sequences with 1511 as `size` and 151 as `need`.
Using two iterations, where the second occurs for each choice of the first element, we generate 10000 cases.

```{r}
gaps <- 1:100
gaps <- gaps * 14
starts <- 1:100
starts <- starts * 13
num_reps = 10000
size <- 1511
need <- 151
outcomes <- vector("double",num_reps)
for (i in 1:100) {
  for (j in 1:100) {
    p <- (i-1) * 100 + j
    pos <- mechanical_selection(size, need, starts[i], gaps[j])
    colSub <- col[pos]
    outcomes[p] <- sum(colSub)/need
  }
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```


```{r}
sum(outcomes)/length(outcomes)
```

As you can see, the mechanical generations generate a histogram that is much narrower than the one we derive from random position selections.

We may think of ourselves as lucky. If we use 1000 start points and 10 different gaps, the histogram looks much flatter.

```{r}
gaps <- 1:10
gaps <- gaps
starts <- 1:1000
starts <- starts
num_reps = 10000
size <- 1511
need <- 151
outcomes <- vector("double",num_reps)
for (i in 1:1000) {
  for (j in 1:10) {
    p <- (i - 1) * 10 + j
    pos <- mechanical_selection(size, need, starts[i], gaps[j])
    colSub <- col[pos]
    outcomes[p] <- sum(colSub)/need
  }
}
outcome_df <- tibble(outcomes)
bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=4) +
  geom_point(aes(x = Yds_avg, y = 0), color = "red", size = 3)
```

```{r}
sum(outcomes)/length(outcomes)
```
So, the use of mechanical index selections requires a careful treatment.
Depending on how mechanically we select the index values, we may obtain different results.

--> 
<!--
## Probabilities of Things Happening

A probability is the likelihood of a thing happening.
In our favorite topic of coin tossing, we have said previously that the the coin shows "Heads" as likely as it does "Tails".
We have also said that the chances are 50-50 that the coin shows "Heads".
The numbers 50 represent the probability in percentage that the coin shows "Heads" (or "Tails").
A fair coin has no memory.
When one turns "Heads" in a throws, it informs us nothing about what will happen in the next throw.
Suppose a referee of football has thrown "Heads" three times in a row, what do you think will happen in the next throw.
Many people think that the next throw must be "Tails" because "Tails" has not turned up in any of the past three throws.
However, noting that the coin is fair, and has no memory, the next throw has equal probability of 50% for both "Heads" and "Tails".

Now, suppose you, as an officiate member of NFL, have received a new coin (because a coin probably gets wear and tear, and of course, a new shiny coin looks more spiffy than an old, dull coin).
How can you be sure that the coin is fair?
Say you have thrown it three times and seen "Heads" three times in a row.
Do you believe that it is a fair coin?

In the program below, we simulate the process of throwing a fair coin a number of times.
The value of `num_throws` represents the number of throws.
To simulate the action of throwing a fair coin, we use `sample` and select from a two-element list [1,2] that we represent using a vector `counts`.
We assume that 1 represents "Heads" and 2 "Tails", but these assignments are exchangeable.
For each throw, we get 1 or 2 as a random sample.
We then examine the results appearing in the list `sample` has generated and compute the number of times 1 appears and the number of times 2 appears, and we record the numbers in a two-element list `counts`.
By dividing the quantities in `counts` by the number of throws, we get the probabilities of "Heads" and "Tails" of the fair coin.


```{r}
counts <- vector("double",2)
choices <- 1:2
num_throws <- 1000
throws <- sample(choices,size=num_throws,replace=TRUE)
for (i in 1:num_reps) {
  counts[throws[i]] <- counts[throws[i]] + 1
}
counts[1] <- counts[1] / num_throws
counts[2] <- counts[2] / num_throws
counts
```

Let us turn the program into a function that takes the number of throws as its argument and returns the two-element probabilities.

```{r}
throw_est <- function(x) {
  counts <- vector("double",2)
  choices <- 1:2
  throws <- sample(choices,size=x,replace=TRUE)
  for (i in 1:x) {
    counts[throws[i]] <- counts[throws[i]] + 1
  }
  counts[1] <- counts[1] / x
  counts[2] <- counts[2] / x
  return(counts)
}
```

Here we use 50 throws.

```{r}
coin <- c("Heads", "Tails")
counts <- throw_est(50)
outcome_df <- data.frame(throw=coin,prob=counts)
# bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = coin, y=counts), color = "grey") +
  geom_bar(stat="identity",fill="steelblue", width=0.5) +
  geom_text(aes(label=counts), vjust=-0.3, size=3.5)
```

Now with 100 throws.

```{r}
coin <- c("Heads", "Tails")
counts <- throw_est(100)
outcome_df <- data.frame(throw=coin,prob=counts)
# bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = coin, y=counts), color = "grey") +
  geom_bar(stat="identity",fill="steelblue", width=0.5) +
  geom_text(aes(label=counts), vjust=-0.3, size=3.5)
```

Here we use 1000 throws.

```{r}
coin <- c("Heads", "Tails")
counts <- throw_est(1000)
outcome_df <- data.frame(throw=coin,prob=counts)
# bins <- seq(0,1750,20)
ggplot(outcome_df, aes(x = coin, y=counts), color = "grey") +
  geom_bar(stat="identity",fill="steelblue", width=0.5) +
  geom_text(aes(label=counts), vjust=-0.3, size=3.5)
```

As we did for the average yardage, let us observe when we execute the simulation of 1000 throws multiple times how the probability of 1 (="Heads") turns up.

```{r}
outcomes <- vector("double",1000)
for (i in 1:num_reps) {
  result <- throw_est(1000)
  outcomes[i] <- result[1]
}
# outcomes
outcome_df <- tibble(outcomes)
bins <- seq(0.44,0.56,0.010)
ggplot(outcome_df, aes(x = outcomes), color = "grey") +
  geom_histogram(binwidth=0.001)
```

Even with 1000 repetitions, the shape is not smooth.

_The Law of Average_

The law of average is the term that refers to the understanding that for an event occurring with no relation to when in the past the event occurred, the ratio that we compute as
\[
\frac{\mathrm{the~number~of~times~the~event~occurred}}{\mathrm{the~number~of~times~the~event~could~have~occurred}}
\]
reaches the inherent ratio in which the event occurs as we make more observations.
To put differently, if there is an experiment that
* we can repeat as many time as we want,
* the experiment has a number of possible outcomes, and
* the outcomes of the previous events do not affect the outcome of any future event
then, we can assume that
* for each outcome, there is a non-negative constant quantity, which we call the *probability* of the outcome,
* the sume of the probability of all the outcomes of the experiment is equal to 1, and
* for each outcome $e$, if we are toi execute the experiment some $N$ times, count how many times $e$ occurs, and compute the ratio $e/N$, then the larger $N$ we use, the ratio $e/N$ gets closer to the probability of $e$ appears as the outcome. 

## Order Statistics

Recall that in our prior exploration of the receiver data, we saw that the average total yards is a little over 211 yards and that the distribution shows a drastic decline.
A static that is popular along with average is *median*, which is the value at the middle position in the data after reordering of the values.
Let's load the data and examine the median of the yardage value.

But first, let's load the library.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

As before, we load the edited data into `receivers_data`.

```{r}
path <- "data/nfl_receivers.csv"
receivers_data <- read_csv(path)
```

We can ask for the median of the yardage values, which appears in the attribute `Yds`.

```{r}
median(receivers_data$Yds)
```

So there is a big gap between the median and the average.
A synonym for average is *mean*.
The statistical literature prefers "mean" over "average".
Our library `tidyverse` has the same reference.
We can get the "mean" of the `Yds` values with `mean` in place of `median`.
Let us look at the ratio of the mean to the average.


```{r}
mean(receivers_data$Yds)/median(receivers_data$Yds)
```

The ratio is close to 2; that is, the mean is almost as twice as much of the median.
What does this mean?
Noting that the gradually decline histogram of the yardage values, we guess that there are so many large values.

As we mentioned earlier, a receiving yardage value can be negative, occurring when the receiving position is behind the position at the play started.
So, the total yardage value may have a negative entry, especially when someone who is not regularly in a receiving position (such as a quarterback) received the ball in one game.

Using `filter` we can find all the data with negative yardage.


```{r}
filter(receivers_data, Yds < 0)
```
There are 18 of them. You may notice that some of them are indeed quarterbacks, e.,g., Andy Dalton, Derek Carr, and Kirk Cousins.

What is the minimum of the negative values?

```{r}
min(receivers_data$Yds)
```

Negative 11 it is.

We have found that the median is 131 and the minimum is $-11$.
The difference between the two is 142.
Of the 1511 data values we have, the median sits right in the middle.
There are $(1511-1)/2 = 755$ values below the median and the same number of values above the median.
The value that is 142 away from the median on the opposite side of $-11$ is 273.
To make the mean equal to the average, the data values above the median only have to be around 273.
The deviation of the mean from the median indicates that the values above the median can be much higher than 273.
Let up look at the top 100 (about 6.6%) of the yardage values.
The `sort` function can be in the decreasing order.
You have only to append `decreasing=TRUE` in the argument.

```{r}
y <- sort(receivers_data$Yds, decreasing=TRUE)
y[1:100]
```

The highest is a breathtaking 1725 and the 100th one is 828.
Here is a list of top 100 (i.e., the yardage value greater than or equal to 828) in the descending order of the yardage value.
Note that Mike Thomas and Julio Jones appear in the top 10.

```{r}
filter(receivers_data, Yds >= 828) %>% arrange(desc(Yds))
```
There are cases, by the way, where two median exists.
Such an event occurs exactly when the number of values is an even number.
If there are 10 values, the 5th and the 6th values in the ascending order are the two medians.
The one at the lower position in the order is the *odd median* and the other one, i.e., the one at the higher position in the order, is the *even median*.
Some additional terminology is worth defining.
The *population* is a group of objects from which we draw samples.
In the case of the receivers data, the American football players who were targets in any play in the 2017, 2018, and 2019 seasons form the population.
The *population size* is the number of objects in the population.
1511 is the population size.
The football players that we drew from the data set are *samples*.
Each football player we included (implictly or explicitly) in our analysis is among the samples.
The *maximum*, the *minimum*, and the *median* are *order statistics*, referring to their values from a list after reordering in ascending order.
The *mean* (or *average*) is a statistic, but is not an order statistic. -->



