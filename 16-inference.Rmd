# Regression

<!--
# Inference for Regression 

Thus far, our analysis of the relation between variables has been purely descriptive. We know how to find the best straight line to draw through a scatter plot. The line is the best in the sense that it has the smallest mean squared error of estimation among all straight lines.

But what if our data were only a sample from a larger population? If in the sample we found a linear relation between the two variables, would the same be true for the population? Would it be exactly the same linear relation? Could we predict the response of a new individual who is not in our sample?

Such questions of inference and prediction arise if we believe that a scatter plot reflects the underlying relation between the two variables being plotted but does not specify the relation completely. For example, a scatter plot of birth weight versus gestational days shows us the precise relation between the two variables in our sample; but we might wonder whether that relation holds true, or almost true, for all babies in the population from which the sample was drawn, or indeed among babies in general.

As always, inferential thinking begins with a careful examination of the assumptions about the data. Sets of assumptions are known as *models*. Sets of assumptions about randomness in roughly linear scatter plots are called *regression models*.

## A Regression Model

In brief, such models say that the underlying relation between the two variables is perfectly linear; this straight line is the *signal* that we would like to identify. However, we are not able to see the line clearly. What we see are points that are scattered around the line. In each of the points, the signal has been contaminated by *random noise*. Our inferential goal, therefore, is to separate the signal from the noise.

In greater detail, the regression model specifies that the points in the scatter plot are generated at random as follows.

- The relation between $x$ and $y$ is perfectly linear. We cannot see this "true line" but it exists.
- The scatter plot is created by taking points on the line and pushing them off the line vertically, either above or below, as follows:
    - For each $x$, find the corresponding point on the true line (that's the signal), and then generate the noise or error.
    - The errors are drawn at random with replacement from a population of errors that has a normal distribution with mean 0.
    - Create a point whose horizontal coordinate is $x$ and whose vertical coordinate is "the height of the true line at $x$, plus the error".
- Finally, erase the true line from the scatter, and display just the points created.

Based on this scatter plot, how should we estimate the true line? The best line that we can put through a scatter plot is the regression line. So the regression line is a natural estimate of the true line. 

The simulation below shows how close the regression line is to the true line. The first panel shows how the scatter plot is generated from the true line. The second shows the scatter plot that we see. The third shows the regression line through the plot. The fourth shows both the regression line and the true line.

To run the simulation, call the function `draw_and_compare` with three arguments: the slope of the true line, the intercept of the true line, and the sample size.

Run the simulation a few times, with different values for the slope and intercept of the true line, and varying sample sizes. Because all the points are generated according to the model, you will see that the regression line is a good estimate of the true line if the sample size is moderately large.

```{r, message = FALSE, echo = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

```{r message = FALSE, echo = FALSE}
draw_and_compare <- function(true_slope, true_int, sample_size) {
  x <- rnorm(sample_size, 50, 5)
  xlims <- c(min(x), max(x))
  eps <- rnorm(sample_size, 0, 6)
  y <- (true_slope*x + true_int) + eps
  tyche <- tibble(x = x, y = y)
  tyche2 <- tibble(x = xlims, y = true_slope * xlims + true_int)

  g <- ggplot() +
    geom_point(data = tyche, aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    geom_line(data = tyche2, aes(x = x, y = y), color = "salmon", size = 1) + 
    ggtitle("True Line, and Points Created")
  print(g)
  g <- ggplot() +
    geom_point(data = tyche, aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    ggtitle("What We Get to See")
  print(g)
  
  g <- ggplot(tyche) + 
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    geom_smooth(aes(x = x, y = y), method = "lm", se = FALSE) + 
    ggtitle("Regression Line: Estimate of True Line")
  print(g)
  
  g <- ggplot(tyche) + 
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    geom_line(data = tyche2, aes(x = x, y = y), color = "salmon", size = 1) + 
    geom_smooth(aes(x = x, y = y), method = "lm", se = FALSE) + 
    ggtitle("Regression Line and True Line")
  print(g)
  
}
```

```{r dpi=80, fig.align="center", message = FALSE}
draw_and_compare(4, -5, 20)
```

In reality, of course, we will never see the true line. What the simulation shows is that if the regression model looks plausible, and if we have a large sample, then the regression line is a good approximation to the true line.

## Inference for the True Slope

Our simulations show that if the regression model holds and the sample size is large, then the regression line is likely to be close to the true line. This allows us to estimate the slope of the true line.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Let us also load in the `baby` dataset we have been working with in earlier sections. 

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

The calculations in this section assume all the relevant functions we have already defined: `slope`, `intercept`, and `fit`.

```{r}
slope <- function(label_x, label_y) {
  r <- cor(label_x,  label_y)
  return (r * sd(label_y) / sd(label_x))
}

intercept <- function(label_x, label_y) {
  return(mean(label_y) - 
           slope(label_x, label_y) * mean(label_x))
}

fit <- function(x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(x, y)
  b <- intercept(x, y)
  return(a * x + b)
}
```

### Estimating the slope of the true line

We will use our familiar sample of mothers and their newborn babies to develop a method of estimating the slope of the true line. First, let's see if we believe that the regression model is an appropriate set of assumptions for describing the relation between birth weight and the number of gestational days.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon")
```

```{r}
cor(pull(baby, `Gestational Days`), pull(baby, `Birth Weight`))
```

By and large, the scatter looks fairly evenly distributed around the line, though there are some points that are scattered on the outskirts of the main cloud. The correlation is 0.4 and the regression line has a positive slope.

Does this reflect the fact that the true line has a positive slope? To answer this question, let us see if we can estimate the true slope. We certainly have one estimate of it: the slope of our regression line. That's about 0.47 ounces per day.

```{r}
slope(pull(baby, `Gestational Days`), pull(baby, `Birth Weight`))
```

But had the scatter plot come out differently, the regression line would have been different and might have had a different slope. How do we figure out how different the slope might have been?

We need another sample of points, so that we can draw the regression line through the new scatter plot and find its slope. But from where will get another sample?

You guessed it – we will *bootstrap our original sample*. That will give us a bootstrapped scatter plot, through which we can draw a regression line.

### Bootstrapping the Scatter Plot 

We can simulate new samples by random sampling with replacement from the original sample, as many times as the original sample size. Each of these new samples will give us a scatter plot. We will call that a bootstrapped scatter plot, and for short, we will call the entire process bootstrapping the scatter plot.

Here is the original scatter diagram from the sample, and four replications of the bootstrap resampling procedure. Notice how the resampled scatter plots are in general a little more sparse than the original. That is because some of the original points do not get selected in the samples.

```{r dpi=80, fig.align="center", message = FALSE, echo = FALSE}
g <- ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  ggtitle("Original sample")
print(g)

for(i in seq(4)) {
  rep <- slice_sample(baby, n = nrow(baby), replace = TRUE)
  g <- ggplot(rep, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
    geom_point(color = "darkcyan", alpha = 0.5) + 
    ggtitle(paste("Bootstrap sample ", i))
  print(g)
}
```

### Estimating the True Slope

We can bootstrap the scatter plot a large number of times, and draw a regression line through each bootstrapped plot. Each of those lines has a slope. We can simply collect all the slopes and draw their empirical histogram. 

```{r}
one_bootstrap <- function(x, df, label_x, label_y) {
  bootstrap_sample <- slice_sample({{ df }}, n = nrow({{ df }}), replace = TRUE)
  bootstrap_slope <- slope(pull(bootstrap_sample, {{ label_x }}), 
                           pull(bootstrap_sample, {{ label_y }}))
  return(bootstrap_slope)
}
```

```{r}
bstrap_slopes <- map_dbl(.x = 1:5000, 
                         .f = one_bootstrap,
                         df = baby, 
                         label_x = `Gestational Days`, 
                         label_y = `Birth Weight`)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(bstrap_slopes)) + 
  geom_histogram(aes(x = bstrap_slopes, y = ..density..), 
                 bins = 20, color = "gray", fill = "darkcyan")
```

We can then construct an approximate 95% confidence interval for the slope of the true line, using the bootstrap percentile method. The confidence interval extends from the 2.5th percentile to the 97.5th percentile of the 5000 bootstrapped slopes.

```{r}
left <- quantile(bstrap_slopes, c(0.025), type = 1)
right <- quantile(bstrap_slopes, c(0.975), type = 1)
c(left, right)
```

An approximate 95% confidence interval for the true slope extends from about 0.38 ounces per day to about 0.56 ounces per day.

### A Function to Bootstrap the Slope

Let us collect all the steps of our method of estimating the slope and define a function `bootstrap_slope` that carries them out. Its arguments are the name of the table and the labels of the predictor and response variables, and the desired number of bootstrap replications. In each replication, the function bootstraps the original scatter plot and calculates the slope of the resulting regression line. It then draws the histogram of all the generated slopes and prints the interval consisting of the "middle 95%" of the slopes.

```{r}
bootstrap_slope <- function(df, x, y, repetitions) {
  
  # For each repetition:
  # Bootstrap the scatter, get the slope of the regression line,
  # collect the vector of generated slopes
  bstrap_slopes <- map_dbl(.x = 1:repetitions, 
                         .f = one_bootstrap,
                         df = {{ df }}, 
                         label_x = {{ x }}, 
                         label_y = {{ y }})
  
  # Find the endpoints of the 95% confidence interval for the true slope
  left <- quantile(bstrap_slopes, c(0.025), type = 1)
  right <- quantile(bstrap_slopes, c(0.975), type = 1)
  
  # Slope of the regression line from the original sample
  observed_slope <- slope(pull({{ df }}, {{ x }}), pull({{ df }}, {{ y }}))
  
  # Visualize results
  g <- ggplot(tibble(bstrap_slopes)) +
    geom_histogram(aes(x = bstrap_slopes, y = ..density..), 
                   bins = 15, color = "gray", fill = "darkcyan") + 
    geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
                 size = 2, color = "salmon") + 
    geom_point(aes(x = observed_slope, y = 0), size = 3, color = "red")
  print(g)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
bootstrap_slope(baby, `Gestational Days`, `Birth Weight`, 5000)
```

Now that we have a function that automates our process of estimating the slope of the true line in a regression model, we can use it on other variables as well.

For example, let's examine the relation between birth weight and the mother's height. Do taller women tend to have heavier babies?

The regression model seems reasonable, based on the scatter plot, but the correlation is not high. It's just about 0.2.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Maternal Height`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon")
```

```{r}
cor(pull(baby, `Maternal Height`), pull(baby, `Birth Weight`))
```

As before, we can use `bootstrap_slope` to estimate the slope of the true line in the regression model.

```{r dpi=80, fig.align="center", message = FALSE}
bootstrap_slope(baby, `Maternal Height`, `Birth Weight`, 5000)
```

A 95% confidence interval for the true slope extends from about 1 ounce per inch to about 1.9 ounces per inch.

### Could the True Slope Be 0?

Suppose we believe that our data follow the regression model, and we fit the regression line to estimate the true line. If the regression line isn't perfectly flat, as is almost invariably the case, we will be observing some linear association in the scatter plot.

But what if that observation is spurious? In other words, what if the true line was flat – that is, there was no linear relation between the two variables – and the association that we observed was just due to randomness in generating the points that form our sample?

Here is a simulation that illustrates why this question arises. We will once again call the function `draw_and_compare`, this time requiring the true line to have slope 0. Our goal is to see whether our regression line shows a slope that is not 0.

Remember that the arguments to the function `draw_and_compare` are the slope and the intercept of the true line, and the number of points to be generated.

```{r dpi=80, fig.align="center", message = FALSE}
draw_and_compare(0, 10, 25)
```

Run the simulation a few times, keeping the slope of the true line 0 each time. You will notice that while the slope of the true line is 0, the slope of the regression line is typically not 0. The regression line sometimes slopes upwards, and sometimes downwards, each time giving us a false impression that the two variables are correlated.

To decide whether or not the slope that we are seeing is real, we would like to test the following hypotheses:

_Null Hypothesis._ The slope of the true line is 0. 

_Alternative Hypothesis._ The slope of the true line is not 0. 

We are well positioned to do this. Since we can construct a 95% confidence interval for the true slope, all we have to do is see whether the interval contains 0. 

If it doesn't, then we can reject the null hypothesis (with the 5% cutoff for the P-value). 

If the confidence interval for the true slope does contain 0, then we don't have enough evidence to reject the null hypothesis. Perhaps the slope that we are seeing is spurious.

Let's use this method in an example. Suppose we try to estimate the birth weight of the baby based on the mother's age. Based on the sample, the slope of the regression line for estimating birth weight based on maternal age is positive, about 0.08 ounces per year. 

```{r}
slope(pull(baby, `Maternal Age`), pull(baby, `Birth Weight`))
```

Though the slope is positive, it's pretty small. The regression line is so close to flat that it raises the question of whether the true line is flat.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Maternal Age`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon")
```

We can use `bootstrap_slope` to estimate the slope of the true line. The calculation shows that an approximate 95% bootstrap confidence interval for the true slope has a negative left end point and a positive right end point – in other words, the interval contains 0. 

```{r}
bootstrap_slope(baby, `Maternal Age`, `Birth Weight`, 5000)
```

Because the interval contains 0, we cannot reject the null hypothesis that the slope of the true linear relation between maternal age and baby's birth weight is 0. Based on this analysis, it would be unwise to predict birth weight based on the regression model with maternal age as the predictor.

## Prediction Intervals

One of the primary uses of regression is to make predictions for a new individual who was not part of our original sample but is similar to the sampled individuals. In the language of the model, we want to estimate $y$ for a new value of $x$.

### Prerequisites

We will make use of the tidyverse in this section, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Let us also load in the `baby` dataset we have been working with in earlier sections. 

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

The calculations in this section assume all the relevant functions we have already defined: `slope`, `intercept`, and `fit`.

```{r}
slope <- function(label_x, label_y) {
  r <- cor(label_x,  label_y)
  return (r * sd(label_y) / sd(label_x))
}

intercept <- function(label_x, label_y) {
  return(mean(label_y) - 
           slope(label_x, label_y) * mean(label_x))
}

fit <- function(x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(x, y)
  b <- intercept(x, y)
  return(a * x + b)
}
```

### Prediction interval for a baby's birth weight 

Our estimate is the height of the true line at $x$. Of course, we don't know the true line. What we have as a substitute is the regression line through our sample of points.

The **fitted value** at a given value of $x$ is the regression estimate of $y$ based on that value of $x$. In other words, the fitted value at a given value of $x$ is the height of the regression line at that $x$.

Suppose we try to predict a baby's birth weight based on the number of gestational days. As we saw in the previous section, the data fit the regression model fairly well and a 95% confidence interval for the slope of the true line doesn't contain 0. So it seems reasonable to carry out our prediction.

The figure below shows where the prediction lies on the regression line. The blue line is at $x = 300$.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
df <- data.frame(x1 = 300, x2 = 300, y1 = 42, y2 = 129.2129241703143)
ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon") + 
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), size = 1,color = "blue", data = df) +
  geom_point(aes(x=300, y=129.2129241703143), color="blue") + 
  coord_cartesian(ylim=c(50,170))
```

The height of the point where the red line hits the regression line is the fitted value at 300 gestational days. 

The function `fitted_value` computes this height. Like the functions `cor`, `slope`, and `intercept`, its arguments include the name of the table and the labels of the $x$ and $y$ columns. But it also requires a fourth argument, which is the value of $x$ at which the estimate will be made.

```{r}
fitted_value <- function(table, x, y, given_x) {
  a <- slope(pull({{ table }}, {{ x }}), pull({{ table }}, {{ y }}))
  b <- intercept(pull({{ table }}, {{ x }}), pull({{ table }}, {{ y }}))
  return(a * given_x  + b)
}
```

```{r}
fit_300 <- fitted_value(baby, `Gestational Days`, `Birth Weight`, 300)
fit_300
```

### The Variability of the Prediction

We have developed a method making one prediction of a new baby's birth weight based on the number of gestational days, using the data in our sample. But as data scientists, we know that the sample might have been different. Had the sample been different, the regression line would have been different too, and so would our prediction. To see how good our prediction is, we must get a sense of how variable the prediction can be.

To do this, we must generate new samples. We can do that by bootstrapping the scatter plot as in the previous section. We will then fit the regression line to the scatter plot in each replication, and make a prediction based on each line. The figure below shows 10 such lines, and the corresponding predicted birth weight at 300 gestational days.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
x <- 300
reg_lines <- matrix(0, 10, 3)
for(i in seq(10)) {
  rep <- slice_sample(baby, n = nrow(baby), replace = TRUE)
  a <- slope(pull(rep, `Gestational Days`), pull(rep, `Birth Weight`))
  b <- intercept(pull(rep, `Gestational Days`), pull(rep, `Birth Weight`))
  pred_at_x <- a * x + b
  reg_lines[i, ] <- c(a, b, pred_at_x)
}
colnames(reg_lines) <- c("slope", "intercept", "prediction")
reg_lines <- as_tibble(reg_lines)
plot_df <- reg_lines %>%
  mutate(ID = seq(1, 10))

# plot the results
ggplot(plot_df) +
  geom_abline(aes(intercept = intercept, slope = slope, color = factor(ID))) + 
  geom_point(aes(x=300, y=prediction, color = factor(ID))) + 
  labs(color='line #') +
  xlim(291, 309) +
  ylim(122, 138)
```

The predictions vary from one line to the next. The table below shows the slope and intercept of each of the 10 lines, along with the prediction.

```{r}
reg_lines
```

### Bootstrap Prediction Interval

If we increase the number of repetitions of the resampling process, we can generate an empirical histogram of the predictions. This will allow us to create an interval of predictions, using the same percentile method that we used create a bootstrap confidence interval for the slope.

Let us define a function called `bootstrap_prediction` to do this. The function takes five arguments:
- the name of the table
- the column labels of the predictor and response variables, in that order
- the value of $x$ at which to make the prediction
- the desired number of bootstrap repetitions

In each repetition, the function bootstraps the original scatter plot and finds the predicted value of $y$ based on the specified value of $x$. Specifically, it calls the function `fitted_value` that we defined earlier in this section to find the fitted value at the specified $x$.

Finally, it draws the empirical histogram of all the predicted values, and prints the interval consisting of the "middle 95%" of the predicted values. It also prints the predicted value based on the regression line through the original scatter plot.

```{r}
one_bootstrap <- function(x, df, label_x, label_y, new_x) {
  bootstrap_sample <- slice_sample({{ df }}, n = nrow({{ df }}), replace = TRUE)
  bootstrap_prediction <- fitted_value(bootstrap_sample, {{ label_x }}, {{ label_y }}, new_x)
  return(bootstrap_prediction)
}
```

```{r}
bootstrap_prediction <- function(df, x, y, new_x, repetitions) {
  # For each repetition:
  # Bootstrap the scatter; 
  # get the regression prediction at new_x; 
  # collect the vector of predictions
  bstrap_predictions <- map_dbl(.x = 1:repetitions, 
                         .f = one_bootstrap,
                         df = {{ df }}, 
                         label_x = {{ x }}, 
                         label_y = {{ y }},
                         new_x = {{ new_x }})
  
  # Find the ends of the approximate 95% prediction interval
  left <- quantile(bstrap_predictions, c(0.025), type = 1)
  right <- quantile(bstrap_predictions, c(0.975), type = 1)
  
  # Prediction based on original sample
  original <- fitted_value({{ df }}, {{ x }}, {{ y }}, new_x)
  
  # Visualize results
  g <- ggplot(tibble(bstrap_predictions)) +
    geom_histogram(aes(x = bstrap_predictions, y = ..density..),
                   bins = 15, color = "gray", fill = "darkcyan") +
    geom_segment(aes(x = left, y = 0, xend = right, yend = 0),
                 size = 2, color = "salmon") +
    geom_point(aes(x = original, y = 0), size = 3, color = "red")
  print(g)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
bootstrap_prediction(baby, `Gestational Days`, `Birth Weight`, 300, 5000)
```

The figure above shows a bootstrap empirical histogram of the predicted birth weight of a baby at 300 gestational days, based on 5,000 repetitions of the bootstrap process. The empirical distribution is roughly normal. 

An approximate 95% prediction interval of scores has been constructed by taking the "middle 95%" of the predictions, that is, the interval from the 2.5th percentile to the 97.5th percentile of the predictions. The interval ranges from about 127 to about 131. The prediction based on the original sample was about 129, which is close to the center of the interval.

### The Effect of Changing the Value of the Predictor

The figure below shows the histogram of 5,000 bootstrap predictions at 285 gestational days. The prediction based on the original sample is about 122 ounces, and the interval ranges from about 121 ounces to about 123 ounces. 

```{r dpi=80, fig.align="center", message = FALSE}
bootstrap_prediction(baby, `Gestational Days`, `Birth Weight`, 285, 5000)
```

Notice that this interval is narrower than the prediction interval at 300 gestational days. Let us investigate the reason for this.

The mean number of gestational days is about 279 days: 

```{r}
mean(pull(baby, `Gestational Days`))
```

So 285 is nearer to the center of the distribution than 300 is. Typically, the regression lines based on the bootstrap samples are closer to each other near the center of the distribution of the predictor variable. Therefore all of the predicted values are closer together as well. This explains the narrower width of the prediction interval. 

You can see this in the figure below, which shows predictions at $x = 285$ and $x = 300$ for each of ten bootstrap replications. Typically, the lines are farther apart at $x = 300$ than at $x = 285$, and therefore the predictions at $x = 300$ are more variable.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
x1 <- 300
x2 <- 285
reg_lines <- matrix(0, 10, 4)

for(i in seq(10)) {
  rep <- slice_sample(baby, n = nrow(baby), replace = TRUE)
  a <- slope(pull(rep, `Gestational Days`), pull(rep, `Birth Weight`))
  b <- intercept(pull(rep, `Gestational Days`), pull(rep, `Birth Weight`))
  pred_at_x1 <- a * x1 + b
  pred_at_x2 <- a * x2 + b
  reg_lines[i, ] <- c(a, b, pred_at_x1, pred_at_x2)
}
colnames(reg_lines) <- c("slope", "intercept", "prediction_x1", "prediction_x2")
reg_lines <- as_tibble(reg_lines)
plot_df <- reg_lines %>%
  mutate(ID = seq(1, 10))

# plot the results
ggplot(plot_df) +
  geom_abline(aes(intercept = intercept, slope = slope, color = factor(ID))) + 
  geom_point(aes(x=x1, y=prediction_x1, color = factor(ID))) + 
  geom_point(aes(x=x2, y=prediction_x2, color = factor(ID))) + 
  labs(color='line #') +
  xlim(260, 310) +
  ylim(110, 135)
```

### Prediction intervals in the wild

Prediction intervals are so useful to regression analysis that they are already baked into the smooth geom -- no need to write your own! To overlay a prediction interval onto a linear regression plot, we can set the optional `se` argument to `TRUE`. Here is the same example of birth weight prediction based on number of gestational days.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_smooth(method = "lm", se = TRUE, color = "salmon")
```

Here it is again, together with the data: 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = TRUE, color = "salmon")
```

The bands show the confidence interval at each gestational day. Note how much narrower they are for values that are closer to the mean number of gestational days (around 279) compared to those that are farther away (e.g. at around 350) -- this agrees with our analysis.    

### Words of caution

All of the predictions and tests that we have performed in this chapter assume that the regression model holds. Specifically, the methods assume that the scatter plot resembles points generated by starting with points that are on a straight line and then pushing them off the line by adding random normal noise.

If the scatter plot does not look like that, then perhaps the model does not hold for the data. If the model does not hold, then calculations that assume the model to be true are not valid.

Therefore, we must first decide whether the regression model holds for our data, before we start making predictions based on the model or testing hypotheses about parameters of the model. A simple way is to do what we did in this section, which is to draw the scatter diagram of the two variables and see whether it looks roughly linear and evenly spread out around a line. We should also run the diagnostics we developed in the previous section using the residual plot.


-->


