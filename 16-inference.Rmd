# Inference for Regression 

Thus far, our analysis of the relation between variables has been purely descriptive. We know how to find the best straight line to draw through a scatter plot. The line is the best in the sense that it has the smallest mean squared error of estimation among all straight lines.

But what if our data were only a sample from a larger population? If in the sample we found a linear relation between the two variables, would the same be true for the population? Would it be exactly the same linear relation? Could we predict the response of a new individual who is not in our sample?

Such questions of inference and prediction arise if we believe that a scatter plot reflects the underlying relation between the two variables being plotted but does not specify the relation completely. For example, a scatter plot of birth weight versus gestational days shows us the precise relation between the two variables in our sample; but we might wonder whether that relation holds true, or almost true, for all babies in the population from which the sample was drawn, or indeed among babies in general.

As always, inferential thinking begins with a careful examination of the assumptions about the data. Sets of assumptions are known as *models*. Sets of assumptions about randomness in roughly linear scatter plots are called *regression models*.

## A Regression Model

In brief, such models say that the underlying relation between the two variables is perfectly linear; this straight line is the *signal* that we would like to identify. However, we are not able to see the line clearly. What we see are points that are scattered around the line. In each of the points, the signal has been contaminated by *random noise*. Our inferential goal, therefore, is to separate the signal from the noise.

In greater detail, the regression model specifies that the points in the scatter plot are generated at random as follows.

- The relation between $x$ and $y$ is perfectly linear. We cannot see this "true line" but it exists.
- The scatter plot is created by taking points on the line and pushing them off the line vertically, either above or below, as follows:
    - For each $x$, find the corresponding point on the true line (that's the signal), and then generate the noise or error.
    - The errors are drawn at random with replacement from a population of errors that has a normal distribution with mean 0.
    - Create a point whose horizontal coordinate is $x$ and whose vertical coordinate is "the height of the true line at $x$, plus the error".
- Finally, erase the true line from the scatter, and display just the points created.

Based on this scatter plot, how should we estimate the true line? The best line that we can put through a scatter plot is the regression line. So the regression line is a natural estimate of the true line. 

The simulation below shows how close the regression line is to the true line. The first panel shows how the scatter plot is generated from the true line. The second shows the scatter plot that we see. The third shows the regression line through the plot. The fourth shows both the regression line and the true line.

To run the simulation, call the function `draw_and_compare` with three arguments: the slope of the true line, the intercept of the true line, and the sample size.

Run the simulation a few times, with different values for the slope and intercept of the true line, and varying sample sizes. Because all the points are generated according to the model, you will see that the regression line is a good estimate of the true line if the sample size is moderately large.

```{r, message = FALSE, echo = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

```{r message = FALSE, echo = FALSE}
draw_and_compare <- function(true_slope, true_int, sample_size) {
  x <- rnorm(sample_size, 50, 5)
  xlims <- c(min(x), max(x))
  eps <- rnorm(sample_size, 0, 6)
  y <- (true_slope*x + true_int) + eps
  tyche <- tibble(x = x, y = y)
  tyche2 <- tibble(x = xlims, y = true_slope * xlims + true_int)

  g <- ggplot() +
    geom_point(data = tyche, aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    geom_line(data = tyche2, aes(x = x, y = y), color = "salmon", size = 1) + 
    ggtitle("True Line, and Points Created")
  print(g)
  g <- ggplot() +
    geom_point(data = tyche, aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    ggtitle("What We Get to See")
  print(g)
  
  g <- ggplot(tyche) + 
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    geom_smooth(aes(x = x, y = y), method = "lm", se = FALSE) + 
    ggtitle("Regression Line: Estimate of True Line")
  print(g)
  
  g <- ggplot(tyche) + 
    geom_point(aes(x = x, y = y), color = "darkcyan", alpha = 0.5) +
    geom_line(data = tyche2, aes(x = x, y = y), color = "salmon", size = 1) + 
    geom_smooth(aes(x = x, y = y), method = "lm", se = FALSE) + 
    ggtitle("Regression Line and True Line")
  print(g)
  
}
```

```{r dpi=80, fig.align="center", message = FALSE}
draw_and_compare(4, -5, 20)
```

In reality, of course, we will never see the true line. What the simulation shows is that if the regression model looks plausible, and if we have a large sample, then the regression line is a good approximation to the true line.

## Inference for the True Slope

Our simulations show that if the regression model holds and the sample size is large, then the regression line is likely to be close to the true line. This allows us to estimate the slope of the true line.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Let us also load in the `baby` dataset we have been working with in earlier sections. 

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

The calculations in this section assume all the relevant functions we have already defined: `slope`, `intercept`, and `fit`.

```{r}
slope <- function(label_x, label_y) {
  r <- cor(label_x,  label_y)
  return (r * sd(label_y) / sd(label_x))
}

intercept <- function(label_x, label_y) {
  return(mean(label_y) - 
           slope(label_x, label_y) * mean(label_x))
}

fit <- function(x, y) {
  # Return the height of the regression line at each x value.
  a <- slope(x, y)
  b <- intercept(x, y)
  return(a * x + b)
}
```

### Estimating the slope of the true line

We will use our familiar sample of mothers and their newborn babies to develop a method of estimating the slope of the true line. First, let's see if we believe that the regression model is an appropriate set of assumptions for describing the relation between birth weight and the number of gestational days.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon")
```

```{r}
cor(pull(baby, `Gestational Days`), pull(baby, `Birth Weight`))
```

By and large, the scatter looks fairly evenly distributed around the line, though there are some points that are scattered on the outskirts of the main cloud. The correlation is 0.4 and the regression line has a positive slope.

Does this reflect the fact that the true line has a positive slope? To answer this question, let us see if we can estimate the true slope. We certainly have one estimate of it: the slope of our regression line. That's about 0.47 ounces per day.

```{r}
slope(pull(baby, `Gestational Days`), pull(baby, `Birth Weight`))
```

But had the scatter plot come out differently, the regression line would have been different and might have had a different slope. How do we figure out how different the slope might have been?

We need another sample of points, so that we can draw the regression line through the new scatter plot and find its slope. But from where will get another sample?

You guessed it â€“ we will *bootstrap our original sample*. That will give us a bootstrapped scatter plot, through which we can draw a regression line.

### Bootstrapping the Scatter Plot 

We can simulate new samples by random sampling with replacement from the original sample, as many times as the original sample size. Each of these new samples will give us a scatter plot. We will call that a bootstrapped scatter plot, and for short, we will call the entire process bootstrapping the scatter plot.

Here is the original scatter diagram from the sample, and four replications of the bootstrap resampling procedure. Notice how the resampled scatter plots are in general a little more sparse than the original. That is because some of the original points do not get selected in the samples.

```{r dpi=80, fig.align="center", message = FALSE, echo = FALSE}
g <- ggplot(baby, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  ggtitle("Original sample")
print(g)

for(i in seq(4)) {
  rep <- slice_sample(baby, n = nrow(baby), replace = TRUE)
  g <- ggplot(rep, aes(x = `Gestational Days`, y = `Birth Weight`)) + 
    geom_point(color = "darkcyan", alpha = 0.5) + 
    ggtitle(paste("Bootstrap sample ", i))
  print(g)
}
```

### Estimating the True Slope

We can bootstrap the scatter plot a large number of times, and draw a regression line through each bootstrapped plot. Each of those lines has a slope. We can simply collect all the slopes and draw their empirical histogram. 

```{r}
one_bootstrap <- function(x, df, label_x, label_y) {
  bootstrap_sample <- slice_sample({{ df }}, n = nrow({{ df }}), replace = TRUE)
  bootstrap_slope <- slope(pull(bootstrap_sample, {{ label_x }}), 
                           pull(bootstrap_sample, {{ label_y }}))
  return(bootstrap_slope)
}
```

```{r}
bstrap_slopes <- map_dbl(.x = 1:5000, 
                         .f = one_bootstrap,
                         df = baby, 
                         label_x = `Gestational Days`, 
                         label_y = `Birth Weight`)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(tibble(bstrap_slopes)) + 
  geom_histogram(aes(x = bstrap_slopes, y = ..density..), 
                 bins = 20, color = "gray", fill = "darkcyan")
```

We can then construct an approximate 95% confidence interval for the slope of the true line, using the bootstrap percentile method. The confidence interval extends from the 2.5th percentile to the 97.5th percentile of the 5000 bootstrapped slopes.

```{r}
left <- quantile(bstrap_slopes, c(0.025), type = 1)
right <- quantile(bstrap_slopes, c(0.975), type = 1)
c(left, right)
```

An approximate 95% confidence interval for the true slope extends from about 0.38 ounces per day to about 0.56 ounces per day.

### A Function to Bootstrap the Slope

Let us collect all the steps of our method of estimating the slope and define a function `bootstrap_slope` that carries them out. Its arguments are the name of the table and the labels of the predictor and response variables, and the desired number of bootstrap replications. In each replication, the function bootstraps the original scatter plot and calculates the slope of the resulting regression line. It then draws the histogram of all the generated slopes and prints the interval consisting of the "middle 95%" of the slopes.

```{r}
bootstrap_slope <- function(df, x, y, repetitions) {
  
  # For each repetition:
  # Bootstrap the scatter, get the slope of the regression line,
  # collect the vector of generated slopes
  bstrap_slopes <- map_dbl(.x = 1:repetitions, 
                         .f = one_bootstrap,
                         df = {{ df }}, 
                         label_x = {{ x }}, 
                         label_y = {{ y }})
  
  # Find the endpoints of the 95% confidence interval for the true slope
  left <- quantile(bstrap_slopes, c(0.025), type = 1)
  right <- quantile(bstrap_slopes, c(0.975), type = 1)
  
  # Slope of the regression line from the original sample
  observed_slope <- slope(pull({{ df }}, {{ x }}), pull({{ df }}, {{ y }}))
  
  # Visualize results
  g <- ggplot(tibble(bstrap_slopes)) +
    geom_histogram(aes(x = bstrap_slopes, y = ..density..), 
                   bins = 15, color = "gray", fill = "darkcyan") + 
    geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
                 size = 2, color = "salmon") + 
    geom_point(aes(x = observed_slope, y = 0), size = 3, color = "red")
  print(g)
}
```

```{r dpi=80, fig.align="center", message = FALSE}
bootstrap_slope(baby, `Gestational Days`, `Birth Weight`, 5000)
```

Now that we have a function that automates our process of estimating the slope of the true line in a regression model, we can use it on other variables as well.

For example, let's examine the relation between birth weight and the mother's height. Do taller women tend to have heavier babies?

The regression model seems reasonable, based on the scatter plot, but the correlation is not high. It's just about 0.2.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Maternal Height`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon")
```

```{r}
cor(pull(baby, `Maternal Height`), pull(baby, `Birth Weight`))
```

As before, we can use `bootstrap_slope` to estimate the slope of the true line in the regression model.

```{r dpi=80, fig.align="center", message = FALSE}
bootstrap_slope(baby, `Maternal Height`, `Birth Weight`, 5000)
```

A 95% confidence interval for the true slope extends from about 1 ounce per inch to about 1.9 ounces per inch.

### Could the True Slope Be 0?

Suppose we believe that our data follow the regression model, and we fit the regression line to estimate the true line. If the regression line isn't perfectly flat, as is almost invariably the case, we will be observing some linear association in the scatter plot.

But what if that observation is spurious? In other words, what if the true line was flat â€“ that is, there was no linear relation between the two variables â€“ and the association that we observed was just due to randomness in generating the points that form our sample?

Here is a simulation that illustrates why this question arises. We will once again call the function `draw_and_compare`, this time requiring the true line to have slope 0. Our goal is to see whether our regression line shows a slope that is not 0.

Remember that the arguments to the function `draw_and_compare` are the slope and the intercept of the true line, and the number of points to be generated.

```{r dpi=80, fig.align="center", message = FALSE}
draw_and_compare(0, 10, 25)
```

Run the simulation a few times, keeping the slope of the true line 0 each time. You will notice that while the slope of the true line is 0, the slope of the regression line is typically not 0. The regression line sometimes slopes upwards, and sometimes downwards, each time giving us a false impression that the two variables are correlated.

To decide whether or not the slope that we are seeing is real, we would like to test the following hypotheses:

_Null Hypothesis._ The slope of the true line is 0. 

_Alternative Hypothesis._ The slope of the true line is not 0. 

We are well positioned to do this. Since we can construct a 95% confidence interval for the true slope, all we have to do is see whether the interval contains 0. 

If it doesn't, then we can reject the null hypothesis (with the 5% cutoff for the P-value). 

If the confidence interval for the true slope does contain 0, then we don't have enough evidence to reject the null hypothesis. Perhaps the slope that we are seeing is spurious.

Let's use this method in an example. Suppose we try to estimate the birth weight of the baby based on the mother's age. Based on the sample, the slope of the regression line for estimating birth weight based on maternal age is positive, about 0.08 ounces per year. 

```{r}
slope(pull(baby, `Maternal Age`), pull(baby, `Birth Weight`))
```

Though the slope is positive, it's pretty small. The regression line is so close to flat that it raises the question of whether the true line is flat.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby, aes(x = `Maternal Age`, y = `Birth Weight`)) + 
  geom_point(color = "darkcyan", alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, color = "salmon")
```

We can use `bootstrap_slope` to estimate the slope of the true line. The calculation shows that an approximate 95% bootstrap confidence interval for the true slope has a negative left end point and a positive right end point â€“ in other words, the interval contains 0. 

```{r}
bootstrap_slope(baby, `Maternal Age`, `Birth Weight`, 5000)
```

Because the interval contains 0, we cannot reject the null hypothesis that the slope of the true linear relation between maternal age and baby's birth weight is 0. Based on this analysis, it would be unwise to predict birth weight based on the regression model with maternal age as the predictor.

## Prediction Intervals

TBA


