---
output:
  pdf_document: default
  html_document: default
---
# Estimation

The previous chapters have developed ways to show how to use data to decide between competing questions about the world. For instance, does Harvard enroll proportionately less Asian-Americans than other private universities in the United States; are the exam grades in one lab section of a course too low when compared to other lab sections; can an experimental drug bring an improvement to patients recovering from brain trauma? We evaluated questions like these by means of an *hypothesis test* where we put forward two hypotheses: a *null* hypothesis and an *alternative* hypothesis.  

Often times we are interested in what a value might look like. For instance, airlines might be interested in the median flight delay of their flights to preserve customer satisfaction; political candidates may look to the percentage of voters favoring them to gauge how aggressive their campaigning should be. 

Put in the language of statistics, we are interested in *estimating* some unknown *parameter* about a population. If we had all the data available to us, we could compute the parameter with ease. However, we often do not have access to the full population (as is the case with polling voters) or there may be too much data to work with that it becomes computationally prohibitive (as is the case with flights).  

We have seen before that empirical distributions can provide reliable approximations to the true (and usually unknown) distribution and that, likewise, a statistic computed from it can provide a reliable estimate of the parameter in question. However, the value of a statistic can turn out *differently* depending on the random samples that are drawn to compose a empirical distribution. How much can the value of a statistic vary? Could we quantify this uncertainty?  

This chapter develops a way to answer this question using an important technique in data science called the *bootstrap*. We begin by introducing order statistics and percentiles. This will provide us the tools needed to develop the bootstrap to produce distributions from a sample, in which we apply order statistics to the generated distributions to obtain something called the *confidence interval*.

## Order Statistics 

The minimum, maximum, and the median are part of what we call *order statistics*. Order statistics are values at certain positions in numerical data after reordering the data in ascending order. 

### Prerequisites

This section will make use of data for all flights that departed New York City in 2013. The dataset is made available by the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/) in the United States. Let's also load in the `tidyverse` as usual.   

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

### The `flights` data frame 

In our prior exploration of this data frame, we generated empirical distributions of departure delays. Let's revisit this study and visualize the departure delays again. 

```{r dpi=80,  fig.align="center", message = FALSE, warning = FALSE}
ggplot(flights) + 
  geom_histogram(aes(x = dep_delay, y = ..density..), col="grey", bins = 30)
```

As before, we are interested in the bulk of the data here, so we can ignore the 1.83% of flights with delays of more than 150 minutes.

```{r}
flights150 <- flights %>%
  filter(dep_delay <= 150)
```

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(flights150) + 
  geom_histogram(aes(x = dep_delay, y = ..density..), col="grey", bins = 30)
```

Let's extract the departure delay column as a vector. 

```{r}
dep_delays <- flights150 %>% pull(dep_delay)
```

### `median`

The *median* is a popular order statistic that gives us a sense of the central tendency of the data. It is the value at the middle position in the data after reordering of the values.

```{r}
median(dep_delays)
```

This tells us that half of the flights had early departures -- not bad! Recall that we also used the mean to understand the central tendency. Note that the *mean* (or *average*) is a statistic, but it is not an order statistic. Let's compare with the mean of departure delays. 

```{r}
mean(dep_delays)
```

There is quite a bit of discrepancy between the two. Observe that the histogram above has a very long right tail; the mean is pulled upward by flights with long departure delays. In general:

> If a distribution has a long tail, the mean will be pulled away from the median in the direction of the tail. Otherwise, if the distribution is symmetrical, the mean and the median will equal. 

When the distribution is "skewed" like the one here, the median can be a stronger indicator of central tendency. 

There are cases, by the way, where two median exists. Such an event occurs exactly when the number of values is an even number. If there are 10 values, the 5th and the 6th values in the ascending order are the two medians. The one at the lower position in the order is the *odd median* and the other one, i.e., the one at the higher position in the order, is the *even median*. To compute the median in this case, we usually take the average of the odd and even median.  

### `min` and `max` 

What is the earliest flight that left? We can find out by looking for the *minimum* departed flight delay. 

```{r}
min(dep_delays)
```

The authors admit that this flight might have left a little too early for their liking. What about the latest flight? 

```{r}
max(dep_delays)
```

Recall that this maximum is actually artificial because we filtered all rows whose departure delay was more than 150. To recover the true maximum, we need to refer to the original `flights` data. 

```{r}
flights %>%
  pull(dep_delay) %>%
  max(na.rm = TRUE)
```

That is almost a *22 hour* delay -- better book a hotel room! 

## Percentiles 

Now that we have an understanding of order statistics, we can use it to develop the notion of a *percentile*. We will also explore a closely related concept called the *quartile*.  

You are probably already familiar with the concept of percentiles from sports or [standardized testing like the SAT](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf). Organizations like the College Board talk so much about percentiles -- to the extent of writing full guides on how to interpret them -- because they are indicators of how students perform relative to other exam-takers. Indeed, the percentile is another order statistic that tells us something about the *rank* of a data point after reordering the elements in a dataset. Now that we have an understanding of order statistics, we can use it to develop the notion of a *percentile*. We will also explore a closely related concept called the *quartile*.     

### Prerequisites

Before starting, let's load the `tidyverse` as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

### The `finals` tibble  

In the spirit of the College Board, we will examine exam scores to develop an understanding of percentiles. Recall that the `finals` data frame contains final exams from two offerings of an undergraduate computer science course taught at the University of Miami. Let's load it in. 

```{r message = FALSE}
finals <- read_csv("data/final_exams.csv")
finals
```

The dataset contains final scores from a total of 105 students. 

```{r}
nrow(finals)
```

We will not concern ourselves with the individual offerings of the course this time. Since the scores are of interest for this study, let us extract a vector of scores from the tibble.  

```{r}
scores <- finals %>%
  pull(grade)
```

To orient ourselves to the data, we can look at the maximum and minimum scores. 

```{r}
max(scores)
min(scores)
```

We may be alarmed to see that the minimum score of 0. Some insight into the course would reveal that there were a few students who did not appear for the final exam (don't be one of them!). 

Finally, let us visualize the distribution of scores. 

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(finals) + 
  geom_histogram(aes(x = grade, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 10)
```


### The `quantile` function

The percentile is an order statistic where the position of the data is not the rank but a percentage that specifies relative position in the data. For instance, the 50th percentile is the smallest value that is at least as large as 50% of the elements in `scores`; it must be a value on the list of scores. We can compute this simply with the `quantile()` function in R. 

```{r}
quantile(scores, c(0.5), type = 1)
```

The value at the 50th percentile is something we already know: the median score! 

```{r}
median(scores)
```

The `quantile` function gives the value that cuts off the first *n* percent of the data values when it is sorted in ascending order. There are many ways to compute percentiles (see the help page for a sneak peek, with `?quantile`). The one that matches the definition used here corresponds to `type = 1`.      

The additional argument passed in is a vector of desired percentages. These must be between 0 and 1. This is how `quantile` gets its name: *quantiles* are *percentiles* scaled to have a value between 0 and 1, e.g. `0.5` rather than `50`. 

Let us look at some more percentile values in the vector of scores.

```{r}
quantile(scores, c(0.05, 0.2, 0.9, 0.95, 0.99, 1), type = 1)
```

Let's pick apart some of these values. We see that the value at the 5th percentile is the lowest exam score that is at least as large as 5% of the scores. We can confirm this easily by summing the number of scores less than 17 and dividing by the total number of scores.  

```{r}
sum(scores < 17) / length(scores)
```

Moving on up, we see that the 95th and 99th percentiles are quite close together. We also observe that the 100th percentile is 98, which corresponds to the maximum score obtained on the final. That is, a 98 is at least as large as 100% of the scores, which is the entire class.  

The 0th percentile is simply the smallest value in the dataset, as 0% of the data is at least as large as it. In other words, there is no exam score in the class lower than a 0. 

```{r, error=TRUE}
quantile(scores, c(0), type = 1)
```

If the College Board says a student is in the "top 10 percentile", this would be a misnomer. What they really mean to say is that the student is in the $1 - \text{top X percentile}$, or 90th percentile. 

### Quartiles

In addition to the medians, common percentiles are the $1/4$th and $3/4$th, which we often call the bottom quarter and the top quarter. Basically, we chop the data in quarters and use the boundaries between the neighboring quarters. Since these percentiles partition the data into quarters, these are given a special name: *quartiles*. 

```{r, error=TRUE}
quantile(scores, c(0/4, 1/4, 2/4, 3/4, 4/4), type = 1)
```

Observe what happens when we omit the vector of percentages. 

```{r}
quantile(scores, type = 1)
```

The corresponding 0th, 25th, 50th, 75th, and 100th percentiles of the vector are returned. 

### Combining two percentiles

By combining two percentiles, we can get a rough sense of the distribution. For example, the combination of 25th and 75th percentiles represents the "middle" 50%. Similarly, the 2.5th and 97.5th percentiles represent the middle 95% of the data. That is, 

```{r}
quantile(scores, c(0.025, 0.975), type = 1)
```

95% of the scores is between 0 and 95. We could find this more directly by realizing that the middle 95% corresponds to going up and down from the 50th percentile by half of that amount, which is 47.5%.

```{r}
middle_area <- 0.95
quantile(scores, 0.5 + (middle_area / 2) * c(-1, 1), type = 1)
```

As one more example, here is the middle 90% of scores. 

```{r}
middle_area <- 0.90
quantile(scores, 0.5 + (middle_area / 2) * c(-1, 1), type = 1)
```

### Advantages of percentiles 

Percentile is a useful concept because it eliminates the use of population size in specifying the position; that is, the position specification does not directly take into account the size of the data. What do we mean by that?

Let's return to the example of final exam scores. Suppose that one offering of the class contained 50 students while another had 200 students. Consider the "top 10 students" in each class. 

Since top 10 is 20% of 50 students, there is a 20% chance for a student to be among the top 10, while the chances decrease to 5% for the class of 200. That is, if we specify a top group with its size, the significance being in the top group varies depending on the size of the population and so we must to specify the size of the underlying group, e.g., "top 10 in a group of 4000 students". Percentiles are nice in that they are not sensitive to these changes.


## Bootstrapping

It is usually the case that a data scientist will receive a collection of samples from an underlying population to which she has no access. If she had access to the underlying population, she could calculate the parameter value directly. Since that is impossible, is there a way for her to deal with the sample collection at hand to generate a range of values for the statistic?

Yes! This is a technique we call *resampling*, which is more popularly known as *bootstrapping*. In bootstrapping, we treat the dataset at hand as the true population and generate samples from it. But there is a catch. Each sample data set that we generate should be equal in size to the original. This necessarily means that our sampling plan be done *with* replacement, rather than without. 

Since the samples have the same size as the original with the use of replacement, duplicates and omissions can arise. That is, there are items that will appear multiple times as well as items that are missing. Because randomness is involved, the discrepancy varies.

### Prerequisites

This section will defer again to the New York City flights in 2013 from the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/). Let's also load in the `tidyverse` as usual.   

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(nycflights13)
```

### Population parameter: the median time spent in the air 

When studying this dataset, we have spent a lot of time examining flight departure delays. This time we will turn our attention to another variable in the tibble which tracks the amount of time a flight spent in air, in minutes. The variable is called `air_time`.

Let's visualize the distribution of air time in `flights`.

Recall the distribution of departure delays in `flights150`. 

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(flights) + 
  geom_histogram(aes(x = air_time, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 20)
```

As before, let's concentrate on the bulk of the data and filter out any flights that flew for more than 400 minutes.  

```{r}
flights400 <- flights %>%
  filter(air_time < 400) %>%
  drop_na()
```

We plot this distribution one more time. 

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(flights400) + 
  geom_histogram(aes(x = air_time, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 20)
```

The parameter we will select for this study is the mean air time. 

```{r}
pop_mean <- flights400 %>%
  pull(air_time) %>%
  mean()
pop_mean
```

Let us see how well we can estimate this value based on a sample of the flights. We will study two such samples: an artificial sample and a random sample.  

### First try: A mechanical sample 

For our mechanical sample, we will assume that we have been given only a cross-section of the flights data and try to estimate the population median based on this sample. Let us suppose we have been given the flight data for only the months of September and October.     

```{r}
flights_sample <- flights400 %>%
  filter(month == 9 | month == 10)
```

There are 55,522 flights appearing in the subset. Let's visualize the distribution of air time from our sample. 

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(flights_sample) + 
  geom_histogram(aes(x = air_time, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 20)
```

It appears close to the population of flights, though there are notable differences: flights that have longer air times (between 300 and 400 minutes) appear exaggerated in this dataset. Let's compute the mean from this sample.

```{r}
sample_mean <- flights_sample %>%
  pull(air_time) %>%
  mean()
sample_mean
```

It is quite different from the population median. Nevertheless, this subset of flights will serve as the dataset from which we will bootstrap our samples. Put another way, __we will treat this sample as if it were the population.__  

### Bootstrapping the Sample Mean

To perform a bootstrap, we will draw from the sample, at random __with replacement__, the same number of times as the size of the sample dataset. 

To simplify the work, let us extract the column of air times as a vector. 

```{r}
air_times <- flights_sample %>% pull(air_time)
```

We know already how to sample at random with replacement from a vector using `sample`. Computing the sample mean is also straightforward: just pipe the returned vector into `mean`. 

```{r}
sample_mean <- air_times %>% 
  sample(replace = TRUE) %>%
  mean()
sample_mean
```

Let us move this work into a function we can call. 

```{r}
one_sample_mean <- function() {
  sample_mean <- flights_sample %>% 
    pull(air_time) %>%
    sample(replace = TRUE) %>%
    mean()
  return(sample_mean)
}
```

Give it a run! 

```{r}
one_sample_mean()
```

This function is actually quite useful. Let's generalize the function so that we may call it with other datasets we will work with. The modified function will receive three parameters: (1) a tibble to sample from, (2) the column to work on, and (3) the statistic to compute. 

```{r}
one_sample_value <- function(df, label, statistic) {
  sample_value <- df %>% 
    pull({{ label }}) %>%
    sample(replace = TRUE) %>%
    statistic()
  return(sample_value)
}
```

We can now call it as follows. 

```{r}
one_sample_value(flights_sample, air_time, mean)
```

> Q: What's the deal with those (ugly) double curly braces (`{{`) ? To make R programming more enjoyable, the tidyverse allows us to write out column names, e.g. `air_time`, just like we would variable names. The catch is that when we try to use such syntax sugar from inside a function, R has no idea what we mean. In other words, when we say `pull(label)` R thinks that we want to extract a vector from a column called `label`, despite the fact we passed in `air_time` as an argument. To lead R in the right direction, we surround `label` with `{{` so that R knows to interpret `label` as, indeed, `air_time`. 

### Empirical Distribution of the Sample Mean

We now have all the pieces in place to perform the bootstrap. We will replicate this process many times so that we can compose an empirical distribution of all the bootstrapped sample means. Let's repeat the process 10,000 times. 

```{r}
bstrap_means <- replicate(n = 10000, one_sample_value(flights_sample, air_time, mean))
```

Let us visualize the bootstrapped sample means using a histogram. 

```{r dpi=80,  fig.align="center", message = FALSE}
df <- tibble(bstrap_means)
ggplot(df, aes(x = bstrap_means, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 8)
```

### Did it Capture the Parameter?

How often does the population mean fall somewhere in the empirical histogram? Does it reside 
"somewhere at the center" or at the fringes where the tails are? Let us be more specific by what we mean when we say "somewhere at the center": the middle 95% of bootstrapped means containing the population mean.  

We can identify the "middle 95%" using the percentiles we learned from the last section. Here they are: 

```{r}
desired_area <- 0.95
middle95 <- quantile(bstrap_means, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle95
```

Let us annotate this interval on the histogram. 

```{r dpi=80,  fig.align="center", message = FALSE}
df <- tibble(bstrap_means)

ggplot(df, aes(x = bstrap_means, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 8) +
  geom_segment(aes(x = middle95[1], y = 0, xend = middle95[2], yend = 0), 
                   size = 2, color = "salmon") 
```

```{r}
pop_mean
```

Our population mean is 149.6 minutes -- that is nowhere to be seen in this interval or even in the histogram! It would seem then that in all of the 10,000 replications of the bootstrap, not even one was able to capture the population mean. What happened? 

Recall the subset selection we used: all flights in September or October. This was a very artificial selection that is prone to bias. We learned before when we discussed sampling plans that bias in the data can mislead the results, especially when using a convenient sample such as the one here. 

### Second try: A random sample 

We will now try to estimate the population mean using a random sample of flights. Let us select at random without replacement 10,000 flights from the data. 

```{r}
flights_sample <- flights400 %>% 
  slice_sample(n = 10000, replace = FALSE)
```

We will visualize what our random sample looks like. 

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(flights_sample) + 
  geom_histogram(aes(x = air_time, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 20)
```

Let us also compute the sample mean again. 

```{r}
sample_mean <- flights_sample %>%
  pull(air_time) %>%
  mean()
sample_mean
```

We observe that the sample mean is also much closer to the population mean, unlike our mechanical selection attempt. This is confirmation of the Law of Averages (finally) at work: when we sample at random and the sample size is large, the distribution of the sample closely follows that of the flight population. 

Let us now repeat the bootstrap. Recall that we will treat this sample as if it were the population. 

### Empirical Distribution of the Sample Mean (revisited)

We have done all the hard work already in setting up the bootstrap. To redo the process, we need only to pass in the random sample contained in `flights_sample`. As before, let us repeat the process 10,000 times. 

```{r}
bstrap_means <- replicate(n = 10000, one_sample_value(flights_sample, air_time, mean))
```

We will identify the "middle 95%". Here is the interval:

```{r}
desired_area <- 0.95
middle95 <- quantile(bstrap_means, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle95
```

Let us annotate this interval on the histogram. We will also plot the population mean as a red dot. 

```{r dpi=80,  fig.align="center", message = FALSE}
df <- tibble(bstrap_means)

ggplot(df, aes(x = bstrap_means, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 8) +
  geom_segment(aes(x = middle95[1], y = 0, xend = middle95[2], yend = 0), 
                   size = 2, color = "salmon") +
  geom_point(aes(x = pop_mean, y = 0), color = "red", size = 3)
```

The population mean of 149.6 minutes falls in this interval. We conclude that the "middle 95%" interval of bootstrapped means successfully captured the parameter. 

### Lucky try? 

Our interval of bootstrapped means captured the parameter in the air time data. But were we just lucky? We can test it out. 

We would like to see how often the "middle 95%" interval captures the parameter. We will need to redo the entire process many times to find an answer. More specifically, we will follow the recipe:

* Generate a sample of size 10,000 from the population. For the sampling plan, sample at random without replacement. 
* Do 10,000 replications of the bootstrap process and find the "middle 95%" interval of bootstrapped means. 

We will repeat this process 100 times so that we end up with 100 intervals; we will count how many of them contain the population mean.  

```{r}
all_the_bootstraps <- function() {
  desired_area <- 0.95
    
  flights_sample <- flights400 %>% slice_sample(n = 10000, replace = FALSE)
  bstrap_means <- replicate(n = 10000, one_sample_value(flights_sample, air_time, mean))
  middle95 <- quantile(bstrap_means, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
  return(middle95)
}
```

```{r}
intervals <- replicate(n = 100, all_the_bootstraps())
```

> NOTE: This simulation will take awhile (> 20 minutes). Grab a coffee!

Let's examine some of the intervals of bootstrapped means. 

```{r}
intervals[,1]
```

```{r}
intervals[,2]
```

Let’s transform `intervals` into a tibble which will make it easier to understand and visualize the results.

```{r}
left_column <- intervals[1,]
right_column <- intervals[2,]
```

```{r}
interval_df <- tibble(
  replication = 1:100,
  left = left_column,
  right = right_column
)
interval_df
```

How many of these contain the population mean? We can count the number of intervals where the population mean is between the left and right endpoints. 

```{r}
interval_df %>%
  filter(left <= pop_mean & right >= pop_mean) %>%
  nrow()
```

We can visualize these intervals by stacking them on top of each other vertically. The vertical red line shows where the population mean lies. Under real-life circumstances, we do not know where it is. 

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(interval_df) + 
  geom_segment(aes(x = left, y = replication, 
                   xend = right, yend = replication), color = "salmon") +
  geom_vline(xintercept = pop_mean, color = "red") + 
  xlab("Air time (minutes)")
```

We expect about 95 of the 100 intervals to cross the vertical line; meaning, it contains the parameter. We would label such intervals as "good". If an interval does not, oh well -- that's the nature of chance. Fortunately, these do not occur often. In fact, they should occur about 5 times among 100 trials, or 95%. The strength of statistics lies not in clairvoyance, but in the ability to quantify our uncertainty. 

### Bootstrap round-up 

Before we close this section, we end with a quick summary on how to perform a bootstrap. 

__Goal:__ To estimate some population parameter we do not know about, e.g., the mean air time of New York City flights. 

* Select a sampling plan. A safe bet is to sample at *random* without replacement from the population. Be sure that the sample drawn is *large* in size.
* Bootstrap the random sample (this time, *with* replacement) and compute the desired statistic from it. 
* Replicate this process a great number of times to obtain many bootstrapped samples.
* Find the "middle 95%" interval of the bootstrapped samples. 

## Confidence Intervals 

The previous section developed a way to estimate the value of a parameter we do not know. Because chance is an inevitable part of drawing a random sample, we cannot be precise and offer a single value for this estimate, e.g., we can determine that the mean height of all individuals in the United States is *exactly* 5.3 feet. Instead, we provide an *interval* of estimates by looking at a bulk of values that are "somewhere in the center". Typically this entails looking at the "middle 95%" interval, but we may prefer other intervals such as the "middle 90%" or even the "middle 99%".  

Recall that knowing the value of the parameter beforehand is a rare luxury out of reach; if we could obtain it somehow, there would be no need for statistical methods like the bootstrap. Instead, data scientists rely on intervals of estimates with confidence that the parameter will sit on this interval some percentage of the time. 

These "intervals of estimates" are so important to statistics and data science that they are given a special name: the *confidence interval*. This section will explore confidence intervals, and their use, in greater depth. 


### Prerequisites

We will make use of the tidyverse in this chapter, so let’s load it in as usual.

```{r message = FALSE}
library(tidyverse)
```

We will also bring forward the `one_sample_value` function we wrote in the previous section. 

```{r}
one_sample_value <- function(df, label, statistic) {
  sample_value <- df %>% 
    pull({{ label }}) %>%
    sample(replace = TRUE) %>%
    statistic()
  return(sample_value)
}
```

For the running example in this section, we will turn to American football from the National Collegiate Athletic Association (NCAA) covering the years 2005 from 2011, which is available at [Football Outsiders](https://www.footballoutsiders.com/college-xp/2012/2005-11-college-football-receiver-data-targets-and-catches). There are two things to note before working with the data: 

* The same receiver may appear more than once across different seasons so we eliminate any duplicate players from the tibble using `distinct`. 
* Because real-life circumstances often do not grant us access to the population, we will mimic such circumstances by using only a random sample of the data and treating that as our population. We will select 1,000 random samples without replacement.

Let us load in this dataset and o

```{r echo = FALSE}
set.seed(10)
```

```{r message = FALSE}
path <- "data/ncaa-receivers-2005-2011.csv"
ncaa <- read_csv(path) %>%
  distinct(Player, .keep_all = TRUE) %>%
  slice_sample(n = 1000, replace = FALSE)
```

We can inspect the resulting table. Note that there are 1,000 rows in the tibble.

```{r}
ncaa
```

### Estimating a Population Median 

We will apply bootstrapping to the `ncaa` tibble to estimate an unknown parameter: the ratio of yards per catch. Let us first form a subset of the tibble with the relevant data. 

```{r}
yards_catches_df <- ncaa %>%
  transmute(player = Player,
            catches = Catches,
            yards = Yards,
            ratio = yards / catches
            )
yards_catches_df
```

Let us visualize the histogram of yards per catch. 

```{r dpi=80,  fig.align="center", warning = FALSE}
ggplot(yards_catches_df) + 
  geom_histogram(aes(x = ratio, y = ..density..), 
                 col="grey", fill = "darkcyan", bins = 20)
```

We can inspect the maximum ratio value which is an impressive 43 yards per catch by Darius Williams. 

```{r}
yards_catches_df %>%
  slice_max(order_by = ratio)
```

The maximum ratio value helps us realize just how far away a value like 43 yards per catch is from the bulk of ratios. In fact, according to the histogram it appears that there are several "unusual" ratios that appear towards the far right of the histogram.  

We saw earlier that the mean can be "pulled" up or down by unusually large or small values. Under such circumstances, we prefer to use the median as it is not sensitive to extreme ratio values. The median ratio in this sample is about 11.23 yards per catch. 

```{r}
yards_catches_df %>%
  pull(ratio) %>%
  median()
```

We are now ready to bootstrap from this random sample. Recall that `one_sample_value` will perform the bootstrap for us. We will replicate the bootstrap process a large number of times, say 10,000, so that we can plot an empirical histogram of the bootstrapped medians. 

```{r}
# Do the bootstrap!
bstrap_medians <- replicate(n = 10000, one_sample_value(yards_catches_df, ratio, median))
```

As before, we will identify the 95% confidence interval. Here is the interval:

```{r}
desired_area <- 0.95
middle <- quantile(bstrap_medians, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle
```

Let us plot the empirical histogram and annotate the interval on this histogram. 

```{r dpi=80,  fig.align="center", message = FALSE}
df <- tibble(bstrap_medians)
ggplot(df, aes(x = bstrap_medians, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 13) +
  geom_segment(aes(x = middle[1], y = 0, xend = middle[2], yend = 0), 
                   size = 2, color = "salmon")
```

This looks a lot like what we saw in the previous section, with one key difference: there is no dot indicating where the parameter is! We do not know where the dot will fall or if it is even on this interval. 
Statistics is not clairvoyance. It is a means to quantify uncertainty. What we have obtained is a 95% confidence interval of estimates, which means that about 95% of the time we will get it right. But that also leaves a 5% chance where we are totally off. Can we control the amount of uncertainty in our results? 


### Levels of Uncertainty: 80% and 99% Confidence Intervals

So far we have examined the 95% confidence interval. Let us see what happens to the interval of estimates when we increase our level confidence. We will examine a 99% confidence interval.

```{r}
desired_area <- 0.99
middle <- quantile(bstrap_medians, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle
```

```{r dpi=80,  fig.align="center", message = FALSE}
df <- tibble(bstrap_medians)
ggplot(df, aes(x = bstrap_medians, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 13) +
  geom_segment(aes(x = middle[1], y = 0, xend = middle[2], yend = 0), 
                   size = 2, color = "salmon")
```

The interval is much wider! It goes from about 10.9 yards per catch to 11.6. We observe a trade-off: as we increase our confidence in the interval of estimates, this is compensated by making the interval *wider*. That is, a 99% confidence interval has a chance of missing the parameter only 1% of the time now. 

Let us move in the other direction and try a 80% confidence interval. 

```{r}
desired_area <- 0.80
middle <- quantile(bstrap_medians, 0.5 + (desired_area / 2) * c(-1, 1), type = 1)
middle
```

```{r dpi=80,  fig.align="center", message = FALSE}
df <- tibble(bstrap_medians)
ggplot(df, aes(x = bstrap_medians, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", bins = 13) +
  geom_segment(aes(x = middle[1], y = 0, xend = middle[2], yend = 0), 
                   size = 2, color = "salmon")
```

This interval is *much* narrower than the 99% interval and only goes from 11 to 11.4 yards per catch. This is a much tighter set of estimates, but we traded a narrower interval for lower confidence. This interval has a chance of missing 20% of the time. 


### Confidence Intervals as an Hypothesis Test

Confidence intervals can be used for more than trying to estimate a population parameter. One popular use case for the confidence interval is something we saw in the previous chapter: the hypothesis test. 

Let us reconsider the 95% confidence interval we obtained. The average ratio goes from 10.98 yards per catch to 11.5 yards per catch. Suppose that the NCAA is interested in testing the following hypothesis about its players:  

__Null hypothesis.__ The average ratio of yards per catch of NCAA receivers is 12 yards per catch. 

__Alternative hypothesis.__ The average ratio of yards per catch of NCAA receivers is not 12 yards per catch. 

If we were testing this hypothesis at the 95% significance level, we would comfortably reject the null hypothesis. Why? 

A population ratio of 12 yards per catch does not sit on our 95% confidence interval for the population ratio. Therefore, at this level of significance, a value of 12 is not plausible.

If we were to lower our confidence (to say 90% or 80%), we may not have rejected the null hypothesis. This raises an important point about cut-offs: some fields demand a high level of significance for a result to be accepted by its scientific community; other fields may require much less convincing. For instance, [experimental studies](https://arxiv.org/pdf/0706.3283.pdf) in Physics demand significance levels at 99.9% [or even 99.99%](https://journals.aps.org/prd/pdf/10.1103/PhysRevD.93.122002?casa_token=LX4dEzEBwoEAAAAA%3ATEemb6ulsnfx89acCPP-GoEzMsjLDng26HX5YJ0CMZczkdOrPpmiHTLaFyUR3c6jZcuDYVgJys92eRU) for a result to be even considered publishable. It is not hard to imagine why: findings in Physics are usually axiomatic and rejecting a null hypothesis implies the discovery of phenomena in nature. A 99.99% confidence interval would guarantee that such a discovery is a fluke only 0.01% of the time. 

The basis for using confidence intervals as a hypothesis test is rooted in statistical theory. In practice, we simply check whether the value supplied by the null hypothesis sits on the confidence interval or not.  

### Final remarks: Use Bootstrap with Care 

We end this section with some points to keep in mind when applying bootstrapping. 

* Avoid introducing bias into the sample that is used as input for bootstrapping. The sampling plan of simple random sampling will usually work best.    
* When the size of a random sample is moderately sized enough, the chance of the bootstrapped sample being identical to it is extremely rare. Therefore, you should aim to work with large random samples. 
* TBA


<!--

Let us formalize the discussion a little.
Suppose we have a sample at hand with $N$ elements, which we will treat as the population.
We will sample $N$ elements from the $N$-element set with replacement.
The ratio between the number of ways to select the $N$ elements in an arbitrary order without replacement and the number with replacement is
\[
\frac{N}{N}\cdot \frac{N-1}{N} \cdot \cdots \cdot \frac{1}{N}
\]
This quantity is approximately $\sqrt{2\pi N}/e^{N}$ for any large enough $N$.
The value $e$ is what we call *Euler's constant* and is about 2.718, and so for a large enough $N$, the ratio is quite small.
Thus, the analysis guarantees that the samples we generate practically will not look like the original.

 Recall the birthday paradox we studied earlier.
There, we noticed that if 23 people are in a room, the chances are fifty-fifty that there are two people having the same birthdays among the 23.
The bootstrapping idea resembles a situation in which there are 365 people instead of 23.
There are already very good chances of having two people the same birthdays with just 23 people.
If there are 365 people, you can divide the 365 people into about 16 groups of 23, each having fifth-fifty chances of two people with identical birthdays, and there are possibilities that there are cross-group matches of birthdays.
Since there are 365 birthdays, duplication eliminates birthdays from the list of birthdays appearing. 

```{r}
nfl <- read_csv("data/nfl_receiving0.csv")
ncaa <- read_csv("data/ncaa-receivers-2005-2011.csv")
```

```{r}
yards_nfl <- nfl$Yds
yards_ncaa <- ncaa$Yards
```

```{r}
t_test <- function(x,y,c) {
  x1 <- sample(x, size = c, replace = TRUE)
  x2 <- sample(y, size = c, replace = TRUE)
  m1 <- mean(x1)
  m2 <- mean(x2)
  x1d <- x1 - m1
  x2d <- x2 - m2
  s1 <- x1d * x1d
  s2 <- x2d * x2d
  sd1 <- sqrt(sum(s1) / (c-1))
  sd2 <- sqrt(sum(s2) / (c-1))
  z <- (m1 - m2)/sqrt((sum(s1) + sum(s2))/(c*(c-1)))
  return(z)
}
```

```{r}
a <- t_test(yards_nfl,yards_ncaa,200)
a
```

```{r}
mean_nfl <- mean(yards_nfl)
mean_ncaa <- mean(yards_ncaa)
mean_nfl
mean_ncaa
max_nfl <- max(yards_nfl)
max_ncaa <- max(yards_ncaa)
max_nfl
max_ncaa
sd_nfl <- sd(yards_nfl)
sd_ncaa <- sd(yards_ncaa)
sd_nfl
sd_ncaa
z <- (mean_nfl - mean_ncaa)/sqrt(sd_nfl*sd_nfl/length(yards_nfl) + sd_ncaa*sd_ncaa/length(yards_ncaa))
z
```


In the previous chapter we began to develop ways of inferential thinking. In particular, we learned how to use data to decide between two hypotheses about the world. But often, instead of comparing hypotheses, we just want to know how big something is.

For example, in an earlier chapter we investigated how many warplanes the enemy might have. In a similar vein, in an election year, we might want to know what percent of voters favor a particular candidate. To assess the current economy, we might be interested in the median annual income of households in the United States.

In this chapter, we will develop a way to *estimate* an unknown *parameter*. Remember that a parameter is a numerical value associated with a population.

To figure out the value of a parameter, we need data. An implicit property of a parameter is that if we have the relevant data for the entire population, we can calculate the exact value of the parameter using the complete data available to us.

Quite often, the population is very large, so large that we cannot hope to have access to its entirety. For example, if it consists of all the households in the United States – then it might be too expensive and time-consuming to gather data from the entire population. In such situations, sampling comes in rescue - data scientists rely on sampling at random from the population.

This leads to a question of inference: How to make justifiable conclusions about the unknown parameter, based on the data in the random sample? We will answer this question by using inferential thinking.

A statistic based on a random sample can be a reasonable estimate of an unknown parameter in the population. For example, you might want to use the *median* annual income of sampled households as an estimate of the median annual income of all households in the U.S.

But the value of any statistic depends on the sample, and the sample is based on random draws. So every time data scientists come up with an estimate based on a random sample, they are faced with a question:

__"How different could this estimate have been, if the sample had come out differently?"__

In this chapter, you will learn one way of answering this question. The answer will give you the tools to estimate a numerical parameter and quantify the amount of error in your estimate.

We will start with a preliminary about *percentiles*. The most famous percentile is the median, often used in summaries of income data. Other percentiles will be important in the method of estimation that we are about to develop. So we will start by defining percentiles carefully.

## Percentiles

Numerical data can be sorted in increasing or decreasing order. Thus the values of a numerical data set have a *rank order*. A percentile is the value at a particular rank.

For example, if your score on a test is on the 95th percentile, a common interpretation is that only 5% of the scores were higher than yours. The median is the 50th percentile; it is commonly assumed that 50% the values in a data set are above the median.

But some care is required in giving percentiles a precise definition that works for all ranks and all lists. To see why, consider an extreme example where all the students in a class score 75 on a test. Then 75 is a natural candidate for the median, but it's not true that 50% of the scores are above 75. Also, 75 is an equally natural candidate for the 95th percentile or the 25th or any other percentile. Ties – that is, equal data values – have to be taken into account when defining percentiles.

You also have to be careful about exactly how far up the list to go when the relevant index isn't clear. For example, what should be the 87th percentile of a collection of 10 values? The 8th value of the sorted collection, or the 9th, or somewhere in between?

In this section, we will give a definition that works consistently for all ranks and all vectors.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

### A Numerical Example

Before giving a general definition of all percentiles, we will define the 80th percentile of a collection of values to be the smallest value in the collection that is at least as large as 80% of all of the values.

For example, let's consider the sizes of the five largest continents – Africa, Antarctica, Asia, North America, and South America – rounded to the nearest million square miles.

```{r}
sizes <- c(12, 17, 6, 9, 7)
```

The 80th percentile is the smallest value that is at least as large as 80% of the elements of `sizes`, that is, four-fifths of the five elements. That's 12:

```{r}
sort(sizes)
```

We can compute this with the `quantile()` function in R. 

### The `quantile` function

The `quantile()` function gives the value that cuts off the first *n* percent of the data values when it is sorted in ascending order. Observe what happens when we just pass the vector `sizes` as an argument. 

```{r}
quantile(sizes)
```

The corresponding 0th, 25th, 50th, 75th, and 100th percentiles of the vector are returned. Since these percentiles partition the data into quarters, these are given a special name called *quartiles*. 

There are many ways to compute percentiles (see the help page for a sneak peek, with `?quantile`). The one that matches the definition used here corresponds to `type = 1`. Note that for computing quartiles, as we did before, the answer remains the same.     

```{r}
quantile(sizes, type = 1)
```

The 80th percentile is a value on the list, namely 12. We can compute this by passing in an additional parameter with the desired percentage ratios. These must be between 0 and 1. This is how  `quantile()` gets its name: *quantiles* are *percentiles* scaled to have a value between 0 and 1, e.g. `0.7` rather than `70`, `0.8` rather than `80`. 

```{r}
quantile(sizes, c(0.8), type = 1)
```

You can see that 80% of the values are less than or equal to it, and that it is the smallest value on the list for which this is true.

Analogously, the 70th percentile is the smallest value in the collection that is at least as large as 70% of the elements of `sizes`. Now 70% of 5 elements is "3.5 elements", so the 70th percentile is the 4th element on the list. That's 12, the same as the 80th percentile for these data.

```{r}
quantile(sizes, c(0.7), type = 1)
```

### The General Definition

Let $p$ be a number between 0 and 100. The $p$th percentile of a collection is the smallest value in the collection that is at least as large as p% of all the values.

By this definition, any percentile between 0 and 100 can be computed for any collection of values, and it is always an element of the collection.

In practical terms, suppose there are $n$ elements in the collection. To find the  $p$th percentile:

* Sort the collection in increasing order.
* Find p% of n:  ($p$/100)×$n$ . Call that $k$.
* If $k$ is an integer, take the $k$th element of the sorted collection.
* If $k$ is not an integer, round it up to the next integer, and take that element of the sorted collection.

### Example

The table scores_and_sections contains one row for each student in a class of 359 students. The columns are the student's discussion section and midterm score.

```{r message = FALSE, warning = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/scores_by_section.csv"
scores_and_sections <- read_csv(url)
scores_and_sections
```

```{r}
scores_and_sections %>%
  select(Midterm) %>%
  ggplot(aes(x = Midterm, y = ..density..)) +
  geom_histogram(color = "gray", breaks = seq(-0.5, 25.6, 1))
``` 

What was the 85th percentile of the scores? To use the `quantile()` function, create a vector `scores` containing the midterm scores, and find the 85th percentile:

```{r}
scores <- pull(scores_and_sections, Midterm)
quantile(scores, c(0.85), type = 1)
```

According to the percentile function, the 85th percentile was 22. To check that this is consistent with our new definition, let's apply the definition directly.

First, put the scores in increasing order:

```{r}
sorted_scores <- sort(pull(scores_and_sections, Midterm))
```

There are 359 scores in the array. So next, find 85% of 359, which is 305.15.

```{r}
0.85 * 359
```

That's not an integer. By our definition, the 85th percentile is the 306th element of `sorted_scores`.

```{r}
# The 306th element of the sorted array

sorted_scores[306]
```

That's the same as the answer we received by using `quantile()`. In future, we will just use `quantile()`.

### Quartiles

We mentioned quartiles briefly earlier. The *first quartile* of a numercial collection is the 25th percentile. The terminology arises from the *first quarter*. The second quartile is the median, and the third quartile is the 75th percentile.

For our `scores` data, those values are:

```{r}
quantile(scores, c(0.25), type = 1)
```

```{r}
quantile(scores, c(0.5), type = 1)
```

```{r}
quantile(scores, c(0.75), type = 1)
```

We saw that when omitting the percentage ratios entirely, the default behavior reports the quartiles automatically. 

```{r}
quantile(scores, type = 1)
```

Distributions of scores are sometimes summarized by the "middle 50%" interval, between the first and third quartiles.

## The Bootstrap

A data scientist is using the data in a random sample to estimate an unknown parameter. She uses the sample to calculate the value of a statistic that she will use as her estimate.

Once she has calculated the observed value of her statistic, she could just present it as her estimate and go on her merry way. But she's a data scientist. She knows that her random sample is just one of numerous possible random samples, and thus her estimate is just one of numerous plausible estimates.

By how much could those estimates vary? To answer this, it appears as though she needs to draw another sample from the population, and compute a new estimate based on the new sample. But she doesn't have the resources to go back to the population and draw another sample.

It looks as though the data scientist is stuck.

Fortunately, a brilliant idea called the *bootstrap* can help her out. Since it is not feasible to generate new samples from the population, the bootstrap generates new random samples by a method called *resampling*: the new samples are drawn at random *from the original sample*.

In this section, we will see how and why the bootstrap works. In the rest of the chapter, we will use the bootstrap for inference.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

### Employee Compensation in the City of San Francisco

[SF OpenData](https://data.sfgov.org/) is a website where the City and County of San Francisco make some of their data publicly available. One of the data sets contains compensation data for employees of the City. These include medical professionals at City-run hospitals, police officers, fire fighters, transportation workers, elected officials, and all other employees of the City.

Compensation data for the calendar year 2015 are in the table `sf2015`.

```{r message = FALSE, warning = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/san_francisco_2015.csv"
sf2015 <- read_csv(url)
sf2015
```

There is one row for each of 42,979 employees. There are numerous columns containing information about City departmental affiliation and details of the different parts of the employee's compensation package. Here is the row corresponding to the late Edward Lee, the Mayor at that time.

```{r}
filter(sf2015, Job == "Mayor")
```

We are going to study the final column, `Total Compensation`. That's the employee's salary plus the City's contribution towards his/her retirement and benefit plans.

Financial packages in a calendar year can sometimes be hard to understand as they depend on the date of hire, whether the employee is changing jobs within the City, and so on. For example, the lowest values in the `Total Compensation` column look a little strange.

```{r}
sf2015 %>%
  arrange(`Total Compensation`) %>%
  select(`Total Compensation`, everything())
```

For clarity of comparison, we will focus our attention on those who had at least the equivalent of a half-time job for the whole year. At a minimum wage of about \$10 per hour, and 20 hours per week for 52 weeks, that's a salary of about \$10,000.

```{r}
sf2015 <- filter(sf2015, Salaries > 10000)
nrow(sf2015)
```

### Population and Parameter 

Let this table of just over 36,500 rows be our population. Here is a histogram of the total compensations.

```{r}
sf_bins <- seq(0, 700000, 25000)
sf2015 %>%
  select(`Total Compensation`) %>%
  ggplot(aes(x = `Total Compensation`, y = ..density..)) + 
  geom_histogram(breaks = sf_bins, color = "gray")
```

While most of the values are below $300,000, a few are quite a bit higher. For example, the total compensation of the Chief Investment Officer was almost \$650,000. That is why the horizontal axis stretches to \$700,000.

```{r}
arrange(sf2015, desc(`Total Compensation`)) %>%
  select(`Total Compensation`, everything()) %>%
  head(n = 2)
```

Now let the parameter be the median of the total compensations.

Since we have the luxury of having all of the data from the population, we can simply calculate the parameter:

```{r}
pop_median <- quantile(pull(sf2015, `Total Compensation`), c(0.50), type = 1)
pop_median
```

The median total compensation of all employees was just over $110,300.

From a practical perspective, there is no reason for us to draw a sample to estimate this parameter since we simply know its value. But in this section we are going to pretend we don't know the value, and see how well we can estimate it based on a random sample.

In later sections, we will come down to earth and work in situations where the parameter is unknown. For now, we are all-knowing.

### A Random Sample and an Estimate

Let us draw a sample of 500 employees at random without replacement, and let the median total compensation of the sampled employees serve as our estimate of the parameter.

```{r}
our_sample <- slice_sample(sf2015, n = 500)
our_sample %>%
  select(`Total Compensation`) %>%
  ggplot(aes(x = `Total Compensation`, y = ..density..)) + 
  geom_histogram(breaks = sf_bins, color = "gray")
```

```{r}
est_median <- quantile(pull(our_sample, `Total Compensation`), c(0.50), 
                       type = 1)
est_median
```

The sample size is large. By the law of averages, the distribution of the sample resembles that of the population, and consequently the sample median is not very far from the population median (though of course it is not exactly the same).

So now we have one estimate of the parameter. But had the sample come out differently, the estimate would have had a different value. We would like to be able to quantify the amount by which the estimate could vary across samples. That measure of variability will help us measure how accurately we can estimate the parameter.

To see how different the estimate would be if the sample had come out differently, we could just draw another sample from the population, but that would be cheating. We are trying to mimic real life, in which we won't have all the population data at hand.

Somehow, we have to get another random sample without sampling from the population.

### The Bootstrap: Resampling from the Sample

What we do have is a large random sample from the population. As we know, a large random sample is likely to resemble the population from which it is drawn. This observation allows data scientists to *lift themselves up by their own bootstraps*: the sampling procedure can be replicated by *sampling from the sample*.

Here are the steps of *the bootstrap method* for generating another random sample that resembles the population:

* __Treat the original sample as if it were the population.__
* __Draw from the sample__, at random __with__ replacement, __the same number of times as the original sample size__.

It is important to resample the same number of times as the original sample size. The reason is that the variability of an estimate depends on the size of the sample. Since our original sample consisted of 500 employees, our sample median was based on 500 values. To see how different the sample could have been, we have to compare it to the median of other samples of size 500.

If we drew 500 times at random *without* replacement from our sample of size 500, we would just get the same sample back. By drawing *with* replacement, we create the possibility for the new samples to be different from the original, because some employees might be drawn more than once and others not at all.

Why is this a good idea? By the law of averages, the distribution of the original sample is likely to resemble the population, and the distributions of all the "resamples" are likely to resemble the original sample. So the distributions of all the resamples are likely to resemble the population as well.

```{r, echo=FALSE, fig.align="center", out.width='50%'}
knitr::include_graphics('images/bootstrap.png')
```

### A Resampled Median

Here is one new sample drawn from the original sample, and the corresponding sample median. Note how we set `n` to be the total number of rows from `our_sample` and that `replace` is set to `TRUE` to toggle sampling with replacement.  

```{r dpi=80,  fig.align="center", message = FALSE}
resample_1 <- slice_sample(our_sample, n = nrow(our_sample), replace = TRUE)
resample_1 %>%
  select(`Total Compensation`) %>%
  ggplot(aes(x = `Total Compensation`, y = ..density..)) + 
  geom_histogram(breaks = sf_bins, color = "gray")
```

```{r}
resampled_median_1 <- quantile(pull(resample_1, `Total Compensation`), 
                               c(0.50), type = 1)
resampled_median_1
```

By resampling, we have another estimate of the population median. By resampling again and again, we will get many such estimates, and hence an empirical distribution of the estimates.

```{r}
resample_2 <- slice_sample(our_sample, n = nrow(our_sample), replace = TRUE)
resampled_median_2 <- quantile(pull(resample_2, `Total Compensation`), 
                               c(0.50), type = 1)
resampled_median_2
```


### Bootstrap Empirical Distribution of the Sample Median

Let us define a function `bootstrap_median()` that takes our original sample, the label of the column containing the variable, and the number of bootstrap samples we want to take, and returns an array of the corresponding resampled medians.

Each time we resample and find the median, we *replicate* the bootstrap process. So the number of bootstrap samples will be called the number of replications.

```{r}
one_bootstrap <- function(one_column_df, x) {
  bootstrap_sample <- slice_sample(one_column_df, 
                                 n = nrow(one_column_df), replace = TRUE)
  resampled_median <- quantile(pull(bootstrap_sample, 1), c(0.50), type = 1)
}
```

```{r}
bootstrap_median <- function(original_sample, label, replications, x) {
  just_one_column <- select(original_sample, {{ label }})
  medians <- map_dbl(.x = 1:replications, 
                     .f = one_bootstrap,
                     one_column_df = just_one_column)
  return(medians)
}
```

We now replicate the bootstrap process 5,000 times. The array bstrap_medians contains the medians of all 5,000 bootstrap samples. Notice that the code takes longer to run than our previous code. It has a lot of resampling to do!

```{r}
bstrap_medians <- bootstrap_median(our_sample, `Total Compensation`, 5000)
```

Here is the histogram of the 5000 medians. The red dot is the population parameter: it is the median of the entire population, which we happen to know but did not use in the bootstrap process.

```{r dpi=80,  fig.align="center", message = FALSE}
resampled_medians <- tibble(bstrap_medians)
ggplot(resampled_medians) +
  geom_histogram(aes(x = bstrap_medians, y = ..density..), 
                 bins = 10, color = "gray") + 
  geom_point(aes(x = pop_median, y = 0), color = "red", size = 3)
```

It is important to remember that the red dot is fixed: it is $110,305.79, the population median. The empirical histogram is the result of random draws, and will be situated randomly relative to the red dot.

Remember also that the point of all these computations is to estimate the population median, which is the red dot. Our estimates are all the randomly generated sampled medians whose histogram you see above. We want those estimates to contain the parameter – it they don't, then they are off.

### Do the Estimates Capture the Parameter?

How often does the empirical histogram of the resampled medians sit firmly over the red dot, and not just brush the dot with its tails? To answer this, we must define "sit firmly". Let's take that to mean "the middle 95% of the resampled medians contains the red dot".

Here are the two ends of the "middle 95%" interval of resampled medians:

```{r}
left <- quantile(bstrap_medians, c(0.025), type = 1)
left
```

```{r}
right <- quantile(bstrap_medians, c(0.975), type = 1)
right
```

The population median of $110,305 is between these two numbers. The interval and the population median are shown on the histogram below.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(resampled_medians) +
  geom_histogram(aes(x = bstrap_medians, y = ..density..), 
                 bins = 10, color = "gray", fill = "darkcyan") + 
  geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
               size = 2, color = "salmon") +
  geom_point(aes(x = pop_median, y = 0), color = "red", size = 3)
```
The "middle 95%" interval of estimates captured the parameter in our example. But was that a fluke?

To see how frequently the interval contains the parameter, we have to run the entire process over and over again. Specifically, we will repeat the following process 100 times:

* Draw an original sample of size 500 from the population.
* Carry out 5,000 replications of the bootstrap process and generate the "middle 95%" interval of resampled medians.

We will end up with 100 intervals, and count how many of them contain the population median.

__Spoiler alert:__ The statistical theory of the bootstrap says that the number should be around 95. It may be in the low 90s or high 90s, but not much farther off 95 than that.

```{r}
mega_bootstrap_simulation <- function(x) {
  our_sample <- slice_sample(sf2015, n = 500)
  bstrap_medians <- bootstrap_median(our_sample, `Total Compensation`, 5000)
  left <- quantile(bstrap_medians, c(0.025), type = 1)
  right <- quantile(bstrap_medians, c(0.975), type = 1)
  return(c(left, right))
}
```

For each of the 100 replications, we get one interval of estimates of the median.

```{r}
# THE MEGA SIMULATION: This one takes several minutes.

# Generate 100 intervals
intervals <- map(1:100, mega_bootstrap_simulation)
```

For each of the 100 replications, we get one interval of estimates of the median.

```{r}
intervals[1]
intervals[2]
```

Let's transform `intervals` into a data frame which will make it easier to understand and visualize the results. 

```{r}
left_column = map_dbl(intervals, function(x) x[1])
right_column = map_dbl(intervals, function(x) x[2])
```

```{r}
interval_df <- tibble(
  replication = 1:100,
  left = left_column,
  right = right_column
)
interval_df
```

The good intervals are those that contain the parameter we are trying to estimate. Typically the parameter is unknown, but in this section we happen to know what the parameter is.

```{r}
pop_median
```

How many of the 100 intervals contain the population median? That's the number of intervals where the left end is below the population median and the right end is above.

```{r}
nrow(filter(interval_df, left <= pop_median & right >= pop_median))
```

It takes a few minutes to construct all the intervals, but try it again if you have the patience. Most likely, about 95 of the 100 intervals will be good ones: they will contain the parameter.

It's hard to show you all the intervals on the horizontal axis as they have large overlaps – after all, they are all trying to estimate the same parameter. The graphic below shows each interval on the same axes by stacking them vertically. The vertical axis is simply the number of the replication from which the interval was generated.

The red line is where the parameter is. Good intervals cover the parameter; there are about 95 of these, typically.

If an interval doesn't cover the parameter, it's a dud. The duds are the ones where you can see "daylight" around the red line. There are very few of them – about 5, typically – but they do happen.

Any method based on sampling has the possibility of being off. The beauty of methods based on random sampling is that we can quantify how often they are likely to be off.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(interval_df) + 
  geom_segment(aes(x = left, y = replication, 
                   xend = right, yend = replication), color = "salmon") +
  geom_vline(xintercept = pop_median, color = "red") + 
  xlab("Median (dollars)")
```

To summarize what the simulation shows, suppose you are estimating the population median by the following process:

* Draw a large random sample from the population.
* Bootstrap your random sample and get an estimate from the new random sample.
* Repeat the above step thousands of times, and get thousands of estimates.
* Pick off the "middle 95%" interval of all the estimates.

That gives you one interval of estimates. Now if you repeat __the entire process__ 100 times, ending up with 100 intervals, then about 95 of those 100 intervals will contain the population parameter.

In other words, this process of estimation captures the parameter about 95% of the time.

You can replace 95% by a different value, as long as it's not 100. Suppose you replace 95% by 80% and keep the sample size fixed at 500. Then your intervals of estimates will be shorter than those we simulated here, because the "middle 80%" is a smaller range than the "middle 95%". Only about 80% of your intervals will contain the parameter.

## Confidence Intervals

We have developed a method for estimating a parameter by using random sampling and the bootstrap. Our method produces an interval of estimates, to account for chance variability in the random sample. By providing an interval of estimates instead of just one estimate, we give ourselves some wiggle room.

In the previous example we saw that our process of estimation produced a good interval about 95% of the time, a "good" interval being one that contains the parameter. We say that we are 95% confident that the process results in a good interval. Our interval of estimates is called a 95% confidence interval for the parameter, and 95% is called the confidence level of the interval.

The situation in the previous example was a bit unusual. Because we happened to know the value of the parameter, we were able to check whether an interval was good or a dud, and this in turn helped us to see that our process of estimation captured the parameter about 95 out of every 100 times we used it.

But usually, data scientists don't know the value of the parameter. That is the reason they want to estimate it in the first place. In such situations, they provide an interval of estimates for the unknown parameter by using methods like the one we have developed. Because of statistical theory and demonstrations like the one we have seen, data scientists can be confident that their process of generating the interval results in a good interval a known percent of the time.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```


### Confidence Interval for a Population Median: Bootstrap Percentile Method

We will now use the bootstrap method to estimate an unknown population median. The data come from a sample of newborns in a large hospital system; we will treat it as if it were a simple random sample though the sampling was done in multiple stages. [Stat Labs](https://www.stat.berkeley.edu/~statlabs/) by Deborah Nolan and Terry Speed has details about a larger dataset from which this set is drawn.

The data frame `baby` contains the following variables for mother-baby pairs: the baby's birth weight in ounces, the number of gestational days, the mother's age in completed years, the mother's height in inches, pregnancy weight in pounds, and whether or not the mother smoked during pregnancy.

```{r}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
baby
```

Birth weight is an important factor in the health of a newborn infant – smaller babies tend to need more medical care in their first days than larger newborns. It is therefore helpful to have an estimate of birth weight before the baby is born. One way to do this is to examine the relationship between birth weight and the number of gestational days.

A simple measure of this relationship is the ratio of birth weight to the number of gestational days. The data frame `ratios` contains the first two columns of `baby`, as well as a column of the ratios. The first entry in that column was calculated as follows:

\[\frac{120 \text{ ounces}}{284 \text{ days}} \approx 0.4225 \text{ ounces per day} \]

```{r}
ratios <- baby %>%
  transmute(birth_weight = `Birth Weight`,
            gest_days = `Gestational Days`,
            ratio = birth_weight / gest_days)
ratios
```

Here is a histogram of the ratios.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(ratios) + 
  geom_histogram(aes(x = ratio, y = ..density..), 
                 color = "gray", fill = "darkcyan")
```

At first glance the histogram looks quite symmetric, with the density at its maximum over the interval 4 ounces per day to 4.5 ounces per day. But a closer look reveals that some of the ratios were quite large by comparison. The maximum value of the ratios was just over 0.78 ounces per day, almost double the typical value.

```{r}
head(arrange(ratios, desc(ratio)), n = 1)
```

The median gives a sense of the typical ratio because it is unaffected by the very large or very small ratios. The median ratio in the sample is about 0.429 ounces per day.

```{r}
median(pull(ratios, ratio))
```

But what was the median in the population? We don't know, so we will estimate it.

Our method will be exactly the same as in the previous section. We will bootstrap the sample 5,000 times resulting in 5,000 estimates of the median. Our 95% confidence interval will be the "middle 95%" of all of our estimates.

Recall the functions `one_bootstrap` and `bootstrap_median` defined in the previous section. We will call this function and construct a 95% confidence interval for the median ratio in the population. Remember that the data frame `ratios` contains the relevant data from our original sample.

```{r}
one_bootstrap <- function(x, one_column_df, statistic) {
  bootstrap_sample <- slice_sample(one_column_df, 
                                 n = nrow(one_column_df), replace = TRUE)
  resampled_median <- statistic(pull(bootstrap_sample, 1))
}
```

```{r}
bootstrap <- function(x, original_sample, stat, label) {
  just_one_column <- select(original_sample, {{ label }})
  estimates <- map_dbl(.x = x,
                   .f = one_bootstrap,
                   one_column_df = just_one_column,
                   statistic = stat)
  return(estimates)
}
```

```{r}
# Generate the medians from 5000 bootstrap samples
bstrap_medians <- bootstrap(1:5000, ratios, median, ratio)
```

```{r}
# Get the endpoints of the 95% confidence interval
left <- quantile(bstrap_medians, c(0.025), type = 1)
right <- quantile(bstrap_medians, c(0.975), type = 1)
c(left, right)
```

The 95% confidence interval goes from about 0.425 ounces per day to about 0.433 ounces per day. We are estimating the median "birth weight to gestational days" ratio in the population is somewhere in the interval 0.425 ounces per day to 0.433 ounces per day.

The estimate of 0.429 based on the original sample happens to be exactly half-way in between the two ends of the interval, though that need not be true in general.

To visualize our results, let us draw the empirical histogram of our bootstrapped medians and place the confidence interval on the horizontal axis.

```{r dpi=80, fig.align="center", message = FALSE}
resampled_medians <- tibble(bstrap_medians)

ggplot(resampled_medians) +
  geom_histogram(aes(x = bstrap_medians, y = ..density..), 
                 bins = 15, color = "gray", fill = "darkcyan") + 
  geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
               size = 2, color = "salmon") 
```

This histogram and interval resembles those we drew in the previous section, with one big difference – there is no red dot showing where the parameter is. We don't know where that dot should be, or whether it is even in the interval.

We just have an interval of estimates. It is a 95% confidence interval of estimates, because the process that generates it produces a good interval about 95% of the time. That certainly beats guessing at random!

Keep in mind that this interval is an approximate 95% confidence interval. There are many approximations involved in its computation. The approximation is not bad, but it is not exact.

### Confidence Interval for a Population Mean: Bootstrap Percentile Method

What we have done for medians can be done for means as well. Suppose we want to estimate the average age of the mothers in the population. A natural estimate is the average age of the mothers in the sample. Here is the distribution of their ages, and their average age which was about 27.2 years.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby) + 
  geom_histogram(aes(x = `Maternal Age`, y = ..density..),
                 color = "gray", fill = "darkcyan")
```

```{r}
mean(pull(baby, `Maternal Age`))
```

What was the average age of the mothers in the population? We don't know the value of this parameter.

Let's estimate the unknown parameter by the bootstrap method. To do this, all we need to do is change the statistic functional passed to the `bootstrap` function from `median` to `mean`. 

```{r}
# Generate the means from 5000 bootstrap samples
bstrap_means <- bootstrap(1:5000, baby, mean, `Maternal Age`)
```

```{r}
# Get the endpoints of the 95% confidence interval
left <- quantile(bstrap_means, c(0.025), type = 1)
right <- quantile(bstrap_means, c(0.975), type = 1)
c(left, right)
```

The 95% confidence interval goes from about 26.9 years to about 27.6 years. That is, we are estimating that the average age of the mothers in the population is somewhere in the interval 26.9 years to 27.6 years.

Notice how close the two ends are to the average of about 27.2 years in the original sample. The sample size is very large – 1,174 mothers – and so the sample averages don't vary much. We will explore this observation further in the next chapter.

The empirical histogram of the 5,000 bootstrapped means is shown below, along with the 95% confidence interval for the population mean.

```{r dpi=80,  fig.align="center", message = FALSE}
tibble(bstrap_means) %>%
ggplot() +
  geom_histogram(aes(x = bstrap_means, y = ..density..), 
                 bins = 15, color = "gray", fill = "darkcyan") + 
  geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
               size = 2, color = "salmon") 
```

Once again, the average of the original sample (27.23 years) is close to the center of the interval. That's not very surprising, because each bootstrapped sample is drawn from that same original sample. The averages of the bootstrapped samples are about symmetrically distributed on either side of the average of the sample from which they were drawn.

Notice also that the empirical histogram of the resampled means has roughly a symmetric bell shape, even though the histogram of the sampled ages was not symmetric at all:

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(baby) +
  geom_histogram(aes(x = `Maternal Age`, y = ..density..), 
                 color = "gray", fill = "darkcyan")
```

This is a consequence of the Central Limit Theorem of probability and statistics. In later sections, we will see what the theorem says.

### An 80% Confidence Interval 

You can use the bootstrapped sample means to construct an interval of any level of confidence. For example, to construct an 80% confidence interval for the mean age in the population, you would take the "middle 80%" of the resampled means. So you would want 10% of the distribution in each of the two tails, and hence the endpoints would be the 10th and 90th percentiles of the resampled means.

```{r}
left_80 <- quantile(bstrap_means, c(0.1), type = 1)
right_80 <- quantile(bstrap_means, c(0.9), type = 1)
c(left_80, right_80)
```

```{r dpi=80,  fig.align="center", message = FALSE}
tibble(bstrap_means) %>%
ggplot() +
  geom_histogram(aes(x = bstrap_means, y = ..density..), 
                 bins = 15, color = "gray", fill = "darkcyan") + 
  geom_segment(aes(x = left_80, y = 0, xend = right_80, yend = 0), 
               size = 2, color = "salmon") 
```

This 80% confidence interval is much shorter than the 95% confidence interval. It only goes from about 27.0 years to about 27.4 years. While that's a tight set of estimates, you know that this process only produces a good interval about 80% of the time.

The earlier process produced a wider interval but we had more confidence in the process that generated it.

To get a narrow confidence interval at a high level of confidence, you'll have to start with a larger sample. We'll see why in the next chapter.

### Confidence Interval for a Population Proportion: Bootstrap Percentile Method

In the sample, 39% of the mothers smoked during pregnancy.

```{r}
nrow(filter(baby, `Maternal Smoker` == TRUE)) / nrow(baby)
```

For what follows, it is useful to observe that this proportion can also be calculated by an array operation. Recall that `sum` will count the number of `TRUE`s present in the column.  

```{r}
maternal_smoker_column <- pull(baby, `Maternal Smoker`)
sum(maternal_smoker_column) / length(maternal_smoker_column)
```

What percent of mothers in the population smoked during pregnancy? This is an unknown parameter which we can estimate by a bootstrap confidence interval. It turns out that the bootstrap functions we wrote earlier are perfect for computing this! 

As before, the only change in computation needed is tweaking the statistic functional that is passed to `bootstrap()`. Estimating the population mean and median was easy: the functions were written for us! For estimating the proportion of smokers, we will need to define our own functional and pass that in instead. 

We have a good idea of what this functional should look like: we will simply replace `maternal_smoker_column` with a more general `x`. 

```{r}
proportion <- function(x) sum(x) / length(x)
```

We are now ready to apply `bootstrap` again to construct an approximate 95% confidence interval for the percent of smokers among the mothers in the population.

```{r}
# Generate the proportions from 5000 bootstrap samples
bstrap_props <- bootstrap(1:5000, baby, proportion, `Maternal Smoker`)
```

```{r}
# Get the endpoints of the 95% confidence interval
left <- quantile(bstrap_props, c(0.025), type = 1)
right <- quantile(bstrap_props, c(0.975), type = 1)
c(left, right)
```

The confidence interval goes from about 36% to about 42%. The original sample percent of 39% is very close to the center of the interval, as you can see below.

```{r}
tibble(bstrap_props) %>%
ggplot() +
  geom_histogram(aes(x = bstrap_props, y = ..density..), 
                 bins = 15, color = "gray", fill = "darkcyan") + 
  geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
               size = 2, color = "salmon") 
```


### Care in Using the Bootstrap

The bootstrap is an elegant and powerful method. Before using it, it is important to keep some points in mind.

* Start with a large random sample. If you don't, the method might not work. Its success is based on large random samples (and hence also resamples from the sample) resembling the population. The Law of Averages says that this is likely to be true provided the random sample is large.

* To approximate the probability distribution of a statistic, it is a good idea to replicate the resampling procedure as many times as possible. A few thousand replications will result in decent approximations to the distribution of sample median, especially if the distribution of the population has one peak and is not very asymmetric. We used 5,000 replications in our examples but would recommend 10,000 in general.

* The bootstrap percentile method works well for estimating the population median or mean based on a large random sample. However, it has limitations, as do all methods of estimation. For example, it is not expected to do well in the following situations.

  * The goal is to estimate the minimum or maximum value in the population, 
    or a very low or very high percentile, or parameters that are greatly 
    influenced by rare elements of the population.
    
  * The probability distribution of the statistic is not roughly bell shaped.
  * The original sample is very small, say less than 10 or 15.

## Using Confidence Intervals

A confidence interval has a single purpose – to estimate an unknown parameter based on data in a random sample. 

### Prerequisites

We will continue to make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

Recall also the bootstrap functions we wrote in the previous section. 

```{r}
one_bootstrap <- function(x, one_column_df, statistic) {
  bootstrap_sample <- slice_sample(one_column_df, 
                                 n = nrow(one_column_df), replace = TRUE)
  resampled_median <- statistic(pull(bootstrap_sample, 1))
}
```

```{r}
bootstrap <- function(x, original_sample, stat, label) {
  just_one_column <- select(original_sample, {{ label }})
  estimates <- map_dbl(.x = x,
                   .f = one_bootstrap,
                   one_column_df = just_one_column,
                   statistic = stat)
  return(estimates)
}
```

### Using Confidence Intervals

In the last section, we said that the interval (36%, 42%) was an approximate 95% confidence interval for the percent of smokers among mothers in the population. That was a formal way of saying that by our estimate, the percent of smokers among the mothers in the population was somewhere between 36% and 42%, and that our process of estimation is correct about 95% of the time.

It is important to resist the impulse to use confidence intervals for other purposes. For example, recall that we calculated the interval (26.9 years, 27.6 years) as an approximate 95% confidence interval for the average age of mothers in the population. A dismayingly common misuse of the interval is to conclude that about 95% of the women were between 26.9 years and 27.6 years old. You don't need to know much about confidence intervals to see that this can't be right – you wouldn't expect 95% of mothers to all be within a few months of each other in age. Indeed, the histogram of the sampled ages shows quite a bit of variation.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(baby) + 
  geom_histogram(aes(x = `Maternal Age`, y = ..density..), 
                 color = "gray", fill = "darkcyan")
```

A small percent of the sampled ages are in the (26.9, 27.6) interval, and you would expect a similar small percent in the population. The interval just estimates one number: the average of all the ages in the population.

However, estimating a parameter by confidence intervals does have an important use besides just telling us roughly how big the parameter is.

### Using a Confidence Interval to Test Hypotheses

Our approximate 95% confidence interval for the average age in the population goes from 26.9 years to 27.6 years. Suppose someone wants to test the following hypotheses:

__Null hypothesis.__ The average age in the population is 30 years.

__Alternative hypothesis.__ The average age in the population is not 30 years.

Then, if you were using the 5% cutoff for the P-value, you would reject the null hypothesis. This is because 30 is not in the 95% confidence interval for the population average. At the 5% level of significance, 30 is not a plausible value for the population average.

This use of confidence intervals is the result of a *duality* between confidence intervals and tests: if you are testing whether or not the population mean is a particular value x, and you use the 5% cutoff for the P-value, then you will reject the null hypothesis if x is not in your 95% confidence interval for the mean.

This can be established by statistical theory. In practice, it just boils down to checking whether or not the value specified in the null hypothesis lies in the confidence interval.

If you were using the 1% cutoff for the P-value, you would have to check if the value specified in the null hypothesis lies in a 99% confidence interval for the population mean.

To a rough approximation, these statements are also true for population proportions, provided the sample is large.

While we now have a way of using confidence intervals to test a particular kind of hypothesis, you might wonder about the value of testing whether or not the average age in a population is equal to 30. Indeed, the value isn't clear. But there are some situations in which a test of this kind of hypothesis is both natural and useful.

We will study this in the context of data that are a subset of the information gathered in a randomized controlled trial about treatments for Hodgkin's disease. Hodgkin's disease is a cancer that typically affects young people. The disease is curable but the treatment can be very harsh. The purpose of the trial was to come up with dosage that would cure the cancer but minimize the adverse effects on the patients.

This data frame `hodgkins` contains data on the effect that the treatment had on the lungs of 22 patients. The columns are:

* Height in cm
* A measure of radiation to the mantle (neck, chest, under arms)
* A measure of chemotherapy
* A score of the health of the lungs at baseline, that is, at the start of the treatment; higher scores correspond to more healthy lungs
* The same score of the health of the lungs, 15 months after treatment

```{r}
path <- "data/hodgkins.csv"
hodgkins <- read_csv(path)
hodgkins
```

We will compare the baseline and 15-month scores. As each row corresponds to one patient, we say that the sample of baseline scores and the sample of 15-month scores are paired - they are not just two sets of 22 values each, but 22 pairs of values, one for each patient.

At a glance, you can see that the 15-month scores tend to be lower than the baseline scores – the sampled patients' lungs seem to be doing worse 15 months after the treatment. This is confirmed by the mostly positive values in the column drop, the amount by which the score dropped from baseline to 15 months.

```{r}
hodgkins <- hodgkins %>%
  mutate(drop = base - month15)
hodgkins
```

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(hodgkins) + 
  geom_histogram(aes(x = drop, y = ..density..), 
                 breaks = seq(-20, 81, 20), 
                 color = "gray", fill = "darkcyan")
```

```{r}
mean(pull(hodgkins, drop))
```

But could this be the result of chance variation? It really doesn't seem so, but the data are from a random sample. Could it be that in the entire population of patients, the average drop is just 0?

To answer this, we can set up two hypotheses:

__Null hypothesis.__ In the population, the average drop is 0.

__Alternative hypothesis.__ In the population, the average drop is not 0.

To test this hypothesis with a 1% cutoff for the P-value, let's construct an approximate 99% confidence interval for the average drop in the population.

```{r}
bstrap_means <- bootstrap(1:10000, hodgkins, mean, drop)

left <- quantile(bstrap_means, c(0.005), type = 1)
right <- quantile(bstrap_means, c(0.995), type = 1)

c(left, right)
```

```{r dpi=80,  fig.align="center", message = FALSE}
tibble(bstrap_means) %>%
  ggplot() +
  geom_histogram(aes(x = bstrap_means, y = ..density..),
                 color = "gray", fill = "darkcyan") + 
  geom_segment(aes(x = left, y = 0, xend = right, yend = 0), 
               size = 2, color = "salmon") 
```

The 99% confidence interval for the average drop in the population goes from about 17 to about 40. The interval doesn't contain 0. So we reject the null hypothesis.

But notice that we have done better than simply concluding that the average drop in the population isn't 0. We have estimated how big the average drop is. That's a more useful result than just saying, "It's not 0."

__A note on accuracy.__ Our confidence interval is quite wide, for two main reasons:

* The confidence level is high (99%).
* The sample size is relatively small compared to those in our earlier 
  examples.
  
In the next chapter, we will examine how the sample size affects accuracy. We will also examine how the empirical distributions of sample means so often come out bell shaped even though the distributions of the underlying data are not bell shaped at all.

### Endnote

The terminology of a field usually comes from the leading researchers in that field. [Brad Efron](https://en.wikipedia.org/wiki/Bradley_Efron), who first proposed the bootstrap technique, used a term that has [American origins](https://en.wikipedia.org/wiki/Bootstrapping). Not to be outdone, Chinese statisticians have [proposed their own method](http://econpapers.repec.org/article/eeestapro/v_3a37_3ay_3a1998_3ai_3a4_3ap_3a321-329.htm).




-->
