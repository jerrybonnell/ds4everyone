# (PART) Introduction {-} 

# What Is Data Science?

Data science is a field of science that aims at discovering patterns and extracting knowledge through analysis of data and developing methods to support such investigation. The field is rooted in mathematics, statistics, computer science, and vision science and has contributed to a wide array of disciplines, e.g.,  biology, chemistry, economics, literary studies, medicine, physical sciences, and psychology. 

What does data science look like? Here is a short list of some examples, followed by a discipline that appears most relevant to the problem. 

* Tracking places where crimes occur in cities for crime prevention (criminology)
* Tracking social media postings to understand changes in public opinion (political science)
* Dynamically changing advertisements in search engines based on browsing history and query terms (marketing)
* Predicting trajectories of hurricanes (physical sciences)
* Finding genetic factors contributing diseases (medicine)
* Differentiating among novelists based on their writing styles (literary studies)
* Winnowing an enormous candidate list of chemicals for a target effect based on their known properties (chemistry)
* Diagnosing pulmonary diseases based on patient X ray images (medicine)
* Identifying students that are at-risk (education)

The prominence of data science comes from the fact that the means for collecting, sharing and analyzing data have become widely accessible thanks to the rise of computers. In many disciplines, research by way of data science has received a formidable status. It is thus important for new generations of professionals (educators, scientists, engineers, doctors, etc.) to understand data science fundamentals and know how to start explorations using data science. The objective of this course is to introduce the basics of data science and to teach data science tools and concepts that have immediate practical value to the students' field of interest.  

One such tool that we will study is the programming language R. R has its roots in statistics and has a wide variety of tools that make it easy for R programmers to conduct statistical analysis and visualization. While R is certainly not the only programming language data scientists use -- you may have heard about Python or Julia -- we identify some key reasons for designating it as the language for this course. 

* The syntax (i.e., the rules for writing code in a programming language) is easier to digest and understand for newcomers to programming. 
* R is not just a language, but an interactive environment for doing data science. This makes R much more flexible than many other programming languages, and helps focus on specific parts of the data science process. 
* R allows the use of "notebooks" that allow programmers to interleave code with prose in an intuitive manner. This is a great way for data scientists to explain and share their work with peers. 

After completing the course, students will be able to keep running and modifying their notebooks with new data sources and ideas.

## Enter the `tidyverse`

The bare-bones version of R, sometimes called *base* R, is complete but requires a bit more effort to write code for data analysis. People have developed code in R for serving certain purposes and have shared it as *packages*. Using packages is key to successful application of R; you can load a package someone else has written into your R programming environment and use it for your coding.

A nonprofit organization promoting R, [CRAN](https://cran.r-project.org/) (the **c**omprehensive **R** **a**rchive **n**etwork), maintains packages researchers have developed. The majority of the packages we will be studying in this course are part of the *tidyverse*, which is actually a collection of packages that are useful for data analysis and visualization; they are designed to work together with ease.  

The tidyverse can be installed with just a single line of code. 

```{r eval=FALSE}
install.packages("tidyverse")
```

Type the above in the console, and press "Enter" to run it. R will download the packages from CRAN and install them on to your computer. 

Let us see an example of how to plot some data using the `tidyverse`. 

### The World's Telephones

R ships with a number of off-the-shelf data sets you can play with. One of them is `WordPhones`, which provides the number of telephones in some large geographical regions of the world in 1951 and 1956-1961.

The analysis begins by loading the package into your programming environment.

```{r message = FALSE, warning = FALSE}
library(tidyverse)
```

This dataset is a 7x7 table of numbers where the rows correspond to years and the columns correspond to seven geographical areas: North America, Europe, Asia, South America, Oceania, Africa, and Middle America (meaning Central America).

We can inspect the table contents by typing its name into the console. 

```{r eval = FALSE}
WorldPhones
```

```{r echo = FALSE}
as.data.frame(WorldPhones)
```

Run your eye down the table and observe that the number of telephones in each region increases, some by much more than others.  

Suppose our goal is to plot the trend in the seven regions over the years. To accomplish this, we need to do some *data tidying*. Namely, we need the region and the number of telephones to appear as column pairs; we also give the year its own column. 

```{r}
transformed <- WorldPhones %>%
  as.data.frame() %>%
  rownames_to_column("Year") %>% 
  pivot_longer(cols = 2:8, names_to = "Area", values_to = "Count")
```

Here is how the new dataset looks like. We see that there are seven rows for each year and the regions and their counts appear vertically as a pair of columns.

```{r}
transformed
```

Don't worry if the code above seems overwhelming. We will go over each piece in great detail during the course. For now, just pay attention to what the dataset looks like after the transformation, and how it differs from the original. 

We can now plot the number of telephones with the year as the horizontal axis and the count as the vertical axis. We assign a different color to each of the seven regions.

```{r}
ggplot(transformed, aes(x = Year, y = Count)) +
  geom_point(aes(color = Area))
```

We call this a *scatter plot*, which we will see later in the course. This scatter plot tells us that the numbers steadily increased in North America, Europe, and Asia.

We can also plot the changes as bars where color denotes a certain region.

```{r}
ggplot(transformed) +
  geom_bar(aes(x = Year, y = Count, fill = as.factor(Area)),
           stat = "identity", position = "fill") +
  labs(fill = "Area")
```

We call this one a *stacked bar chart*, which we will also see later. Every plot tells a different story. This one signals to us how the *proportion* of telephones changes across each region.     

Can you spot some more differences between the two plots, especially in regards to what they say about the data?

## Processing Text 

Here is another example where we play with a textual dataset. As before, do not pay attention to the details of the code and what it is doing. Rather, familiarize yourself with the output of each code "block" and observe how it builds up toward accomplishing the goal. 

### The Classics: Herman Melville's Moby Dick  

[The Gutenberg Project](http://www.gutenberg.org/) is an international project to provide literary texts in a variety of formats for public use. One of the main literary works the site offers is "Moby Dick" by Herman Melville. You can download from the website by providing its link.

As before, we need `tidyverse`. We will also use another package called `readtext`.

```{r message=FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

We load the data from the Gutenberg Project and remove any preface or introductory material; we designate the loaded text with the name `moby`, so that we can refer to it again later on. 
```{r}
url <- "https://www.gutenberg.org/files/2701/2701-0.txt"
moby <- readtext(url) %>%
  str_sub(27780)
```

The current version of the text is what data scientists call "dirty." To make it tidy, we trim whitespaces and newline characters from the text, and lowercase all characters.   

```{r}
moby_chapters <- unlist(str_split(moby, fixed("CHAPTER"))) %>%
  str_trim() %>%
  str_replace_all("\\s+", " ") %>%
  str_to_lower()
```

We store the text in a table called `moby_df`. 

```{r, results='hide'}
moby_df <- tibble("chapters" = moby_chapters) %>% 
  slice(-1)
```

Here are what the first five chapters of Moby Dick look like after tidying. 

```{r eval = FALSE}
slice_head(moby_df, n = 5)
```

```{r warning = FALSE, message = FALSE, echo = FALSE}
library(kableExtra)

moby_chapters_tmp <- unlist(str_split(moby, fixed("CHAPTER"))) %>% 
  str_trim() %>%
  str_replace_all("\\s+", " ") %>%
  str_to_lower() %>%
  str_sub(end = 200) 
disp_df <- data.frame("chapters" = moby_chapters_tmp)


knitr::kable(
  head(disp_df, 5), booktabs = TRUE) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  column_spec(1, width = "30em")
```


### Plotting Word Relationships 

Now that we have the main text stored in tabular form, we are ready to do some analysis. Let us look at some associations between words in the text, for instance, between "ship" and "sea", and between "god" and "sea".

We compute in each chapter how many times each of the words occur: "ship", "sea", and "god". The results are stored as new columns appended to the table `moby_df`.

```{r}
moby_df <- moby_df %>%
  mutate(
    chapter_num = row_number(),
    ship = str_count(chapters, "ship"),
    sea = str_count(chapters, "sea"),
    god = str_count(chapters, "god")
  ) 
```

Here is what those new columns look like. 

```{r}
moby_df %>%
  select(chapter_num, ship, sea, god)
```

We draw a scatter plot showing the relationships between the pairs "ship" and "sea", and "god" and "sea". The former is shown in cyan and the latter in red. 

```{r message = FALSE}
ggplot(moby_df) +
  geom_point(aes(x = sea, y = ship, color = "ship")) + 
  geom_point(aes(x = sea, y = god, color = "god")) + 
  ylab("count")
```

We see a strong association between "sea" and "ship" since chapters that mention the word "sea" a lot tend to also mention "ship" a whole lot as well. The relationship between "sea" and "god" is less apparent. There is one chapter where "god" and "sea" both occur frequently, but overall it seems that the occurrences of "god" have nothing to do with occurrences of "sea". 

Indeed, word counts can only take us so far. But they are a first step in the interpretative process. For instance, why does it seem that there is no relationship between the sea, where most of the action takes place, and religion, as exemplified by the occurrences of the word "god"? Machine-assisted analysis of texts in this manner, which extends well beyond word counts, has become so popular that an interdisciplinary field called [Digital Humanities](https://en.wikipedia.org/wiki/Digital_humanities)  has become dedicated to its study. 

### A Rough Outline

Our two introductory examples have hopefully shown you a glimpse of what data science looks like. Here is the plan for the course. 

* We begin with a coverage of the fundamentals of R programming.
* We will move on to learning how the `tidyverse` can be used for data tidying and visualization.
* We then study how to extract *insights* from our data using statistical techniques.
* We end with some special topics, e.g., making *predictions* about data using regression analysis.  

We hope that you will find the journey interesting.

