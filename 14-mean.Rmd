# Why the Mean Matters

In this course we have studied several different statistics, including total variation distance, the maximum, the median, and also the mean. Under clear assumptions about randomness, we have drawn empirical distributions of all of these statistics. Some, like the maximum and the total variation distance, have distributions that are clearly skewed in one direction or the other. But the empirical distribution of the sample mean has almost always turned out close to bell-shaped, regardless of the population being studied.

If a property of random samples is true *regardless of the population*, it becomes a powerful tool for inference because we rarely know much about the data in the entire population. The distribution of the mean of a large random sample falls into this category of properties. That is why random sample means are extensively used in data science.

In this chapter, we will study means and what we can say about them with only minimal assumptions about the underlying populations. Question that we will address include:

* What exactly does the mean measure?
* How close to the mean are most of the data?
* How is the sample size related to the variability of the sample mean?
* Why do empirical distributions of random sample means come out bell shaped?
* How can we use sample means effectively for inference?

## Properties of the Mean 

In this course, we have used the words "average" and "mean" interchangeably, and will continue to do so. The definition of the mean will be familiar to you from your high school days or even earlier.

__Definition.__ The *average* or *mean* of a collection of numbers is the sum of all the elements of the collection, divided by the number of elements in the collection.

The function `mean` returns the mean of a vector. 

```{r}
not_symmetric <- c(2, 3, 3, 9)
mean(not_symmetric)
```

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

### Basic Properties

The definition and the example above point to some properties of the mean.

* It need not be an element of the collection.
* It need not be an integer even if all the elements of the collection are integers.
* It is somewhere between the smallest and largest values in the collection.
* It need not be halfway between the two extremes; it is not in general true that half the elements in a collection are above the mean.
* If the collection consists of values of a variable measured in specified units, then the mean has the same units too.


We will now study some other properties that are helpful in understanding the mean and its relation to other statistics.

### The Mean is a "Smoother"

You can think of taking the mean as an "equalizing" or "smoothing" operation. For example, imagine the entries in `not_symmetric` above as the dollars in the pockets of four different people. To get the mean, you first put all of the money into one big pot and then divide it evenly among the four people. They had started out with different amounts of money in their pockets (\$2, \$3, \$3, and \$9), but now each person has \$4.25, the mean amount.

### Proportions are Means

If a collection consists only of ones and zeroes, then the sum of the collection is the number of ones in it, and the mean of the collection is the proportion of ones.

```{r}
zero_one <- c(1, 1, 1, 0)
sum(zero_one)
```

```{r}
mean(zero_one)
```

You can replace 1 by the Boolean `TRUE` and 0 by `FALSE`:

```{r}
mean(c(TRUE, TRUE, TRUE, FALSE))
```

Because proportions are a special case of means, results about random sample means apply to random sample proportions as well.

### The Mean and the Histogram 

The mean of the collection {2, 3, 3, 9} is 4.25, which is not the "halfway point" of the data. So then what does the mean measure?

To see this, notice that the mean can be calculated in different ways.

$$\begin{align*}
\mbox{mean} ~ &=~ 4.25 \\ \\
&=~ \frac{2 + 3 + 3 + 9}{4} \\ \\
&=~ 2 \cdot \frac{1}{4} ~~ + ~~ 3 \cdot \frac{1}{4} ~~ + ~~ 3 \cdot \frac{1}{4} ~~ + ~~ 9 \cdot \frac{1}{4} \\ \\
&=~ 2 \cdot \frac{1}{4} ~~ + ~~ 3 \cdot \frac{2}{4} ~~ + ~~ 9 \cdot \frac{1}{4} \\ \\
&=~ 2 \cdot 0.25 ~~ + ~~ 3 \cdot 0.5 ~~ + ~~ 9 \cdot 0.25
\end{align*}$$

The last expression is an example of a general fact: when we calculate the mean, each distinct value in the collection is *weighted* by the proportion of times it appears in the collection.

This has an important consequence. The mean of a collection depends only on the distinct values and their proportions, not on the number of elements in the collection. In other words, the mean of a collection depends only on the distribution of values in the collection.

Therefore, __if two collections have the same distribution, then they have the same mean.__

For example, here is another collection that has the same distribution as `not_symmetric` and hence the same mean.

```{r}
not_symmetric
```

```{r}
same_distribution <- c(2, 2, 3, 3, 3, 3, 9, 9)
mean(same_distribution)
```

The mean is a physical attribute of the histogram of the distribution. Here is the histogram of the distribution of `not_symmetric` or equivalently the distribution of `same_distribution`.

```{r dpi=80, fig.align="center", message = FALSE}
tibble(not_symmetric) %>%
  ggplot() + 
  geom_histogram(aes(x = not_symmetric, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 10)
```

Imagine the histogram as a figure made out of cardboard attached to a wire that runs along the horizontal axis, and imagine the bars as weights attached at the values 2, 3, and 9. Suppose you try to balance this figure on a point on the wire. If the point is near 2, the figure will tip over to the right. If the point is near 9, the figure will tip over to the left. Somewhere in between is the point where the figure will balance; that point is the 4.25, the mean.

> The mean is the center of gravity or balance point of the histogram.

To understand why that is, it helps to know some physics. The center of gravity is calculated exactly as we calculated the mean, by using the distinct values weighted by their proportions.

Because the mean is a balance point, it is sometimes displayed as a *fulcrum* or triangle at the base of the histogram.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
tibble(not_symmetric) %>%
  ggplot() + 
  geom_histogram(aes(x = not_symmetric, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 10) +
  geom_point(aes(x = mean(not_symmetric), y = -0.02), 
             color = "salmon", shape = 17, size = 4)
```

### The Mean and the Median

If a student's score on a test is below average, does that imply that the student is in the bottom half of the class on that test?

Happily for the student, the answer is, "Not necessarily." The reason has to do with the relation between the average, which is the balance point of the histogram, and the median, which is the "half-way point" of the data.

The relationship is easy to see in a simple example. Here is a histogram of the collection {2, 3, 3, 4} which is in the vector `symmetric`. The distribution is symmetric about 3. The mean and the median are both equal to 3.

```{r}
symmetric <- c(2, 3, 3, 4)
```

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(tibble(symmetric)) + 
geom_histogram(aes(x = symmetric, y = ..density..),
               bins = 3,
               color = "gray", fill = "darkcyan") +
geom_point(aes(x = mean(symmetric), y = -0.02), 
           color = "salmon", shape = 17, size = 4)
```

```{r}
mean(symmetric)
```

```{r}
quantile(symmetric, c(0.5), type = 1)
```

In general, __for symmetric distributions, the mean and the median are equal.__

What if the distribution is not symmetric? Let's compare `symmetric` and `not_symmetric`.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot() + 
  geom_histogram(data = tibble(symmetric), aes(
                       x = symmetric, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 10) +
  geom_histogram(data = tibble(not_symmetric), aes(
                     x = not_symmetric, y = ..density..), alpha = 0.8,
                 color = "gray", fill = "salmon", bins = 10) + 
  geom_point(aes(x = mean(symmetric), y = -0.02), 
             color = "darkcyan", shape = 17, size = 4) + 
  geom_point(aes(x = mean(not_symmetric), y = -0.02), 
             color = "salmon", shape = 17, size = 4)
```

The dark cyan histogram represents the original `symmetric` distribution. The salmon colored histogram of `not_symmetric` starts out the same as the blue at the left end, but its rightmost bar has slid over to the value 9. The darker shaded region is where the two histograms overlap.

The median and mean of the blue distribution are both equal to 3. The median of the gold distribution is also equal to 3, though the right half is distributed differently from the left.

But the mean of the gold distribution is not 3: the gold histogram would not balance at 3. The balance point has shifted to the right, to 4.25.

In the gold distribution, 3 out of 4 entries (75%) are below average. The student with a below average score can therefore take heart. He or she might be in the majority of the class.

In general, __if the histogram has a tail on one side (the formal term is "skewed"), then the mean is pulled away from the median in the direction of the tail__.

### Example

The data frame `sf2015` contains salary and benefits data for San Francisco City employees in 2015. As before, we will restrict our analysis to those who had the equivalent of at least half-time employment for the year.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/san_francisco_2015.csv"
sf2015 <- read_csv(url) %>% filter(Salaries > 10000)
```

As we saw earlier, the highest compensation was above \$600,000 but the vast majority of employees had compensations below \$300,000.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(sf2015) + 
  geom_histogram(aes(x = `Total Compensation`, y = ..density..),
                 breaks = seq(10000, 700000, 25000),
                 color = "gray", fill = "darkcyan")
```

This histogram is skewed to the right; it has a right-hand tail.

The mean gets pulled away from the median in the direction of the tail. So we expect the mean compensation to be larger than the median, and that is indeed the case.

```{r}
compensation <- pull(sf2015, `Total Compensation`)
quantile(compensation, c(0.5), type = 1)
```

```{r}
mean(compensation)
```

Distributions of incomes of large populations tend to be right skewed. When the bulk of a population has middle to low incomes, but a very small proportion has very high incomes, the histogram has a long, thin tail to the right.

The mean income is affected by this tail: the farther the tail stretches to the right, the larger the mean becomes. But the median is not affected by values at the extremes of the distribution. That is why economists often summarize income distributions by the median instead of the mean.

## Variability

The mean tells us where a histogram balances. But in almost every histogram we have seen, the values spread out on both sides of the mean. How far from the mean can they be? To answer this question, we will develop a measure of variability about the mean.

We will start by describing how to calculate the measure. Then we will see why it is a good measure to calculate.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(gapminder)
library(nycflights13)
```

### The Rough Size of Deviations from Average

For simplicity, we will begin our calculations in the context of a simple vector `any_numbers` consisting of just four values. As you will see, our method will extend easily to any other vector of values.

```{r}
any_numbers <- c(1, 2, 2, 10)
```

The goal is to measure roughly how far off the numbers are from their average. To do this, we first need the average:

```{r}
# Step 1. The average.

mean_value <- mean(any_numbers)
mean_value
```

Next, let's find out how far each value is from the mean. These are called the *deviations from the average*. A "deviation from average" is just a value minus the average. The data frame `calculation_steps` displays the results.

```{r}
deviations <- any_numbers - mean_value
calculation_steps <- tibble(
  value = any_numbers, 
  deviation_from_average = deviations
)
calculation_steps
```

Some of the deviations are negative; those correspond to values that are below average. Positive deviations correspond to above-average values.

To calculate roughly how big the deviations are, it is natural to compute the mean of the deviations. But something interesting happens when all the deviations are added together:

```{r}
sum(deviations)
```

The positive deviations exactly cancel out the negative ones. This is true of all lists of numbers, no matter what the histogram of the list looks like: __the sum of the deviations from average is zero__.

Since the sum of the deviations is 0, the mean of the deviations will be 0 as well:

```{r}
mean(deviations)
```

Because of this, the mean of the deviations is not a useful measure of the size of the deviations. What we really want to know is roughly how big the deviations are, regardless of whether they are positive or negative. So we need a way to eliminate the signs of the deviations.

There are two time-honored ways of losing signs: the absolute value, and the square. It turns out that taking the square constructs a measure with extremely powerful properties, some of which we will study in this course.

So let's eliminate the signs by squaring all the deviations. Then we will take the mean of the squares:

```{r}
# Step 3. The squared deviations from average

squared_deviations <- deviations ** 2
calculation_steps <- calculation_steps %>%
  mutate(sq_deviation_from_average = squared_deviations)
calculation_steps
```

```{r}
# Step 4. Variance = the mean squared deviation from average
variance <- mean(squared_deviations)
variance
```

__Variance:__ The mean squared deviation calculated above is called the variance of the values.

While the variance does give us an idea of spread, it is not on the same scale as the original variable as its units are the square of the original. This makes interpretation very difficult.

So we return to the original scale by taking the positive square root of the variance:

```{r}
# Step 5.
# Standard Deviation:    root mean squared deviation from average
# Steps of calculation:   5    4      3       2             1
sd <- variance ** 0.5
sd
```

### Standard Deviation

The quantity that we have just computed is called the *standard deviation* of the list, and is abbreviated as SD. It measures roughly how far the numbers on the list are from their average.

__Definition.__ The SD of a list is defined as the *root mean square of deviations from average*. That's a mouthful. But read it from right to left and you have the sequence of steps in the calculation.

__Computation.__ The five steps described above result in the SD. You can also use the function `sd` to compute the SD of values in a vector:

```{r}
sd(any_numbers)
```

When we are working specifically with the *population* standard deviation, and not a sample from it, we will prefer to use `pop.sd` by defining the following function. This undoes a statistical correction provided by R. 

```{r}
pop.sd <- function(x) sd(x) * sqrt((length(x)-1) / length(x))
```

### Working with the SD

To see what we can learn from the SD, let's move to a more interesting dataset than `any_numbers`. The data frame `gapminder` contains a subset of data from the [Gapminder project](http://gapminder.org/), which provides values for life expectancy, GDP per capita, and population, every five years, from 1952 to 2007. For this study, we will concentrate on the data from the most recent year reported, 2007.   

```{r}
gap2007 <- gapminder %>%
  filter(year == 2007)
gap2007
```

Here is a histogram of life expectancy.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(gap2007) +
  geom_histogram(aes(x = lifeExp, y = ..density..),
                 color = "gray", fill = "darkcyan")
```

It is no surprise that the life expectancy of modern countries clusters around 70 and above. The average life expectancy in 2007 is approximately 67 years old.  

```{r}
mean_lexp <- mean(pull(gap2007, lifeExp))
mean_lexp
```

About how far off are the countries' life expectancy from the average? This is measured by the SD of life expectancy, which is about 12 years.

```{r}
sd_lexp = sd(pull(gap2007, lifeExp))
sd_lexp
```

Japan ranked highest with a life expectancy of 82.6 years. 

```{r}
arrange(gap2007, desc(lifeExp)) %>%
  head(5)
```

Japan was about 15.6 years above the average life expectancy!

```{r}
82.603 - mean_lexp
```

That's a deviation from average, and it is about 1.29 times the standard deviation:

```{r}
(82.603 - mean_lexp) / sd_lexp
```

In other words, the highest life expectancy for a country in 2007 was about 1.29 SDs above average.

At a life expectancy of approximately 40 years, Swaziland ranked among the lowest in the world with respect to life expectancy. Its life expectancy was about 2.3 SDs below average. 

```{r}
arrange(gap2007, lifeExp) %>%
  head(5)
```

```{r}
(39.613 - mean_lexp) / sd_lexp
```

Swaziland (now officially known as Eswatini) experienced [high prevalence of HIV/AIDS](https://reliefweb.int/report/swaziland/swaziland-life-expectancy-drop-40-yrs-2010-report) in the years leading up to 2007, which contributed to a dramatic decline in life expectancy.   

What we have observed is that the highest- and lowest-ranking countries in terms of life expectancy were both just a few SDs away from the average life expectancy. This is an example of why the SD is a useful measure of spread. No matter what the shape of the histogram, the average and the SD together tell you a lot about where the histogram is situated on the number line.

### First main reason for measuring spread by the SD 

__Informal statement.__ In all numerical data sets, the bulk of the entries are within the range "average $\pm$ a few SDs".

For now, resist the desire to know exactly what fuzzy words like "bulk" and "few" mean. We will make them precise later in this section. Let's just examine the statement in the context of some more examples.

We have already seen that the life expectancies of countries around the world in 2007 were in the range "average $\pm$ 3 SDs".

What about GDP per capita? Here is a histogram of the distribution, along with the mean and SD. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(gap2007) +
  geom_histogram(aes(x = gdpPercap, y = ..density..),
                 color = "gray", fill = "darkcyan")
```

```{r}
mean_gdp <- mean(pull(gap2007, gdpPercap))
sd_gdp <- sd(pull(gap2007, gdpPercap))
c(mean_gdp, sd_gdp)
```

The average GDP per capita was \$11,680.07, and the SD was about \$12,859.94.

How far off is GDP per capita from the average? Just as we did with the heights, let's look at the two extreme values of the ages.

Norway has the highest, at $49,357.19. 

```{r}
arrange(gap2007, desc(gdpPercap)) %>%
  head(5)
```

This is about 2.93 SDs above average.

```{r}
(49357.19 - mean_gdp) / sd_gdp
```

The lowest GDP per capita was recorded by Republic of the Congo. This is about 0.89 SDs below average.   

```{r}
arrange(gap2007, gdpPercap) %>%
  head(5)
```
```{r}
(277.5519 - mean_gdp) / sd_gdp
```

What we have observed for life expectancy and GDP per capita is true in great generality. For *all* lists, the bulk of the entries are no more than 2 or 3 SDs away from the average.

### Chebychev's Bounds

The Russian mathematician [Pafnuty Chebychev](https://en.wikipedia.org/wiki/Pafnuty_Chebyshev) (1821-1894) proved a result that makes our rough statements precise.

> For all lists, and all numbers $z$, the proportion of entries that are in the range "average $\pm z$ SDs" is at least $1 - \frac{1}{z^2}$

It is important to note that the result gives a bound, not an exact value or an approximation.

What makes the result powerful is that it is true for all lists – all distributions, no matter how irregular.

Specifically, it says that for every list:

* the proportion in the range "average $\pm$ 2 SDs" is __at least $1 - 1/4$ = 0.75__
* the proportion in the range "average $\pm$ 3 SDs" is __at least $1 - 1/9$ $\approx$ 0.89__
* the proportion in the range "average $\pm$ 4.5 SDs" is __at least $1 - 1/{4.5}^2$ $\approx$ 0.95__

As we noted above, Chebychev's result gives a lower bound, not an exact answer or an approximation. For example, the percent of entries in the range "average $\pm$ 2 SDs" might be quite a bit larger than 75%. But it cannot be smaller.

### Standard units

In the calculations above, the quantity $z$ measures *standard units*, the number of standard deviations above average.

Some values of standard units are negative, corresponding to original values that are below average. Other values of standard units are positive. But no matter what the distribution of the list looks like, Chebychev's bounds imply that standard units will typically be in the (-5, 5) range.

To convert a value to standard units, first find how far it is from average, and then compare that deviation with the standard deviation.

\[ z = \frac{\text{value} - \text{average}}{\text{SD}}\]

As we will see, standard units are frequently used in data analysis. It is so useful, in fact, that R provides a function `scale` to convert a vector of numbers to standard units using the above formula. 

### Example

As we saw in an earlier section, the data frame `flights` contains a column `dep_delay` consisting of the departure delay times, in minutes, of over thousands of flights that departed NYC in 2013. We will create a new column called `delay_standard` by applying the function `scale` to the column of delay times. This allows us to see all the delay times in minutes as well as their corresponding values in standard units. Note that we filter any flights where missing data, i.e. `NA`, is present.

```{r}
flights_standard <- flights %>%
  drop_na() %>% 
  transmute(dep_delay,
            delay_standard = scale(dep_delay))
flights_standard
```

The standard units that we can see are consistent with what we expect based on Chebychev's bounds. These are of quite small size. 

But something rather alarming happens when we sort the delay times from highest to lowest. The standard units that we can see are extremely high!

```{r}
arrange(flights_standard, desc(delay_standard))
```

What this shows is that it is possible for data to be many SDs above average -- and for flights to be delayed by almost 22 hours, yikes! The highest value of delay is about 32 in standard units.

However, the proportion of these extreme values is small, and Chebychev's bounds still hold true. For example, let us calculate the percent of delay times that are in the range "average $\pm$ 3 SDs". This is the same as the percent of times for which the standard units are in the range (-3, 3). That is about 98%, as computed below, consistent with Chebychev's bound of "at least 89%".

```{r}
within_3_sd <- nrow(filter(flights_standard, between(delay_standard, -3, 3)))
within_3_sd / nrow(flights_standard)
```

The histogram of delay times is shown below, with the horizontal axis in standard units. By the table above, the right hand tail continues all the way out to $z = 32.15$ standard units (1301 minutes). The area of the histogram outside the range $z = -3$ to $z = 3$ is about 2%, put together in tiny little bits that are mostly invisible in the histogram.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(flights_standard) + 
  geom_histogram(aes(x = delay_standard, y = ..density..),
                 color = "gray", fill = "darkcyan") + 
  scale_x_continuous(breaks = seq(-3, 33, 3)) + 
  xlab("Delay (Standard Units)")
```


## The SD and the Normal Curve

We know that the mean is the balance point of the histogram. Unlike the mean, the SD is usually not easy to identify by looking at the histogram.

However, there is one shape of distribution for which the SD is almost as clearly identifiable as the mean. That is the bell-shaped disribution. This section examines that shape, as it appears frequently in probability histograms and also in some histograms of data.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

### A Roughly Bell-Shaped Histogram of Data

Let us look at the distribution of heights of mothers in our familiar sample of 1,174 mother-newborn pairs. The mothers' heights have a mean of 64 inches and an SD of 2.5 inches. Unlike the life expectancy of countries around the world, the mothers' heights are distributed fairly symmetrically about the mean in a bell-shaped curve.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/baby.csv"
baby <- read_csv(url)
```

```{r}
heights <- pull(baby, `Maternal Height`)
mean_height <- round(mean(heights), 1)
mean_height
```

```{r}
sd_height <- round(sd(heights), 1)
sd_height
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(baby) + 
  geom_histogram(aes(x = `Maternal Height`, y = ..density..), 
                 color = "gray", fill = "darkcyan",
                 breaks = seq(55.5, 72.5, 1)) + 
  scale_x_continuous(breaks = seq(-3, 3.1, 1)*sd_height + mean_height)
```

The last two lines of code in the cell above change the labeling of the horizontal axis. Now, the labels correspond to "average $\pm z$ SDs" for 
$z = 0, \pm 1, \pm 2,$ and $\pm 3$. Because of the shape of the distribution, the "center" has an unambiguous meaning and is clearly visible at 64.

### How to Spot the SD on a Bell Shaped Curve

To see how the SD is related to the curve, start at the top of the curve and look towards the right. Notice that there is a place where the curve changes from looking like an "upside-down cup" to a "right-way-up cup"; formally, the curve has a point of inflection. That point is one SD above average. It is the point $z = 1$, which is "average plus 1 SD" = 66.5 inches.

Symmetrically on the left-hand side of the mean, the point of inflection is at $z = -1$, that is, "average minus 1 SD" = 61.5 inches.

In general:

> For bell-shaped distributions, the SD is the distance between the mean and the points of inflection on either side.

### The standard normal curve

All the bell-shaped histograms that we have seen look essentially the same apart from the labels on the axes. Indeed, there is really just one basic curve from which all of these curves can be drawn just by relabeling the axes appropriately.

To draw that basic curve, we will use the units into which we can convert every list: standard units. The resulting curve is therefore called the *standard normal curve*.

The standard normal curve has an impressive equation. But for now, it is best to think of it as a smoothed outline of a histogram of a variable that has been measured in standard units and has a bell-shaped distribution.

$$
\phi(z) = {\frac{1}{\sqrt{2 \pi}}} e^{-\frac{1}{2}z^2}, ~~ -\infty < z < \infty
$$

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -∞ < z < ∞") +
  theme(plot.title = element_text(hjust = 0.5))
```

As always when you examine a new histogram, start by looking at the horizontal axis. On the horizontal axis of the standard normal curve, the values are standard units.

Here are some properties of the curve. Some are apparent by observation, and others require a considerable amount of mathematics to establish.

* The total area under the curve is 1. So you can think of it as a histogram drawn to the density scale.

* The curve is symmetric about 0. So if a variable has this distribution, its mean and median are both 0.

* The points of inflection of the curve are at -1 and +1.

* If a variable has this distribution, its SD is 1. The normal curve is one of the very few distributions that has an SD so clearly identifiable on the histogram.

Since we are thinking of the curve as a smoothed histogram, we will want to represent proportions of the total amount of data by areas under the curve.

Areas under smooth curves are often found by calculus, using a method called integration. It is a fact of mathematics, however, that the standard normal curve cannot be integrated in any of the usual ways of calculus.

Therefore, areas under the curve have to be approximated. That is why almost all statistics textbooks carry tables of areas under the normal curve. It is also why all statistical systems, especially statistical programming languages like R, include methods that provide excellent approximations to those areas.

### The standard normal "cdf"

The fundamental function for finding areas under the normal curve is `pnorm`. It takes a numerical argument and returns all the area under the curve to the left of that number. Formally, it is called the "cumulative distribution function" of the standard normal curve. That rather unwieldy mouthful is abbreviated as cdf.

Let us use this function to find the area to the left of $z = 1$ under the standard normal curve.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-3,1), geom = "area", fill = "salmon") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -∞ < z < 1") +
  theme(plot.title = element_text(hjust = 0.5))
```

The numerical value of the shaded area can be found by calling `pnorm`.

```{r}
pnorm(1)
```

That's about 84%. We can now use the symmetry of the curve and the fact that the total area under the curve is 1 to find other areas.

The area to the right of $z = 1$ is about 100% - 84% = 16%.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(1,3), geom = "area", fill = "salmon") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) 1 < z < ∞") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
1 - pnorm(1)
```

The area between $z = -1$ and $z = 1$ can be computed in several different ways. It is the salmon-colored area under the curve below.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-1,1), geom = "area", fill = "salmon") +
  stat_function(fun = dnorm, 
                xlim = c(-3,-1), geom = "area", fill = "darkcyan") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -1 < z < 1") +
  theme(plot.title = element_text(hjust = 0.5))
```


For example, we could calculate the area as "100% - two equal tails", which works out to roughly 100% - 2x16% = 68%.

Or we could note that the area between $z = 1$ and $z = -1$ is equal to all the area to the left of $z = 1$, minus all the area to the left of $z = -1$. 

```{r}
pnorm(1) - pnorm(-1)
```

By a similar calculation, we see that the area between $z = -2$ and $z = 2$ is about 95%.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-2,2), geom = "area", fill = "salmon") +
  stat_function(fun = dnorm, 
                xlim = c(-3,-2), geom = "area", fill = "darkcyan") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") + 
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -2 < z < 2") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
pnorm(2) - pnorm(-2)
```

In other words, if a histogram is roughly bell shaped, the proportion of data in the range "average $\pm 2$ SDs" is about 95%. 

That is quite a bit more than Chebychev's lower bound of 75%. Chebychev's bound is weaker because it has to work for all distributions. If we know that a distribution is normal, we have good approximations to the proportions, not just bounds.

The table below compares what we know about all distributions and about normal distributions. Notice that when $z = 1$, Chebychev's bound is correct but not illuminating.

| Percent in Range | All Distributions: Bound | Normal Distribution: Approximation |
|---------------:|:------------------:|:-------------------:|
|average $\pm$ 1 SD  | at least 0%         | about 68%           |
|average $\pm$ 2 SDs | at least 75%        | about 95%           |
|average $\pm$ 3 SDs | at least 88.888...% | about 99.73%        |


## The Central Limit Theorem

Very few of the data histograms that we have seen in this course have been bell shaped. When we have come across a bell shaped distribution, it has almost invariably been an empirical histogram of a statistic based on a random sample.

The examples below show two very different situations in which an approximate bell shape appears in such histograms.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
library(nycflights13)
```

### Net Gain in Roulette 

In an earlier section, the bell appeared as the rough shape of the total amount of money we would make if we placed the same bet repeatedly on different spins of a roulette wheel.

```{r message=FALSE}
wheel <- read_csv("data/roulette_wheel.csv")
wheel
```

Recall that the bet on red pays even money, 1 to 1. We defined the function `red_winnings` that returns the net winnings on one $1 bet on red. Specifically, the function takes a color as its argument and returns 1 if the color is red. For all other colors it returns -1.

```{r}
red_winnings <- function(color) {
  if(color == "red") {
    return(1)
  } else {
    return(-1)
  }
}
```

The data frame `red` shows each pocket's winnings on red.

```{r}
red <- wheel %>%
  mutate(winnings_red = map_dbl(color, red_winnings))
red
```

Your net gain on one bet is one random draw from the Winnings: Red column. There is an 18/38 chance making \$1, and a 20/38 chance of making -\$1. This probability distribution is shown in the histogram below.

```{r}
ggplot(red) + 
  geom_histogram(aes(x = winnings_red, y = ..density..), 
                 breaks = seq(-1.5, 1.6, 1),
                 color = "gray", fill = "darkcyan")
```

Now suppose you bet many times on red. Your net winnings will be the sum of many draws made at random with replacement from the distribution above.

It will take a bit of math to list all the possible values of your net winnings along with all of their chances. We won't do that; instead, we will approximate the probability distribution by simulation, as we have done all along in this course.

The code below simulates your net gain if you bet $1 on red on 400 different spins of the roulette wheel.

```{r}
one_roulette_simulation <- function(x, num_bets) {
  spins <- slice_sample(red, n = num_bets, replace = TRUE)
  net_gain_red <- sum(pull(spins, winnings_red))
  return(net_gain_red)
}
```

```{r}
num_bets <- 400
repetitions <- 10000
net_gains <- map_dbl(1:repetitions, one_roulette_simulation, num_bets)
```

Let's generalize our simulation function a bit so that we can re-use it in
later simulations. Mainly we are replacing the column `winnings_red` with 
a generic `label`, and likewise `red` with `df`. We also substitute `sum`
for some functional called `statistic`. 

```{r}
one_simulation <- function(x, df, label, func, num_draws) {
  label <- enquo(label) # Create quosure
  
  sample_df <- slice_sample(df, n = num_draws, replace = TRUE)
  results <- func(pull(sample_df, !!label))
  return(results)
}
```

```{r}
num_bets <- 400
repetitions <- 10000
net_gains <- map_dbl(.x = 1:repetitions, 
                     .f = one_simulation,
                     df = red,
                     label = winnings_red,
                     func = sum, 
                     num_draws = num_bets)
```

Let's now visualize the results. 

```{r}
tibble(net_gains) %>%
  ggplot() + 
  geom_histogram(aes(x = net_gains, y = ..density..),
                 breaks = seq(-80, 50, 6),
                 color = "gray", fill = "darkcyan") + 
  scale_x_continuous(breaks = seq(-80, 40, 20)) +
  xlab("Net Gain on Red")
```

That's a roughly bell shaped histogram, even though the distribution we are drawing from is nowhere near bell shaped.

__Center.__ The distribution is centered near -20 dollars, roughly. To see why, note that your winnings will be \$1 on about 18/38 of the bets, and -\$1 on the remaining 20/38. So your average winnings per dollar bet will be roughly -5.26 cents:

```{r}
average_per_bet <- 1*(18/38) + (-1)*(20/38)
average_per_bet
```

So in 400 bets you expect that your net gain will be about -\$21:

```{r}
400 * average_per_bet
```

For confirmation, we can compute the mean of the 10,000 simulated net gains:

```{r}
mean(net_gains)
```

__Spread.__ Run your eye along the curve starting at the center and notice that the point of inflection is near 0. On a bell shaped curve, the SD is the distance from the center to a point of inflection. The center is roughly -\$20, which means that the SD of the distribution is around \$20.

In the next section we will see where the \$20 comes from. For now, let's confirm our observation by simply calculating the SD of the 10,000 simulated net gains:

```{r}
sd(net_gains)
```

__Summary.__ The net gain in 400 bets is the sum of the 400 amounts won on each individual bet. The probability distribution of that sum is approximately normal, with an average and an SD that we can approximate.

### Average Flight Delay

The data frame `flights` contains data on departure delays of 336,776 flights that departed NYC in 2013. As we have seen before, the distribution of delays has a long right-hand tail.

```{r}
flights <- flights %>%
  drop_na()
```

```{r dpi=80, fig.align="center", message = FALSE, warning = FALSE}
ggplot(flights) +
  geom_histogram(aes(x = dep_delay, y = ..density..),
                 color = "gray", fill = "darkcyan")
```

The mean delay was about 16.6 minutes and the SD was about 39.5 minutes. Notice how large the SD is, compared to the mean. Those large deviations on the right have an effect, even though they are a very small proportion of the data.

```{r}
mean_delay <- mean(pull(flights, dep_delay))
sd_delay <- sd(pull(flights, dep_delay))
c(mean_delay, sd_delay)
```

Now suppose we sampled 400 delays at random with replacement. You could sample without replacement if you like, but the results would be very similar to with-replacement sampling. If you sample a few hundred out of 336,776 without replacement, you hardly change the population each time you pull out a value.

In the sample, what could the average delay be? We expect it to be around 12 or 13, because that's the population average; but it is likely to be somewhat off. Let's see what we get by sampling. 

```{r}
sampled <- slice_sample(flights, n = 400, replace = TRUE)
mean(pull(sampled, dep_delay))
```

The sample average varies according to how the sample comes out, so we will simulate the sampling process repeatedly and draw the empirical histogram of the sample average. That will be an approximation to the probability histogram of the sample average.

```{r}
sample_size <- 400
repetitions <- 10000
sample_means <- map_dbl(.x = 1:repetitions, 
                     .f = one_simulation,
                     df = flights,
                     label = dep_delay,
                     func = mean, 
                     num_draws = sample_size)
```

```{r dpi=80, fig.align="center", message = FALSE}
tibble(sample_means) %>%
  ggplot() + 
  geom_histogram(aes(x = sample_means, y = ..density..),
                 color = "gray", fill = "darkcyan") + 
  scale_x_continuous(breaks = seq(6, 22, 2))
```

Once again, we see a rough bell shape, even though we are drawing from a very skewed distribution. The bell is centered somewhere between 12 and 13, as we expect.

### Central Limit Theorem

The reason why the bell shape appears in such settings is a remarkable result of probability theory called the __Central Limit Theorem__.

> The Central Limit Theorem says that the probability distribution of the sum or average of a large random sample drawn with replacement will be roughly normal, *regardless of the distribution of the population from which the sample is drawn*.

As we noted when we were studying Chebychev's bounds, results that can be applied to random samples *regardless of the distribution of the population* are very powerful, because in data science we rarely know the distribution of the population.

The Central Limit Theorem makes it possible to make inferences with very little knowledge about the population, provided we have a large random sample. That is why it is central to the field of statistical inference.

### Proportion of Purple Flowers

Recall Mendel's probability model for the colors of the flowers of a species of pea plant. The model says that the flower colors of the plants are like draws made at random with replacement from {Purple, Purple, Purple, White}.

In a large sample of plants, about what proportion will have purple flowers? We would expect the answer to be about 0.75, the proportion purple in the model. And, because proportions are means, the Central Limit Theorem says that the distribution of the sample proportion of purple plants is roughly normal.

We can confirm this by simulation. Let's simulate the proportion of purple-flowered plants in a sample of 200 plants.

```{r}
colors <- c('Purple', 'Purple', 'Purple', 'White')
model <- tibble(colors)
model
```
```{r}
num_plants <- 200
repetitions <- 10000
# define a proportion with a functional 
flower_proportion <- function(x) sum(x == 'Purple') / length(x)

sample_props <- map_dbl(.x = 1:repetitions, 
                     .f = one_simulation,
                     df = model,
                     label = colors,
                     func = flower_proportion, 
                     num_draws = num_plants)

```

```{r dpi=80, fig.align="center", message = FALSE}
tibble(sample_props) %>%
  ggplot() + 
  geom_histogram(aes(x = sample_props, y = ..density..),
                 breaks = seq(0.65, 0.85, 0.01),
                 color = "gray", fill = "darkcyan") 
```

There's that normal curve again, as predicted by the Central Limit Theorem, centered at around 0.75 just as you would expect.

How would this distribution change if we increased the sample size? Let's run the code again with a sample size of 800, and collect the results of simulations in the same table in which we collected simulations based on a sample size of 200. We will keep the number of repetitions the same as before so that the two columns have the same length.

```{r}
num_plants <- 800

sample_props2 <- map_dbl(.x = 1:repetitions, 
                     .f = one_simulation,
                     df = model,
                     label = colors,
                     func = flower_proportion, 
                     num_draws = num_plants)
```

```{r dpi=80, fig.align="center", message = FALSE}
# get the sample proportion results in a table-ready form for plotting
plot_df <- tibble(
  props = c(sample_props, sample_props2),
  from =  c(rep("sample proportion: 200", 10000), 
            rep("sample proportion: 800", 10000))
)

ggplot(plot_df) + 
  geom_histogram(aes(x = props, y = ..density.., fill = from),
                 breaks = seq(0.65, 0.85, 0.01),
                 alpha = 0.8, position = "identity")
```

Both distributions are approximately normal but one is narrower than the other. The proportions based on a sample size of 800 are more tightly clustered around 0.75 than those from a sample size of 200. Increasing the sample size has decreased the variability in the sample proportion.

This should not be surprising. We have leaned many times of the intuition that a larger sample size generally reduces the variability of a statistic. However, in the case of a sample average, we can *quantify* the relationship between sample size and variability.

Exactly how does the sample size affect the variability of a sample average or proportion? That is the question we will examine in the next section.

## The Variability of the Sample Mean

By the Central Limit Theorem, the probability distribution of the mean of a large random sample is roughly normal. The bell curve is centered at the population mean. Some of the sample means are higher, and some lower, but the deviations from the population mean are roughly symmetric on either side, as we have seen repeatedly. Formally, probability theory shows that the sample mean is an *unbiased* estimate of the population mean.

In our simulations, we also noticed that the means of larger samples tend to be more tightly clustered around the population mean than means of smaller samples. In this section, we will quantify the variability of the sample mean and develop a relation between the variability and the sample size.

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
library(nycflights13)
```

### Flight delays revisited

Let's start with our data frame `flights` of flight delays. The mean delay is about 12.56 minutes, and the distribution of delays is skewed to the right.

```{r}
flights <- flights %>%
  drop_na()
```

```{r}
pop_mean <- mean(pull(flights, dep_delay))
pop_mean
```

```{r dpi=80, fig.align="center", message = FALSE, warning = FALSE}
ggplot(flights) +
  geom_histogram(aes(x = dep_delay, y = ..density..),
                 color = "gray", fill = "darkcyan") + 
  geom_point(aes(x = pop_mean, y = 0), size = 2, color = "salmon")
```
Now let's take random samples and look at the probability distribution of the sample mean. As usual, we will use simulation to get an empirical approximation to this distribution.

We will define a function `simulate_sample_mean` to do this, because we are going to vary the sample size later. The arguments are the sample size and the number of simulations. This function will re-use the `one_simulation` function we wrote in the previous section. 

```{r}
one_simulation <- function(x, df, label, func, num_draws) {
  label <- enquo(label) # Create quosure
  
  sample_df <- slice_sample(df, n = num_draws, replace = TRUE)
  result <- func(pull(sample_df, !!label))
  return(result)
}
```

```{r}
simulate_sample_mean <- function(sample_size, repetitions) {
  means <- map_dbl(.x = 1:repetitions, 
                   .f = one_simulation,
                   df = flights,
                   label = dep_delay,
                   func = mean, 
                   num_draws = sample_size)

  print(paste("Sample size: ", sample_size))
  print(paste("Population mean: ", mean(pull(flights, dep_delay))))
  print(paste("Average of sample means: ", mean(means)))
  print(paste("Population SD: ", sd(pull(flights, dep_delay))))
  print(paste("SD of sample means: ", sd(means)))
  
  means_df <- tibble(means)
  g <- ggplot(means_df) + 
    geom_histogram(aes(x = means, y = ..density..), bins = 20,
                   color = "gray", fill = "darkcyan") +
    ggtitle(paste("Sample size ", sample_size))
  return(g)
}
```

Let us simulate the mean of a random sample of 100 delays, then of 400 delays, and finally of 625 delays. We will perform 10,000 repetitions of each of these process. 

```{r dpi=80, fig.align="center", message = FALSE, warning = FALSE}
g <- simulate_sample_mean(100, 10000)
g
```

```{r dpi=80, fig.align="center", message = FALSE, warning = FALSE}
g <- simulate_sample_mean(400, 10000)
g
```

```{r dpi=80, fig.align="center", message = FALSE, warning = FALSE}
g <- simulate_sample_mean(625, 10000)
g
```

You can see the Central Limit Theorem in action – the histograms of the sample means are roughly normal, even though the histogram of the delays themselves is far from normal.

You can also see that each of the three histograms of the sample means is centered very close to the population mean. In each case, the "average of sample means" is very close to 16.66 minutes, the population mean. Both values are provided in the printout above each histogram. As expected, the sample mean is an unbiased estimate of the population mean.

### The SD of All the Sample Means

You can also see that the histograms get narrower, and hence taller, as the sample size increases. We have seen that before, but now we will pay closer attention to the measure of spread.

The SD of the population of all delays is about 40 minutes.

```{r}
pop_sd <- sd(pull(flights, dep_delay))
pop_sd
```

Take a look at the SDs in the sample mean histograms above. In all three of them, the SD of the population of delays is about 40 minutes, because all the samples were taken from the same population.

Now look at the SD of all 10,000 sample means, when the sample size is 100. That SD is about one-tenth of the population SD. When the sample size is 400, the SD of all the sample means is about one-twentieth of the population SD. When the sample size is 625, the SD of the sample means is about one-twentyfifth of the population SD.

It seems like a good idea to compare the SD of the empirical distribution of the sample means to the quantity "population SD divided by the square root of the sample size."

The cell takes a while to run, as it's a large simulation. But you'll soon see that it's worth the wait.

```{r}
experiment <- function(x, repetitions) {
  means <- map_dbl(.x = 1:repetitions, 
                   .f = one_simulation,
                   df = flights,
                   label = dep_delay,
                   func = mean, 
                   num_draws = x)
  return(sd(means))
}
```

```{r}
repetitions <- 10000
sample_sizes <- seq(25, 626, 25)
sd_means <- map_dbl(.x = sample_sizes, 
                    .f = experiment,
                    repetitions = repetitions)
```

Here are the numerical values in tabular form. For each sample size in the first column, 10,000 random samples of that size were drawn, and the 10,000 sample means were calculated. The second column contains the SD of those 10,000 sample means. The third column contains the result of the calculation "population SD divided by the square root of the sample size."

```{r}
sd_comparison <- tibble(
  sample_sizes = sample_sizes, 
  sd_10000_sample_means = sd_means, 
  `pop_sd/sqrt(n)` = pop_sd / sqrt(sample_sizes)
)
sd_comparison
```

The values in the second and third columns are very close. If we plot each of those columns with the sample size on the horizontal axis, the two graphs are essentially indistinguishable.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
plot_df <- tibble(
  sample_size = rep(sample_sizes, 2), 
  sd_sample_means = c(sd_means, pop_sd / sqrt(sample_sizes)),
  type = c(rep('SD of 10,000 means', length(sample_sizes)),
           rep('pop_sd/sqrt(n)', length(sample_sizes)))
)

ggplot(plot_df) + 
  geom_line(aes(x = sample_size, y = sd_sample_means, linetype = type),
            color = "salmon")
```
There really are two curves there. But they are so close to each other that it looks as though there is just one.

What we are seeing is an instance of a general result. Remember that the graph above is based on 10,000 replications for each sample size. But there are many more than 10,000 samples of each size. The probability distribution of the sample mean is based on the means of all possible samples of a fixed size.

Fix a sample size. If the samples are drawn at random with replacement from the population, then

\[ \text{SD of all possible sample means} 
    = \frac{\text{Population SD}}{\sqrt{\text{sample size}}} \]

This is the standard deviation of the averages of all the possible samples that could be drawn. __It measures roughly how far off the sample means are from the population mean.__

### The Central Limit Theorem for the Sample Mean

If you draw a large random sample with replacement from a population, then, regardless of the distribution of the population, the probability distribution of the sample mean is roughly normal, centered at the population mean, with an SD equal to the population SD divided by the square root of the sample size.

### The Accuracy of the Sample Mean 

The SD of all possible sample means measures how variable the sample mean can be. As such, it is taken as a measure of the accuracy of the sample mean as an estimate of the population mean. The smaller the SD, the more accurate the estimate.

The formula shows that:

* The population size doesn't affect the accuracy of the sample mean. The population size doesn't appear anywhere in the formula.
* The population SD is a constant; it's the same for every sample drawn from the population. The sample size can be varied. Because the sample size appears in the denominator, the variability of the sample mean *decreases* as the sample size increases, and hence the accuracy increases.

### The Square Root Law 

From the table of SD comparisons, you can see that the SD of the means of random samples of 25 flight delays is about 8 minutes. If you multiply the sample size by 4, you'll get samples of size 100. The SD of the means of all of those samples is about 4 minutes. That's smaller than 8 minutes, but it's not 4 times as small; it's only 2 times as small. That's because the sample size in the denominator has a square root over it. The sample size increased by a factor of 4, but the SD went down by a factor of $2 = \sqrt{4}$. In other words, the accuracy went up by a factor of $2 = \sqrt{4}$. 

In general, when you multiply the sample size by a factor, the accuracy of the sample mean goes up by the square root of that factor.

So to increase accuracy by a factor of 10, you have to multiply sample size by a factor of 100. Accuracy doesn't come cheap!

## Choosing a Sample Size

Candidate A is contesting an election. A polling organization wants to estimate the proportion of voters who will vote for her. Let's suppose that they plan to take a simple random sample of voters, though in reality their method of sampling would be more complex. How can they decide how large their sample should be, to get a desired level of accuracy?

We are now in a position to answer this question, after making a few assumptions:

* The population of voters is very large and that therefore we can just as well assume that the random sample will be drawn with replacement.

* The polling organization will make its estimate by constructing an approximate 95% confidence interval for the percent of voters who will vote for Candidate A.

* The desired level of accuracy is that the width of the interval should be no more than 1%. That's pretty accurate! For example, the confidence interval (33.2%, 34%) would be fine but (33.2%, 35%) would not.

We will work with the sample proportion of voters for Candidate A. Recall that a proportion is a mean, when the values in the population are only 0 (the type of individual you are not counting) or 1 (the type of individual you are counting).

### Width of Confidence Interval 

If we had a random sample, we could go about using the bootstrap to construct a confidence interval for the percent of voters for Candidate A. But we don't have a sample yet – we are trying to find out how big the sample has to be so that our confidence interval is as narrow as we want it to be.

In situations like this, it helps to see what theory predicts.

The Central Limit Theorem says that the probabilities for the sample proportion are roughly normally distributed, centered at the population proportion of 1's, with an SD equal to the SD of the population of 0's and 1's divided by the square root of the sample size.

So the confidence interval will still be the "middle 95%" of a normal distribution, even though we can't pick off the ends as the 2.5th and 97.5th percentiles of bootstrapped proportions.

Is there another way to find how wide the interval would be? Yes, because we know that for normally distributed variables, the interval "center $\pm$ 2 SDs" contains 95% of the data.

The confidence interval will stretch for 2 SDs of the sample proportion, on either side of the center. So the width of the interval will be 4 SDs of the sample proportion.

We are willing to tolerate a width of 1% = 0.01. So, using the formula developed in the last section,

$$
4 \times \frac{\mbox{SD of the 0-1 population}}{\sqrt{\mbox{sample size}}}
~ \le ~ 0.01
$$

So

$$
\sqrt{\mbox{sample size}} ~ \ge ~ 4 \times \frac{\mbox{SD of the 0-1 population}}{0.01}
$$


### The SD of a collection of 0's and 1's 

If we knew the SD of the population, we'd be done. We could calculate the square root of the sample size, and then take the square to get the sample size. But we don't know the SD of the population. The population consists of 1 for each voter for Candidate A, and 0 for all other voters, and *we don't know what proportion of each kind there are*. That's what we're trying to estimate.

So are we stuck? No, because we can *bound* the SD of the population. Here are histograms of two such distributions, one for an equal proportion of 1's and 0's, and one with 90% 1's and 10% 0's. Which one has the bigger SD?

```{r}
pop_50 <- c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0)
pop_90 <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 0)

coins <- tibble(
  prop = c(pop_50, pop_90),
  from = c(rep("Proportion of 1's: 0.5", length(pop_50)),
           rep("Proportion of 1's: 0.9", length(pop_50)))
  )

ggplot(coins) + 
  geom_histogram(aes(x = prop, y = ..density.., fill = from),
                 breaks = seq(-0.5, 1.6, 1), 
                 alpha = 0.8, position = "identity")
```

Remember that the possible values in the population are only 0 and 1.

The blue histogram (50% 1's and 50% 0's) has more spread than the gold. The mean is 0.5. Half the deviations from mean are equal to 0.5 and the other half equal to -0.5, so the SD is 0.5.

In the gold histogram, all of the area is being squished up around 1, leading to less spread. 90% of the deviations are small: 0.1. The other 10% are -0.9 which is large, but overall the spread is smaller than in the blue histogram.

The same observation would hold if we varied the proportion of 1's or let the proportion of 0's be larger than the proportion of 1's. Let's check this by calculating the SDs of populations of 10 elements that only consist of 0's and 1's, in varying proportions. The function `rep(N, i)` is useful for this. It takes two arguments, the number `N` to repeat and `i` repetitions of it, and returns the desired vector consisting of that many `N`'s, e.g.

```{r}
c(rep(1, 3), rep(0, 7))
```

Because we are computing directly the population standard deviation, we will use `pop.sd` instead of the usual `sd` for this experiment. 

```{r}
pop.sd <- function(x) sd(x) * sqrt((length(x)-1) / length(x))
```

```{r}
compute_sd_from_population <- function(i) {
  # Create a vector of i 1's and (10-i) 0's
  population <- c(rep(1, i), rep(0, 10 - i))
  return(pop.sd(population))
}
```

```{r}
sds <- map_dbl(1:9, compute_sd_from_population)

zero_one_sds <- tibble(
  pop_prop_of_1s = seq(0.1, 0.9, 0.1),
  pop_sd = sds
)
zero_one_sds
```
Not surprisingly, the SD of a population with 10% 1's and 90% 0's is the same as that of a population with 90% 1's and 10% 0's. That's because you switch the bars of one histogram to get the other; there is no change in spread.

More importantly for our purposes, the SD increases as the proportion of 1's increases, until the proportion of 1's is 0.5; then it starts to decrease symmetrically.

```{r}
ggplot(zero_one_sds) + 
  geom_point(aes(x = pop_prop_of_1s, y = pop_sd), color = "darkcyan")
```

__Summary:__ The SD of a population of 1's and 0's is at most 0.5. That's the value of the SD when 50% of the population is coded 1 and the other 50% are coded 0.

### The Sample Size

We know that 

$$
\sqrt{\mbox{sample size}} ~ \ge ~ 4 \times \frac{\mbox{SD of the 0-1 population}}{0.01}
$$

and that the SD of the 0-1 population is at most 0.5, regardless of the proportion of 1's in the population. So it is safe to take

$$
\sqrt{\mbox{sample size}} ~ \ge ~ 4 \times \frac{0.5}{0.01} ~=~ 200
$$

So the sample size should be at least $200^2 = 40,000$. That's an enormous sample! But that's what you need if you want to guarantee great accuracy with high confidence no matter what the population looks like.






