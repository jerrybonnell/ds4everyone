# Why the Mean Matters

In this course we have studied several different statistics, including total variation distance, the maximum, the median, and also the mean. Under clear assumptions about randomness, we have drawn empirical distributions of all of these statistics. Some, like the maximum and the total variation distance, have distributions that are clearly skewed in one direction or the other. But the empirical distribution of the sample mean has almost always turned out close to bell-shaped, regardless of the population being studied.

If a property of random samples is true *regardless of the population*, it becomes a powerful tool for inference because we rarely know much about the data in the entire population. The distribution of the mean of a large random sample falls into this category of properties. That is why random sample means are extensively used in data science.

In this chapter, we will study means and what we can say about them with only minimal assumptions about the underlying populations. Question that we will address include:

* What exactly does the mean measure?
* How close to the mean are most of the data?
* How is the sample size related to the variability of the sample mean?
* Why do empirical distributions of random sample means come out bell shaped?
* How can we use sample means effectively for inference?

## Properties of the Mean 

In this course, we have used the words "average" and "mean" interchangeably, and will continue to do so. The definition of the mean will be familiar to you from your high school days or even earlier.

__Definition.__ The *average* or *mean* of a collection of numbers is the sum of all the elements of the collection, divided by the number of elements in the collection.

The function `mean` returns the mean of a vector. 

```{r}
not_symmetric <- c(2, 3, 3, 9)
mean(not_symmetric)
```

### Prerequisites

We will make use of the tidyverse in this chapter, so let's load it in as usual.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readtext)
```

### Basic Properties

The definition and the example above point to some properties of the mean.

* It need not be an element of the collection.
* It need not be an integer even if all the elements of the collection are integers.
* It is somewhere between the smallest and largest values in the collection.
* It need not be halfway between the two extremes; it is not in general true that half the elements in a collection are above the mean.
* If the collection consists of values of a variable measured in specified units, then the mean has the same units too.


We will now study some other properties that are helpful in understanding the mean and its relation to other statistics.

### The Mean is a "Smoother"

You can think of taking the mean as an "equalizing" or "smoothing" operation. For example, imagine the entries in `not_symmetric` above as the dollars in the pockets of four different people. To get the mean, you first put all of the money into one big pot and then divide it evenly among the four people. They had started out with different amounts of money in their pockets (\$2, \$3, \$3, and \$9), but now each person has \$4.25, the mean amount.

### Proportions are Means

If a collection consists only of ones and zeroes, then the sum of the collection is the number of ones in it, and the mean of the collection is the proportion of ones.

```{r}
zero_one <- c(1, 1, 1, 0)
sum(zero_one)
```

```{r}
mean(zero_one)
```

You can replace 1 by the Boolean `TRUE` and 0 by `FALSE`:

```{r}
mean(c(TRUE, TRUE, TRUE, FALSE))
```

Because proportions are a special case of means, results about random sample means apply to random sample proportions as well.

### The Mean and the Histogram 

The mean of the collection {2, 3, 3, 9} is 4.25, which is not the "halfway point" of the data. So then what does the mean measure?

To see this, notice that the mean can be calculated in different ways.

$$\begin{align*}
\mbox{mean} ~ &=~ 4.25 \\ \\
&=~ \frac{2 + 3 + 3 + 9}{4} \\ \\
&=~ 2 \cdot \frac{1}{4} ~~ + ~~ 3 \cdot \frac{1}{4} ~~ + ~~ 3 \cdot \frac{1}{4} ~~ + ~~ 9 \cdot \frac{1}{4} \\ \\
&=~ 2 \cdot \frac{1}{4} ~~ + ~~ 3 \cdot \frac{2}{4} ~~ + ~~ 9 \cdot \frac{1}{4} \\ \\
&=~ 2 \cdot 0.25 ~~ + ~~ 3 \cdot 0.5 ~~ + ~~ 9 \cdot 0.25
\end{align*}$$

The last expression is an example of a general fact: when we calculate the mean, each distinct value in the collection is *weighted* by the proportion of times it appears in the collection.

This has an important consequence. The mean of a collection depends only on the distinct values and their proportions, not on the number of elements in the collection. In other words, the mean of a collection depends only on the distribution of values in the collection.

Therefore, __if two collections have the same distribution, then they have the same mean.__

For example, here is another collection that has the same distribution as `not_symmetric` and hence the same mean.

```{r}
not_symmetric
```

```{r}
same_distribution <- c(2, 2, 3, 3, 3, 3, 9, 9)
mean(same_distribution)
```

The mean is a physical attribute of the histogram of the distribution. Here is the histogram of the distribution of `not_symmetric` or equivalently the distribution of `same_distribution`.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
tibble(not_symmetric) %>%
  ggplot() + 
  geom_histogram(aes(x = not_symmetric, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 10)
```

Imagine the histogram as a figure made out of cardboard attached to a wire that runs along the horizontal axis, and imagine the bars as weights attached at the values 2, 3, and 9. Suppose you try to balance this figure on a point on the wire. If the point is near 2, the figure will tip over to the right. If the point is near 9, the figure will tip over to the left. Somewhere in between is the point where the figure will balance; that point is the 4.25, the mean.

> The mean is the center of gravity or balance point of the histogram.

To understand why that is, it helps to know some physics. The center of gravity is calculated exactly as we calculated the mean, by using the distinct values weighted by their proportions.

Because the mean is a balance point, it is sometimes displayed as a *fulcrum* or triangle at the base of the histogram.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
tibble(not_symmetric) %>%
  ggplot() + 
  geom_histogram(aes(x = not_symmetric, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 10) +
  geom_point(aes(x = mean(not_symmetric), y = -0.02), 
             color = "salmon", shape = 17, size = 4)
```

### The Mean and the Median

If a student's score on a test is below average, does that imply that the student is in the bottom half of the class on that test?

Happily for the student, the answer is, "Not necessarily." The reason has to do with the relation between the average, which is the balance point of the histogram, and the median, which is the "half-way point" of the data.

The relationship is easy to see in a simple example. Here is a histogram of the collection {2, 3, 3, 4} which is in the vector `symmetric`. The distribution is symmetric about 3. The mean and the median are both equal to 3.

```{r}
symmetric <- c(2, 3, 3, 4)
```

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(tibble(symmetric)) + 
geom_histogram(aes(x = symmetric, y = ..density..),
               bins = 3,
               color = "gray", fill = "darkcyan") +
geom_point(aes(x = mean(symmetric), y = -0.02), 
           color = "salmon", shape = 17, size = 4)
```

```{r}
mean(symmetric)
```

```{r}
quantile(symmetric, c(0.5), type = 1)
```

In general, __for symmetric distributions, the mean and the median are equal.__

What if the distribution is not symmetric? Let's compare `symmetric` and `not_symmetric`.

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot() + 
  geom_histogram(data = tibble(symmetric), aes(
                       x = symmetric, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 10) +
  geom_histogram(data = tibble(not_symmetric), aes(
                     x = not_symmetric, y = ..density..), alpha = 0.8,
                 color = "gray", fill = "salmon", bins = 10) + 
  geom_point(aes(x = mean(symmetric), y = -0.02), 
             color = "darkcyan", shape = 17, size = 4) + 
  geom_point(aes(x = mean(not_symmetric), y = -0.02), 
             color = "salmon", shape = 17, size = 4)
```

The dark cyan histogram represents the original `symmetric` distribution. The salmon colored histogram of `not_symmetric` starts out the same as the blue at the left end, but its rightmost bar has slid over to the value 9. The darker shaded region is where the two histograms overlap.

The median and mean of the blue distribution are both equal to 3. The median of the gold distribution is also equal to 3, though the right half is distributed differently from the left.

But the mean of the gold distribution is not 3: the gold histogram would not balance at 3. The balance point has shifted to the right, to 4.25.

In the gold distribution, 3 out of 4 entries (75%) are below average. The student with a below average score can therefore take heart. He or she might be in the majority of the class.

In general, __if the histogram has a tail on one side (the formal term is "skewed"), then the mean is pulled away from the median in the direction of the tail__.

### Example

The data frame `sf2015` contains salary and benefits data for San Francisco City employees in 2015. As before, we will restrict our analysis to those who had the equivalent of at least half-time employment for the year.

```{r message = FALSE}
url <- "https://raw.githubusercontent.com/data-8/materials-su19/master/materials/su19/lec/san_francisco_2015.csv"
sf2015 <- read_csv(url) %>% filter(Salaries > 10000)
```

As we saw earlier, the highest compensation was above \$600,000 but the vast majority of employees had compensations below \$300,000.

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(sf2015) + 
  geom_histogram(aes(x = `Total Compensation`, y = ..density..),
                 breaks = seq(10000, 700000, 25000),
                 color = "gray", fill = "darkcyan")
```

This histogram is skewed to the right; it has a right-hand tail.

The mean gets pulled away from the median in the direction of the tail. So we expect the mean compensation to be larger than the median, and that is indeed the case.

```{r}
compensation <- pull(sf2015, `Total Compensation`)
quantile(compensation, c(0.5), type = 1)
```

```{r}
mean(compensation)
```

Distributions of incomes of large populations tend to be right skewed. When the bulk of a population has middle to low incomes, but a very small proportion has very high incomes, the histogram has a long, thin tail to the right.

The mean income is affected by this tail: the farther the tail stretches to the right, the larger the mean becomes. But the median is not affected by values at the extremes of the distribution. That is why economists often summarize income distributions by the median instead of the mean.

## Variability

TBA

## The SD and the Normal Curve

TBA

## The Central Limit Theorem

TBA

## The Variability of the Sample Mean

TBA

## Choosing a Sample Size

TBA 












