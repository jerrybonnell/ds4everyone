---
output:
  pdf_document: default
  html_document: default
---

# Towards Normality

We have studied several statistics in this course such as the mean and order statistics like the median. We have also drawn empirical distributions to simulate a statistic to see if it approximates well some parameter value of interest. For some statistics, like total variation distance, we saw that the distribution skewed in some direction. But the empirical distribution for the sample mean has consistently shown something that resembles a bell shape, regardless of the underlying population.  

Recall that a goal of this course is to make inferences about data in a population for which we know very little about. If we can extract properties of random samples that are true *regardless of the underlying population*, we have at hand a powerful tool for doing inference. The distribution of the mean is one such property, which we will develop more in depth in this chapter.      

When we refer to these bell-like curves, we are talking about a distribution called the *normal distribution*. To develop an intuition for this, we will first examine a new (but important) statistic called the *standard deviation*, which measures generally how much the data points are away from the mean. It is important because the shape of a normal distribution is completely determinable from its mean and standard deviation. We will then study that by taking a large number of samples from a population, where each sample has no relation to another, the resulting distribution will look like -- you guessed it -- a normal distribution. 

<!-- The mean and the standard deviation jointly provide descriptions of distributions at hand.
There is a type of distributions which we call *normal distributions* (or *bell curves*).
We learn that if we take a large number of samples from a population, where sample values have no relations with each other, then the distribution of the mean you obtain from large number of samples will look like a normal distribution.
Not only that, the resemblance between the distribution of the mean and the normal distribution increases as the number of samples increases.
By taking advantage of the property, you can "map" a collection of samples onto the normal distribution that the samples are likely to represent.
We call the mapping *Z-score*.
The mapping is universal, and so by applying the mapping to collections of samples, you can compare data from difference collections. --> 

## Standard Deviation

The *standard deviation* (or *SD* for short) measures how much the data points are away from the mean. Put another way, it is a measure of the *spread* in the data. 

### Prerequisites

We will continue to make use of the tidyverse in this section so let us load it in. 

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

### Definition of Standard Deviation

Suppose we have some number, say $N$, of samples and for each sample, we have its measurement. Let us say $x_1, \ldots, x_N$ are the measurements.
The mean of the samples is the total of $x_1, \ldots, x_N$ divided by the number of samples $N$, that is
\[
\frac{x_1 + \cdots + x_N}{N}
\]
Let $\mu$ denote the average. 
The *variance* of the samples is the average of the square amounts of these samples from the mean.
\[
\frac{(x_1 - \mu)^2 + \cdots + (x_N - \mu)^2}{N}
\]
We use symbol $\sigma^2$ to represent the variance.
Because of the subtraction of the mean from the individual values, the variance measures the spread of the data around the mean.
Since each term in the total is a square, the unit of the variance is the square of the unit of the samples.
For example, if the original measure is meters then the unit of the variance is square meters.
To make the two units comparable to each other, we take the square root of the variance, which we denote by $\sigma$.
\[
\sigma = \sqrt{\frac{(x_1 - \mu)^2 + \cdots + (x_N - \mu)^2}{N}}
\]
We call the quantity $\sigma$ the *standard deviation*.

### Example: Exam Scores 

Suppose we have drawn ten sample exam scores.

```{r}
sample_scores <- c(78, 89, 98, 90, 96, 90, 84, 91, 98, 76)
sample_scores
```

Let us first compute the mean and the squared element-wise differences from the mean. 

```{r}
mu <- mean(sample_scores)
diffs_from_mu_squared <- (sample_scores - mean(sample_scores)) ** 2
mu
diffs_from_mu_squared
```

Computing the variance is straightforward. We need only to sum up `diffs_from_mu_squared` and divide the resulting quantity by the number of scores. 

```{r}
variance <- sum(diffs_from_mu_squared) / length(sample_scores)
variance
```

Finding the standard deviation is easy -- just take the square root! 

```{r}
sdev <- sqrt(variance)
sdev
```

The mean is 89 points and the standard deviation is about 7.3 points. Let us visualize the distribution of scores. 

```{r dpi=80, fig.align="center", message = FALSE}
tibble(sample_scores) %>%
  ggplot() + 
  geom_histogram(aes(x = sample_scores, y = ..density..),
                 color = "gray", fill = "darkcyan", binwidth = 3) + 
  scale_x_continuous(breaks = seq(70, 100, 4))
```


Let us compare these scores with another group of 10 scores.

```{r}
sample_scores2 <- c(88, 89, 93, 90, 86, 90, 84, 91, 93, 80)
sample_scores2
```

We will repeat the above steps. 

```{r}
mu2 <- mean(sample_scores2)
diffs_from_mu_squared2 <- (sample_scores2 - mean(sample_scores2)) ** 2
variance2 <- sum(diffs_from_mu_squared2) / length(sample_scores2)
sdev2 <- sqrt(variance2)
```

Let us examine the mean and standard deviation of this group. 

```{r}
mu2
sdev2
```

Interestingly, the group has the same mean but the standard deviation is much smaller. Notice that the sum of the squared difference values are much larger in the first vector than in the second. 

```{r}
sum(diffs_from_mu_squared)
sum(diffs_from_mu_squared2)
```

This tells us that the grades in the second group are closer together. We can confirm our findings with a histogram of the score distribution. 

```{r dpi=80, fig.align="center", message = FALSE}
tibble(sample_scores2) %>%
  ggplot() + 
  geom_histogram(aes(x = sample_scores2, y = ..density..),
                 color = "gray", fill = "darkcyan", binwidth = 3) + 
  scale_x_continuous(breaks = seq(70, 100, 4))
```

Observe that the scores are much closer together compared to the first histogram. 


### Sample Standard Deviation

Often in the calculation of standard deviation, instead of dividing by $N$, we divide by $N-1$. We call the versions of $\sigma^2$ and $\sigma$ with division by $N$ the *population variance* and the *population standard deviation*, respectively. The versions that use division by $N-1$ are the *sample variance* and *sample standard deviation*.

There is a subtle consideration in why we want to use $N-1$ for the variance. To see why, suppose you have made up your mind on the value of the mean, say it is 6, and determine all the values in your data __except for the last one__. Here is an example of the situation. 

\[
\mu = 6 = \frac{2 + 8 + 4 + \text{ ?}}{4}
\]

Once you have finished determining three out of the four values, you no longer have anymore freedom in choosing the fourth value. This is because of a constraint composed by the mean formula. That is, if we isolate the sum of the numbers from above...


\[
2 + 8 + 4 + \text{ ?} = 6 * 4
\]

Thus, we can figure quite easily the value of the $?$. 

\[
\text{ ?} = 6 * 4 - (2 + 8 + 4) = 10 
\]

The phenomena we observed is representative of a general fact. In the formula that involves $\mu$ and $x_1, \ldots, x_N$, like the one for variance, there are no longer $N$ independent values, there are only $N-1$ of them.

The difference in quantity by using $N-1$ instead of $N$ may not be large, specifically, when $N$ is large and the differences are small. Still, for an accurate understanding of the data spread, the choice can be highly critical. For this reason, statisticians prefer to work with the sample standard deviation when dealing with samples. We will do so as well for the remainder of this course.  

### The `sd` function

Fortunately R comes with a function `sd` that computes the *sample* standard deviation. We  compare them side-by-side with the values we computed earlier.

```{r}
c(sdev,sd(sample_scores))
c(sdev2,sd(sample_scores2))
```

Because of the use of $N-1$ in place of $N$, the values in the right column are greater than those on the left.


## More on Standard Deviation

The previous section introduced the notion of standard deviation (SD) and how it is a measure of *spread* in the data. We build upon our discussion of SD in this section. 

### Prerequisites

We will continue to make use of the tidyverse. To demonstrate an example of the use (as well as misuses) of SD, we also load in two synthetically generated datasets from the package `datasauRus`.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(datasauRus)
```

We retrieve the two datasets here, and will explore them shortly. 

```{r, message = FALSE, warning = FALSE}
star <- datasaurus_dozen %>%
  filter(dataset == "star") 
bullseye <- datasaurus_dozen %>%
  filter(dataset  == "bullseye")
```

### Working with SD 

To see what we can glean from the SD, let us turn to a more interesting dataset. The tibble `starwars` contains data about all characters in the Star Wars canon. The table records a wealth of information about each character, e.g., name, height (cm), weight (kg), home world, etc.  Pull up the help (`?starwars`) for more information about the dataset. 

```{r}
starwars
```

Let us focus on the characters' heights. It turns out that the height information is missing for some of them; let us remove these entries from the dataset. 

```{r}
filter(starwars, is.na(height))
```

```{r}
starwars_clean <- starwars %>%
  filter(!is.na(height))
```

Here is a histogram of the characters' heights. 

```{r dpi=80, fig.align="center", warning = FALSE, message = FALSE}
ggplot(starwars) + 
  geom_histogram(aes(x = height, y = ..density..),
                 color = "gray", fill = "darkcyan", bins = 20)
```

The average height of Star Wars characters is just over 174 centimeters (or 5'8''), which is about 1.4 centimeters shorter than the average height of men in the United States (5'9''). 

```{r}
mean_height <- starwars_clean %>%
  pull(height) %>%
  mean()
mean_height
```

The SD tells us how far off a character's height is from the average, which is about 34.77 centimeters. 

```{r}
sd_height <- starwars_clean %>%
  pull(height) %>%
  sd()
sd_height
```

The shortest character in cannon is the legendary Jedi Master Yoda, registering a height of just 66 centimeters!

```{r}
starwars_clean %>%
  arrange(height) %>%
  head(1)
```

Yoda is about 108 centimeters shorter than the average height. 

```{r}
66 - mean_height
```

Put another way, Yoda is about 3 SDs *below* the average height. 

```{r}
(66 - mean_height) / sd_height
```

We can repeat the same steps for the tallest character in canon: the Quermian Yarael Poof.   

```{r}
starwars_clean %>%
  arrange(desc(height)) %>%
  head(1)
```

Yarael Poof's height is about 2.58 SDs *above* the average height.  

```{r}
(264 - mean_height) / sd_height
```

It seems then that the tallest and shortest characters are only a few SDs away from the mean -- no more than 3. The mean and SD is useful in this way: all the heights of Star Wars characters can be found within 3 SDs of the mean. This gives us a good sense as to where the histogram falls on the number line.

### Standard units

The number of standard deviations a value is away from the mean can be calculated as follows: 
\[
z = \frac{\text{value} - \text{mean}}{\text{SD}}

\]

The resulting quantity $z$ measures *standard units* and is sometimes called the *z-score*. We saw two such examples of standard units when studying the tallest and shortest Star Wars characters. 

R provides a function `scale` that converts values into standard units. Such a transformation is called *scaling*. For instance, we can add a new column with all of the heights in standard units. 

```{r}
starwars_clean %>%
  mutate(su = scale(height)) %>%
  select(name, height, su)
```

As before, note that the standard units are much less than 3 SDs (in general, they need not be). 

Scaling is often used in data analysis as it allows us to put data in a comparable format. Let us visit another example to see why.

### Example: Judging a Contest 

Suppose three eccentric judges evaluated eight contestants at a contest. They evaluate each contestant on the scale of 1 to 10. Enter the judges. 

* __Mrs. Sweet.__ Her cakes are loved by everyone. She also has a reputation for giving high scores. She has never given a score below 5.
* __Mr. Coolblood.__ He has a reputation of being ruthless with a strong penchant for corn dogs. He has never given a high score to anyone in the past.
* __MS Hodgpodge.__ Her salsa dip is quite spicy. Yet she tends to spread her scores fairly. If there are enough contestants, she gives 1 or 2 to at least one contestant and 9 or 10 to at least one contestant.

Here are the results.

```{r}
contest <- tribble(~name, ~sweet, ~coolblood, ~hodgepodge,
                        "Ashley",  9, 5, 9,
                        "Bruce",   8, 6, 4,
                        "Cathryn", 7, 5, 5,
                        "Drew",    8, 2, 1,
                        "Emily",   9, 7, 7,
                        "Frank",   6, 1, 1,
                        "Gail",    8, 4, 4,
                        "Harry",   5, 3, 3
                        ) 
contest
```


Would it be enough to simply total their scores to determine the winner? Or should we take into account their eccentricities and *scale* the scores somehow? 

That is where standard units come to play. We adjust each judge's scores by subtracting his/her mean and then dividing it by his/her standard deviation. After adjustment, each score represents relative to the standard deviation how much away the original score is from the mean.

Let us compute the raw total of the three scores, and compare this with the scaled scores. 

```{r}
contest %>% mutate(
  sweet_su = scale(sweet),
  hodge_su = scale(hodgepodge),
  cool_su = scale(coolblood),
  raw_sum = sweet + coolblood + hodgepodge, 
  scaled_sum = sweet_su + hodge_su + cool_su
  ) %>%
  select(name, raw_sum, scaled_sum)
```

Based on the raw scores, we identify a two-way tie between Ashely and Emily. The scaled scores allow us to break the scores and we can announce, with confidence, Emily as the winner! 

### Be careful with summary statistics! 

The SD is part of what we call *summary statistics* as they are useful for *summarizing* information about a distribution. The SD tells us about the *spread* of the data and where a histogram might sit on a number line. 

Indeed, SD is an important tool and we will continue our study of it in the following sections. However, SD -- as with all summary statistics -- must be used with caution. We end our discussion in this section with an instructive example as to why.  

Recall that we have two datasets `star` and `bullseye`. Each contains some `x` and `y` coordinate pairs. Let us examine some of the `x` coordinates in each.

```{r}
star_x <- star %>%
  pull(x)
head(star_x)
```

```{r}
bullseye_x <- bullseye %>%
  pull(x)
head(bullseye_x)
```

The values seem close, but look different enough. Let us compute the SD. 

```{r}
sd(star_x)
sd(bullseye_x)
```

How about the mean?

```{r}
mean(star_x)
mean(bullseye_x)
```

The result points to a clear answer: both distributions represented by `star_x` and `bullseye_x` have the *exact* same SD and mean. We may be tempted then to conclude that both distributions are *equivalent*. We would be mistaken!

Whenever in doubt, turn to visualization. We plot a histogram of the distributions overlaid on top of each other. If the distributions are actually equal, we expect this to be reflected in the histogram. 

```{r dpi=80, fig.align="center", message = FALSE}
datasaurus_dozen %>%
  filter(dataset == "star" | dataset == "bullseye") %>%
  ggplot() + 
  geom_histogram(aes(x = x, y = ..density.., fill = dataset),
                 color = "gray", alpha = 0.9, bins = 10) + 
  scale_x_continuous(breaks = seq(15, 90, 5))
```

The histogram confirms that, contrary to what we might expect, these distributions are very much different -- despite having the same SD and mean! Thus, the moral of this lesson: be careful with summary statistics and __always visualize your data.__ 

The `star` and `bullseye` datasets have an additional `y` coordinate which we have ignored in this study. We leave it as an exercise to the reader to determine if the distributions represented by `y` are also different yet have identical summary statistics. 


## The Normal Curve

The mean and SD are key pieces of information in determining the shape of some distributions.
The most famous of them is the *normal distribution*, which we turn to in this section. 

### Prerequisites 

We will continue to make use of the tidyverse so let us load it in. 

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```

### The Standard Normal Curve

The standard normal curve has a rather complicated formula: 

$$
\phi(z) = {\frac{1}{\sqrt{2 \pi}}} e^{-\frac{1}{2}z^2}, ~~ -\infty < z < \infty
$$

where $\pi$ is the constant $3.141592\ldots$ and $e$ is *Euler's number* $2.71828\ldots$. It is best to think of this visually as in the following plot. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "black") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -∞ < z < ∞") +
  theme(plot.title = element_text(hjust = 0.5))
```

The values on the x-axis are in *standard units* (or *z*-scored values). We observe that the curve is symmetric around 0 where the "bulk" of the data is close to the mean. Following are some properties about the curve: 

* The total area under the curve is 1.
* The curve is symmetric so it inherits a property we know about symmetric distributions: the mean and median are both 0. 
* The SD is 1 which, fortunately for us, is clearly identifiable on the x-axis.   
* The curve has two *points of inflection* at -1 and +1, which are annotated on the following plot.  

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "black") +
  geom_point(aes(x = -1, y = 0.24), color = "red", size = 3) + 
  geom_point(aes(x = 1, y = 0.24), color = "red", size = 3) + 
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -∞ < z < ∞") +
  theme(plot.title = element_text(hjust = 0.5))
```

Observe how the curve looks like a salad bowl in the regions $(-\infty, -1)$, and $(1, \infty)$ and in the region $(-1, 1)$ the bowl is flipped upside down! 

### Area Under the Curve

We can find the area under the standard normal curve with the function `pnorm`. Let us use this function to find the amount of area to the left of $z = -1$. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-3,-1), geom = "area", fill = "salmon") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -∞ < z < -1") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
pnorm(-1)
```

So about 15.9% of the data lies to the left of $z = -1$. Recall from our properties that the curve is symmetric and that the total area must sum to 1. We can take advantage of these two to calculate other areas, e.g., the area to the *right* of -1. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-1,3), geom = "area", fill = "salmon") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -1 < z < ∞") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
1 - pnorm(-1)
```

That's about 84% of the data. 

Here is a trickier problem: how much area is within 1 SD? Put another way, what is the area between $z = -1$ and $z = 1$? That's the orange-shaded area in the following plot.  

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-1,1), geom = "area", fill = "salmon") +
  stat_function(fun = dnorm, 
                xlim = c(-3,-1), geom = "area", fill = "darkcyan") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -1 < z < 1") +
  theme(plot.title = element_text(hjust = 0.5))
```

You might be able to guess at a few ways to answer this. One way to do it is to find the area to left of $z = -1$ (shaded in dark cyan) and subtract it from the area to the left of $z = 1$. This resulting subtraction is the area in orange, between $z = -1$ and $z = 1$. 

```{r}
pnorm(1) - pnorm(-1)
```

That's about 68.3% of the data. 

How much data is within 2 SDs?  

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dnorm, 
                xlim = c(-2,2), geom = "area", fill = "salmon") +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("ɸ(z)") +
  xlab("z") +
  ggtitle("Normal curve ~ (μ = 0, σ = 1) -2 < z < 2") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
pnorm(2) - pnorm(-2)
```

That's about 95% of the data. To complete the story, let's look at the amount of data within 3 SDs.

```{r}
pnorm(3) - pnorm(-3)
```

That covers *almost* all the data, but mind the italics. There is still about 0.3% of the data that lies beyond 3 SDs, which can happen. 

Therefore, for a standard normal curve, we can calculate the probability of where a sample might lie. We have: 

* a sample that falls within $\pm 1$ SD is about __68%__.
* a sample that falls within $\pm 2$ SDs is about __95%__.
* a sample that falls between $\pm 3$ is about __99.73%__.

### Exam Scores and the `dnorm` function

A frequent -- and perhaps even cliched -- reference point to the bell curve occurs in the analysis of test scores.

[Student Performance in Exams](https://www.kaggle.com/spscientist/students-performance-in-exams) is a data set available from the data archive [Kaggle](http://www.kaggle.com).

```{r message = FALSE}
path <- "data/StudentsPerformance.csv"
students_data <- read_csv(path)
```

Here is the histogram of math scores.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(students_data, aes(x = `math score`, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", breaks = seq(0, 100, 4)) 
```

Here is the histogram of the reading scores.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(students_data, aes(x = `reading score`, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", breaks = seq(0, 100, 4)) 
```

Here is the histogram of the writing scores.

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(students_data, aes(x = `writing score`, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", breaks = seq(0, 100, 4)) 
```

Let us compute the mean and standard deviation of the math scores.

```{r}
math_mean <- mean(students_data$`math score`)
math_sd <- sd(students_data$`math score`)
c(math_mean,math_sd)
```

We can create a normal curve using the function `dnorm`. It takes as arguments a vector of x values, the mean, and SD; in terms of the plot, it returns the "y coordinate" for each corresponding x value. Let us construct a normal curve for the math scores using the mean and SD we have just obtained, and overlay it atop the histogram. 

```{r}
math_norm <- dnorm(students_data$`math score`, mean = math_mean, sd = math_sd)
```

```{r dpi=80,  fig.align="center", message = FALSE}
ggplot(students_data, aes(x = `math score`, y = ..density..)) + 
  geom_histogram(col="grey", fill = "darkcyan", breaks = seq(0, 100, 4)) +
  geom_line(mapping = aes(x = `math score`, y = math_norm), color = "salmon") +
  xlab("score") +
  ggtitle("Normal curve and the histogram") 
```

We observe the following.

* The histogram shows strong resemblance to the normal curve.
* The right end shows that the histograms and the curve go beyond 100, but 100 is the maximum and so there are no data points beyond 100.

How close is the curve to the histogram? Let us compare the proportion of students whose math score is at most 95 with respect to the two distributions.

```{r}
pnorm(95, mean = math_mean, sd = math_sd)
```

```{r}
num_rows <- students_data %>%
  filter(`math score` <= 95) %>%
  nrow()
num_rows / nrow(students_data)
```

The normal distribution says 97.17% while the actual record is 97.80%. Pretty close. 

How about 85?

```{r}
pnorm(85, mean = math_mean, sd = math_sd)
```

```{r}
num_rows <- students_data %>%
  filter(`math score` <= 85) %>%
  nrow()
num_rows / nrow(students_data)
```

The two values are 89.38% versus 89.70%.

## Central Limit Theorem

As noted at the outset of this chapter, the bell-shaped distribution has been a running motif through most of our examples. While most of the data histograms we studied have not turned out bell-shaped, the empirical distributions representing some simulated statistic has reliably turned out that way.

This is no coincidence. In fact, it is the consequence of an impressive theory in statistics called the *Central Limit Theorem*. We will study the theorem in this section, but let us first see a situation where a bell-shaped distribution results to set up the context. 

### Prerequisites

We will make use of the tidyverse in this chapter, so let’s load it in as usual.

```{r}
library(tidyverse)
```

We will also work with receiver data from the NFL and NCAA in this section, so let us load in those datasets. 

```{r message = FALSE}
nfl <- read_csv("data/nfl_receiving0.csv")
ncaa <- read_csv("data/ncaa-receivers-2005-2011.csv")
```

### Net Allotments from a Clumsy Clerk 

Recall that in an earlier section, we simulated the story of a minister and the doubling grains. In that story, the amount of grains a minister receives doubles each day; by the end of the 64th day, he expects to receive an impressive total of $2^{64}-1$ grains. 

The king has put a clerk in charge to assign the correct amount of grains each day. However, she is human and tabulates the grains by hand, so is prone to error: she may double count the number of grains on some days or forget to count a day altogether. But for the most part she gets it right. 

This yields three events on a given day: the counting was *as expected*, it was *double counted*, or the clerk *forgot*. We said that the probabilities for these events are $2/3$, $1/3$, and $1/3$, respectively. Here is the distribution. 

```{r dpi=80, fig.align="center", message = FALSE}
df <- tribble(~event, ~probability,
              "as expected", 2/3,
              "double counted", 1/3,
              "forgot", 1/3
              )
ggplot(df) + 
  geom_bar(aes(x = event, y = probability), 
                 stat = "identity", fill = "darkcyan")
```

The number of grain allotments received after 64 days is the *sum* of draws made at random with replacement from this distribution.  

Using `sample` we can see the result of the grain allotment on any given day. 

```{r}
allotment <- sample(c(0, 1, 2), prob = c(1/3, 2/3, 1/3), replace = TRUE, size = 1) 
allotment
```

If `allotment` turns out `1`, the minister received the correct amount of grains that day; `0` indicates that no grains were received and `2` means he received double the amount. The minister hopes that the allotments will total to be 64 and, to the king's dismay, perhaps even more.  

We are now ready to simulate one value of the statistic. Let us put our work into a function we can use called `one_simulation`.

```{r}
one_simulation <- function() {
  allotments <- sample(c(0, 1, 2), prob = c(1/3, 2/3, 1/3), replace = TRUE, size = 64)
  return(sum(allotments))
} 
one_simulation()
```

The following code simulates 10,000 times the net allotments the minister received at the end of the 64th day. 

```{r echo = FALSE}
set.seed(1000)
```

```{r}
num_repetitions <- 10000
net_allotments <- replicate(n = num_repetitions, one_simulation())

results <- tibble(
  repetition = 1:num_repetitions,
  net_allotments = net_allotments
)
results
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(results) + 
  geom_histogram(aes(x = net_allotments, y = ..density..), 
                 color = "gray", fill = "darkcyan", bins = 15)
```

We observe a bell-shaped curve, even though the distribution we drew from does not look bell-shaped. Note the the distribution is __centered__ around 64 days, as expected. With kudos to the clerk for a job well done, we note that accomplishing such a feat in reality would be impossible.    

To understand the __spread__, look for the inflection point in this histogram. That point occurs at around 70 days, which means the SD is the distance from the center to this point -- that looks to be about 5.6 days. 

```{r}
results %>%
  pull(net_allotments) %>%
  sd()
```

To confirm the bell-shaped curve we are seeing, we can create a normal distribution from this mean and SD and overlay it atop the empirical histogram. We observe that it provides a good fit. 

```{r}
curve <- dnorm(net_allotments, mean = 64, sd = 5.66)
```

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(results) + 
  geom_histogram(aes(x = net_allotments, y = ..density..), 
                 color = "gray", fill = "darkcyan", bins = 15) +
  geom_line(mapping = aes(x = net_allotments, y = curve), color = "salmon") 
```

### Central Limit Theorem 

The Central Limit Theorem is a formalization of the phenomena we observed from the doubling grains story. Formally, it states the following. 

> The sum or average of a large random sample that is identically and independently distributed will resemble a normal distribution, regardless of the underlying distribution from which the sample is drawn. 

The "identically and independently distributed" condition, often abbreviated simply as IID, is a mouthful. It means that the random samples we draw must have no relation to each other, i.e., the drawing of one sample (say, the event *forgot*) does not make another event more or less likely of occcuring. Hence, we prefer a sampling plan of sampling with replacement. 

The Central Limit Theorem is a powerful concept because it becomes possible to make inferences about unknown populations with very little knowledge. And, the larger the sample size becomes, the stronger the resemblance. 

### Receiver Yardage in the NFL and NCAA 

The Central Limit Theorem says that the sum of IID samples follows a distribution that resembles a normal distribution. Let us turn to the receiver data in the NFL and NCAA. In a large sample of football receivers, what is the sample average for yardage? According to the theorem, we expect the distribution to be roughly normal. 

Here is the yardage distribution for NFL players. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(nfl) +
  geom_histogram(aes(x = Yds, y = ..density..), color = "gray", fill="darkcyan")
```

And here it is for NCAA players. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(ncaa) +
  geom_histogram(aes(x = Yards, y = ..density..), color = "gray", fill="darkcyan")
```

Observe how these data histograms look nothing like a normal distribution. 

Let us simulate the average yardage in a sample of 100 players for both the NFL and NCAA leagues. We will write a function to simulate one value for us. 

```{r}
one_simulation <- function(df, label, sample_size) {
  sample_mean <- df %>%
    slice_sample(n = sample_size, replace = TRUE) %>%
    pull({{ label }}) %>%
    mean()
  return(sample_mean)
} 
```

The function `one_simulation` takes as arguments the tibble `df`, the column `label` to use for computing the statistic, and the sample size `sample_size`. 

Here is one run of the function with the NFL data. 

```{r}
one_simulation(nfl, Yds, 100)
```

As before, we will simulate the statistic for NFL receivers 10,000 times. 

```{r echo = FALSE}
set.seed(1000)
```

```{r}
num_repetitions <- 10000
sample_means <- replicate(n = num_repetitions, one_simulation(nfl, Yds, 100))

nfl_results <- tibble(
  repetition = 1:num_repetitions,
  sample_mean = sample_means, 
  league = "NFL"
)
nfl_results
```

Let us repeat the simulation for the NCAA receivers. 

```{r echo = FALSE}
set.seed(1000)
```

```{r}
sample_means <- replicate(n = num_repetitions, one_simulation(ncaa, Yards, 100))

ncaa_results <- tibble(
  repetition = 1:num_repetitions,
  sample_mean = sample_means,
  league = "NCAA"
)
ncaa_results
```

Let us merge the two tibbles together so that we can plot the empirical distributions together. 

```{r}
results <- bind_rows(nfl_results, ncaa_results)
```

Let us plot the two empirical distributions together. 

```{r dpi=80, fig.align="center", message = FALSE}
ggplot(results) +
  geom_histogram(aes(x = sample_mean, y = ..density.., fill = league), 
                 bins = 20, color = "gray", position = "identity", alpha = 0.8)
```

Indeed, we see that both distributions are approximately normal, where each centers around a different mean. To confirm the shape, we overlay a normal curve atop each histogram. 

```{r dpi=80, fig.align="center", echo = FALSE, message = FALSE}
dnorm1 <- dnorm(nfl_results$sample_mean, 
                mean = mean(nfl_results$sample_mean), sd = sd(nfl_results$sample_mean))
dnorm2 <- dnorm(ncaa_results$sample_mean, 
                mean = mean(ncaa_results$sample_mean), sd = sd(ncaa_results$sample_mean))

ggplot() +
  geom_histogram(data = results, aes(x = sample_mean, y = ..density.., fill = league), 
                 bins = 20, color = "gray", position = "identity", alpha = 0.8) + 
  geom_line(data = nfl_results, mapping = aes(x = sample_mean, y = dnorm1), color = "darkcyan") +
  geom_line(data = ncaa_results, mapping = aes(x = sample_mean, y = dnorm2), color = "salmon4") 
```

<!-- each is about 30 SD -->

We leave it as an exercise to the reader to determine the center and spread for each of the distributions. 


